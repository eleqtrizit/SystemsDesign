{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Systems Design and Coding","text":"<p>A curated a list of resources for someone trying to crack a software development or site reliability engineer (SRE) interview.  </p>"},{"location":"#coding","title":"Coding","text":""},{"location":"#blind-75","title":"Blind 75","text":"<p>Top 75 coding questions, as identified posts on the app Blind.  </p> <p>Google Spreadsheet (Questions + Videos, copy and use!)</p> <p>Grind75, study plan by the Blind 75 author.</p> <p>NeetCode.io</p> <p>Youtube Solutions Playlist</p> <p>Leetcode</p>"},{"location":"#companywise-problems","title":"Companywise Problems","text":"<p>Coding problems categorized by company.</p> <p>leetcode-company-wise-problems-2022</p>"},{"location":"#system-design-primer","title":"System Design Primer","text":"<p>The System Design Primer Github page is the open-sourced bible on this topic.  It comes with flashcards, mock interviews (w/solutions), and much more.</p> <p>Related Notes...</p>"},{"location":"#cap-theorum","title":"CAP Theorum","text":"<p>(Dev, SRE) Any distributed data store can have 2 of these 3 things, Consistency, Availability, Partition Tolerance.  Understand what these mean to choose the right data store for a project.</p>"},{"location":"#distibuted-system-fundamentals","title":"Distibuted System Fundamentals","text":"<p>(Dev, SRE) TLDR summary of ACID, Replication, Sync/Async, Consensus, Partitioning, Distributed transactions, and Storage</p>"},{"location":"#how-to-design-large-scale-systems","title":"How to Design Large Scale Systems","text":"<p>(Dev, SRE) Quick list of considerations.</p>"},{"location":"#system-design-interview-steps","title":"System Design Interview Steps","text":"<p>(Dev, SRE) How to tackle the interview systematically.</p>"},{"location":"#designing-data-intensive-applications","title":"Designing Data Intensive Applications","text":"<p>This section is based on the book published by O'Reilly. PDF</p> <p>This book is a must-read. It gives you a high level understanding of different technology, including the idea behind it, the pros and cons, and the problem it is trying to solve. A great book for practitioners who want to learn all the essential concepts quickly.</p>"},{"location":"#of-particular-importance","title":"Of particular importance:","text":"<ul> <li>Reliable, scalable, and maintainable applications (Dev, SRE)</li> <li>Data models and query language -&gt; Relational model vs document model (Dev, SRE)</li> <li>Replication (SRE) </li> <li>Partitioning (Dev, SRE)</li> <li>Transactions (Dev, SRE)</li> <li>The trouble with distributed systems (SRE)</li> <li>Batch processing (Dev, SRE)</li> <li>Stream processing (Dev, SRE)</li> </ul>"},{"location":"#cliff-notes","title":"Cliff Notes","text":"<p>Cliff Notes - moyano83 Cliff Notes - Someguy</p>"},{"location":"#specific-conceptsresources","title":"Specific Concepts/Resources","text":"<p>Asynchronism (Dev, SRE)</p> <p>Caching (Dev, SRE)</p> <p>Databases (Dev, SRE)</p> <p>File Systems (SRE)</p> <p>Google Systems Design - Code Deployment (Dev, SRE)</p> <p>HTTP (SRE)</p> <p>Linux Interview Questions (SRE)</p> <p>Linux Networking (SRE)</p> <p>Linux OS Internals (SRE)</p> <p>Load Balancers (SRE)</p> <p>Networking (SRE)</p> <p>SRE Interview Questions (SRE)</p> <p>SSH (SRE)</p> <p>Systems Design Template (Dev, SRE)</p> <p>Troubleshooting via Metrics (SRE)</p> <p>Unix Processes (SRE)</p> <p>What happens when you type google.com into a browser (SRE)</p>"},{"location":"#extras","title":"Extras","text":"<p>Github Repo for this site</p> <p>Published to Github Pages via: jobindj/obsidian-publish-mkdocs Theme: Material</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/","title":"Designing Data-Intensive Applications","text":"<p>From someguy on github</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#reliable-scalable-and-maintainable-applications","title":"Reliable, scalable, and maintainable applications","text":"<p>A data-intensive application is typically built from standard building blocks. They usually need to: * Store data (databases) * Speed up reads (caches) * Search data (search indexes) * Send a message to another process asynchronously (stream processing) * Periodically crunch data (batch processing)</p> <ul> <li>Reliability. To work correctly even in the face of adversity.</li> <li>Scalability. Reasonable ways of dealing with growth.</li> <li>Maintainability. Be able to work on it productively.</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#reliability","title":"Reliability","text":"<p>Typical expectations: * Application performs the function the user expected * Tolerate the user making mistakes * Its performance is good * The system prevents abuse</p> <p>Systems that anticipate faults and can cope with them are called fault-tolerant or resilient.</p> <p>A fault is usually defined as one component of the system deviating from its spec, whereas failure is when the system as a whole stops providing the required service to the user.</p> <p>You should generally prefer tolerating faults over preventing faults.</p> <ul> <li>Hardware faults. Until recently redundancy of hardware components was sufficient for most applications. As data volumes increase, more applications use a larger number of machines, proportionally increasing the rate of hardware faults. There is a move towards systems that tolerate the loss of entire machines. A system that tolerates machine failure can be patched one node at a time, without downtime of the entire system (rolling upgrade).</li> <li>Software errors. It is unlikely that a large number of hardware components will fail at the same time. Software errors are a systematic error within the system, they tend to cause many more system failures than uncorrelated hardware faults.</li> <li>Human errors. Humans are known to be unreliable. Configuration errors by operators are a leading cause of outages. You can make systems more reliable:<ul> <li>Minimising the opportunities for error, peg: with admin interfaces that make easy to do the \"right thing\" and discourage the \"wrong thing\".</li> <li>Provide fully featured non-production sandbox environments where people can explore and experiment safely.</li> <li>Automated testing.</li> <li>Quick and easy recovery from human error, fast to rollback configuration changes, roll out new code gradually and tools to recompute data.</li> <li>Set up detailed and clear monitoring, such as performance metrics and error rates (telemetry).</li> <li>Implement good management practices and training.</li> </ul> </li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#scalability","title":"Scalability","text":"<p>This is how do we cope with increased load. We need to succinctly describe the current load on the system; only then we can discuss growth questions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#twitter-example","title":"Twitter example","text":"<p>Twitter main operations - Post tweet: a user can publish a new message to their followers (4.6k req/sec, over 12k req/sec peak) - Home timeline: a user can view tweets posted by the people they follow (300k req/sec)</p> <p>Two ways of implementing those operations: 1. Posting a tweet simply inserts the new tweet into a global collection of tweets. When a user requests their home timeline, look up all the people they follow, find all the tweets for those users, and merge them (sorted by time). This could be done with a SQL <code>JOIN</code>. 2. Maintain a cache for each user's home timeline. When a user posts a tweet, look up all the people who follow that user, and insert the new tweet into each of their home timeline caches.</p> <p>Approach 1, systems struggle to keep up with the load of home timeline queries. So the company switched to approach 2. The average rate of published tweets is almost two orders of magnitude lower than the rate of home timeline reads.</p> <p>Downside of approach 2 is that posting a tweet now requires a lot of extra work. Some users have over 30 million followers. A single tweet may result in over 30 million writes to home timelines.</p> <p>Twitter moved to an hybrid of both approaches. Tweets continue to be fanned out to home timelines but a small number of users with a very large number of followers are fetched separately and merged with that user's home timeline when it is read, like in approach 1.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#describing-performance","title":"Describing performance","text":"<p>What happens when the load increases: * How is the performance affected? * How much do you need to increase your resources?</p> <p>In a batch processing system such as Hadoop, we usually care about throughput, or the number of records we can process per second.</p> <p>It's common to see the average response time of a service reported. However, the mean is not very good metric if you want to know your \"typical\" response time, it does not tell you how many users actually experienced that delay.</p> <p>Better to use percentiles. * Median (50th percentile or p50). Half of user requests are served in less than the median response time, and the other half take longer than the median * Percentiles 95th, 99th and 99.9th (p95, p99 and p999) are good to figure out how bad your outliners are.</p> <p>Amazon describes response time requirements for internal services in terms of the 99.9th percentile because the customers with the slowest requests are often those who have the most data. The most valuable customers.</p> <p>On the other hand, optimising for the 99.99th percentile would be too expensive.</p> <p>Service level objectives (SLOs) and service level agreements (SLAs) are contracts that define the expected performance and availability of a service. An SLA may state the median response time to be less than 200ms and a 99th percentile under 1s. These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met.</p> <p>Queueing delays often account for large part of the response times at high percentiles. It is important to measure times on the client side.</p> <p>When generating load artificially, the client needs to keep sending requests independently of the response time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#latency-and-response-time","title":"Latency and response time","text":"<p>The response time is what the client sees. Latency is the duration that a request is waiting to be handled.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#percentiles-in-practice","title":"Percentiles in practice","text":"<p>Calls in parallel, the end-user request still needs to wait for the slowest of the parallel calls to complete. The chance of getting a slow call increases if an end-user request requires multiple backend calls.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#approaches-for-coping-with-load","title":"Approaches for coping with load","text":"<ul> <li>Scaling up or vertical scaling: Moving to a more powerful machine</li> <li>Scaling out or horizontal scaling: Distributing the load across multiple smaller machines.</li> <li>Elastic systems: Automatically add computing resources when detected load increase. Quite useful if load is unpredictable.</li> </ul> <p>Distributing stateless services across multiple machines is fairly straightforward. Taking stateful data systems from a single node to a distributed setup can introduce a lot of complexity. Until recently it was common wisdom to keep your database on a single node.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#maintainability","title":"Maintainability","text":"<p>The majority of the cost of software is in its ongoing maintenance. There are three design principles for software systems: * Operability. Make it easy for operation teams to keep the system running. * Simplicity. Easy for new engineers to understand the system by removing as much complexity as possible. * Evolvability. Make it easy for engineers to make changes to the system in the future.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#operability-making-life-easy-for-operations","title":"Operability: making life easy for operations","text":"<p>A good operations team is responsible for * Monitoring and quickly restoring service if it goes into bad state * Tracking down the cause of problems * Keeping software and platforms up to date * Keeping tabs on how different systems affect each other * Anticipating future problems * Establishing good practices and tools for development * Perform complex maintenance tasks, like platform migration * Maintaining the security of the system * Defining processes that make operations predictable * Preserving the organisation's knowledge about the system</p> <p>Good operability means making routine tasks easy.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#simplicity-managing-complexity","title":"Simplicity: managing complexity","text":"<p>When complexity makes maintenance hard, budget and schedules are often overrun. There is a greater risk of introducing bugs.</p> <p>Making a system simpler means removing accidental complexity, as non inherent in the problem that the software solves (as seen by users).</p> <p>One of the best tools we have for removing accidental complexity is abstraction that hides the implementation details behind clean and simple to understand APIs and facades.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#evolvability-making-change-easy","title":"Evolvability: making change easy","text":"<p>Agile working patterns provide a framework for adapting to change.</p> <ul> <li>Functional requirements: what the application should do</li> <li>Nonfunctional requirements: general properties like security, reliability, compliance, scalability, compatibility and maintainability.</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#data-models-and-query-language","title":"Data models and query language","text":"<p>Most applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below by providing a clean data model. These abstractions allow different groups of people to work effectively.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#relational-model-vs-document-model","title":"Relational model vs document model","text":"<p>The roots of relational databases lie in business data processing, transaction processing and batch processing.</p> <p>The goal was to hide the implementation details behind a cleaner interface.</p> <p>Not Only SQL has a few driving forces: * Greater scalability * preference for free and open source software * Specialised query optimisations * Desire for a more dynamic and expressive data model</p> <p>With a SQL model, if data is stored in a relational tables, an awkward translation layer is translated, this is called impedance mismatch.</p> <p>JSON model reduces the impedance mismatch and the lack of schema is often cited as an advantage.</p> <p>JSON representation has better locality than the multi-table SQL schema. All the relevant information is in one place, and one query is sufficient.</p> <p>In relational databases, it's normal to refer to rows in other tables by ID, because joins are easy. In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak.</p> <p>If the database itself does not support joins, you have to emulate a join in application code by making multiple queries.</p> <p>The most popular database for business data processing in the 1970s was the IBM's Information Management System (IMS).</p> <p>IMS used a hierarchical model and like document databases worked well for one-to-many relationships, but it made many-to-,any relationships difficult, and it didn't support joins.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-network-model","title":"The network model","text":"<p>Standardised by a committee called the Conference on Data Systems Languages (CODASYL) model was a generalisation of the hierarchical model. In the tree structure of the hierarchical model, every record has exactly one parent, while in the network model, a record could have multiple parents.</p> <p>The links between records are like pointers in a programming language. The only way of accessing a record was to follow a path from a root record called access path.</p> <p>A query in CODASYL was performed by moving a cursor through the database by iterating over a list of records. If you didn't have a path to the data you wanted, you were in a difficult situation as it was difficult to make changes to an application's data model.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-relational-model","title":"The relational model","text":"<p>By contrast, the relational model was a way to lay out all the data in the open\" a relation (table) is simply a collection of tuples (rows), and that's it.</p> <p>The query optimiser automatically decides which parts of the query to execute in which order, and which indexes to use (the access path).</p> <p>The relational model thus made it much easier to add new features to applications.</p> <p>The main arguments in favour of the document data model are schema flexibility, better performance due to locality, and sometimes closer data structures to the ones used by the applications. The relation model counters by providing better support for joins, and many-to-one and many-to-many relationships.</p> <p>If the data in your application has a document-like structure, then it's probably a good idea to use a document model. The relational technique of shredding can lead unnecessary complicated application code.</p> <p>The poor support for joins in document databases may or may not be a problem.</p> <p>If you application does use many-to-many relationships, the document model becomes less appealing. Joins can be emulated in application code by making multiple requests. Using the document model can lead to significantly more complex application code and worse performance.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#schema-flexibility","title":"Schema flexibility","text":"<p>Most document databases do not enforce any schema on the data in documents. Arbitrary keys and values can be added to a document, when reading, clients have no guarantees as to what fields the documents may contain.</p> <p>Document databases are sometimes called schemaless, but maybe a more appropriate term is schema-on-read, in contrast to schema-on-write.</p> <p>Schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking.</p> <p>The schema-on-read approach if the items on the collection don't have all the same structure (heterogeneous) * Many different types of objects * Data determined by external systems</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#data-locality-for-queries","title":"Data locality for queries","text":"<p>If your application often needs to access the entire document, there is a performance advantage to this storage locality.</p> <p>The database typically needs to load the entire document, even if you access only a small portion of it. On updates, the entire document usually needs to be rewritten, it is recommended that you keep documents fairly small.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#convergence-of-document-and-relational-databases","title":"Convergence of document and relational databases","text":"<p>PostgreSQL does support JSON documents. RethinkDB supports relational-like joins in its query language and some MongoDB drivers automatically resolve database references. Relational and document databases are becoming more similar over time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#query-languages-for-data","title":"Query languages for data","text":"<p>SQL is a declarative query language. In an imperative language, you tell the computer to perform certain operations in order.</p> <p>In a declarative query language you just specify the pattern of the data you want, but not how to achieve that goal.</p> <p>A declarative query language hides implementation details of the database engine, making it possible for the database system to introduce performance improvements without requiring any changes to queries.</p> <p>Declarative languages often lend themselves to parallel execution while imperative code is very hard to parallelise across multiple cores because it specifies instructions that must be performed in a particular order. Declarative languages specify only the pattern of the results, not the algorithm that is used to determine results.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#declarative-queries-on-the-web","title":"Declarative queries on the web","text":"<p>In a web browser, using declarative CSS styling is much better than manipulating styles imperatively in JavaScript. Declarative languages like SQL turned out to be much better than imperative query APIs.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#mapreduce-querying","title":"MapReduce querying","text":"<p>MapReduce is a programming model for processing large amounts of data in bulk across many machines, popularised by Google.</p> <p>Mongo offers a MapReduce solution.</p> <pre><code>db.observations.mapReduce(\n    function map() { 2\n        var year  = this.observationTimestamp.getFullYear();\n        var month = this.observationTimestamp.getMonth() + 1;\n        emit(year + \"-\" + month, this.numAnimals); 3\n    },\n    function reduce(key, values) { 4\n        return Array.sum(values); 5\n    },\n    {\n        query: { family: \"Sharks\" }, 1\n        out: \"monthlySharkReport\" 6\n    }\n);\n</code></pre> <p>The <code>map</code> and <code>reduce</code> functions must be pure functions, they cannot perform additional database queries and they must not have any side effects. These restrictions allow the database to run the functions anywhere, in any order, and rerun them on failure.</p> <p>A usability problem with MapReduce is that you have to write two carefully coordinated functions. A declarative language offers more opportunities for a query optimiser to improve the performance of a query. For there reasons, MongoDB 2.2 added support for a declarative query language called aggregation pipeline</p> <pre><code>db.observations.aggregate([\n    { $match: { family: \"Sharks\" } },\n    { $group: {\n        _id: {\n            year:  { $year:  \"$observationTimestamp\" },\n            month: { $month: \"$observationTimestamp\" }\n        },\n        totalAnimals: { $sum: \"$numAnimals\" }\n    } }\n]);\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#graph-like-data-models","title":"Graph-like data models","text":"<p>If many-to-many relationships are very common in your application, it becomes more natural to start modelling your data as a graph.</p> <p>A graph consists of vertices (nodes or entities) and edges (relationships or arcs).</p> <p>Well-known algorithms can operate on these graphs, like the shortest path between two points, or popularity of a web page.</p> <p>There are several ways of structuring and querying the data. The property graph model (implemented by Neo4j, Titan, and Infinite Graph) and the triple-store model (implemented by Datomic, AllegroGraph, and others). There are also three declarative query languages for graphs: Cypher, SPARQL, and Datalog.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#property-graphs","title":"Property graphs","text":"<p>Each vertex consists of: * Unique identifier * Outgoing edges * Incoming edges * Collection of properties (key-value pairs)</p> <p>Each edge consists of: * Unique identifier * Vertex at which the edge starts (tail vertex) * Vertex at which the edge ends (head vertex) * Label to describe the kind of relationship between the two vertices * A collection of properties (key-value pairs)</p> <p>Graphs provide a great deal of flexibility for data modelling. Graphs are good for evolvability.</p> <ul> <li>Cypher is a declarative language for property graphs created by Neo4j</li> <li>Graph queries in SQL. In a relational database, you usually know in advance which joins you need in your query. In a graph query, the number if joins is not fixed in advance. In Cypher <code>:WITHIN*0...</code> expresses \"follow a <code>WITHIN</code> edge, zero or more times\" (like the <code>*</code> operator in a regular expression). This idea of variable-length traversal paths in a query can be expressed using something called recursive common table expressions (the <code>WITH RECURSIVE</code> syntax).</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#triple-stores-and-sparql","title":"Triple-stores and SPARQL","text":"<p>In a triple-store, all information is stored in the form of very simple three-part statements: subject, predicate, object (peg: Jim, likes, bananas). A triple is equivalent to a vertex in graph.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-sparql-query-language","title":"The SPARQL query language","text":"<p>SPARQL is a query language for triple-stores using the RDF data model.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-foundation-datalog","title":"The foundation: Datalog","text":"<p>Datalog provides the foundation that later query languages build upon. Its model is similar to the triple-store model, generalised a bit. Instead of writing a triple (subject, predicate, object), we write as predicate(subject, object).</p> <p>We define rules that tell the database about new predicates and rules can refer to other rules, just like functions can call other functions or recursively call themselves.</p> <p>Rules can be combined and reused in different queries. It's less convenient for simple one-off queries, but it can cope better if your data is complex.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#storage-and-retrieval","title":"Storage and retrieval","text":"<p>Databases need to do two things: store the data and give the data back to you.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#data-structures-that-power-up-your-database","title":"Data structures that power up your database","text":"<p>Many databases use a log, which is append-only data file. Real databases have more issues to deal with tho (concurrency control, reclaiming disk space so the log doesn't grow forever and handling errors and partially written records).</p> <p>A log is an append-only sequence of records</p> <p>In order to efficiently find the value for a particular key, we need a different data structure: an index. An index is an additional structure that is derived from the primary data.</p> <p>Well-chosen indexes speed up read queries but every index slows down writes. That's why databases don't index everything by default, but require you to choose indexes manually using your knowledge on typical query patterns.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#hash-indexes","title":"Hash indexes","text":"<p>Key-value stores are quite similar to the dictionary type (hash map or hash table).</p> <p>Let's say our storage consists only of appending to a file. The simplest indexing strategy is to keep an in-memory hash map where every key is mapped to a byte offset in the data file. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote.</p> <p>Bitcask (the default storage engine in Riak) does it like that. The only requirement it has is that all the keys fit in the available RAM. Values can use more space than there is available in memory, since they can be loaded from disk.</p> <p>A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. There are a lot of writes, but there are too many distinct keys, you have a large number of writes per key, but it's feasible to keep all keys in memory.</p> <p>As we only ever append to a file, so how do we avoid eventually running out of disk space? A good solution is to break the log into segments of certain size by closing the segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform compaction on these segments. Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.</p> <p>We can also merge several segments together at the sae time as performing the compaction. Segments are never modified after they have been written, so the merged segment is written to a new file. Merging and compaction of frozen segments can be done in a background thread. After the merging process is complete, we switch read requests to use the new merged segment instead of the old segments, and the old segment files can simply be deleted.</p> <p>Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find a value for a key, we first check the most recent segment hash map; if the key is not present we check the second-most recent segment and so on. The merging process keeps the number of segments small, so lookups don't need to check many hash maps.</p> <p>Some issues that are important in a real implementation: * File format. It is simpler to use binary format. * Deleting records. Append special deletion record to the data file (tombstone) that tells the merging process to discard previous values. * Crash recovery. If restarted, the in-memory hash maps are lost. You can recover from reading each segment but that would take long time. Bitcask speeds up recovery by storing a snapshot of each segment hash map on disk. * Partially written records. The database may crash at any time. Bitcask includes checksums allowing corrupted parts of the log to be detected and ignored. * Concurrency control. As writes are appended to the log in a strictly sequential order, a common implementation is to have a single writer thread. Segments are immutable, so they can be read concurrently by multiple threads.</p> <p>Append-only design turns out to be good for several reasons: * Appending and segment merging are sequential write operations, much faster than random writes, especially on magnetic spinning-disks. * Concurrency and crash recovery are much simpler. * Merging old segments avoids files getting fragmented over time.</p> <p>Hash table has its limitations too: * The hash table must fit in memory. It is difficult to make an on-disk hash map perform well. * Range queries are not efficient.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#sstables-and-lsm-trees","title":"SSTables and LSM-Trees","text":"<p>We introduce a new requirement to segment files: we require that the sequence of key-value pairs is sorted by key.</p> <p>We call this Sorted String Table, or SSTable. We require that each key only appears once within each merged segment file (compaction already ensures that). SSTables have few big advantages over log segments with hash indexes 1. Merging segments is simple and efficient (we can use algorithms like mergesort). When multiple segments contain the same key, we can keep the value from the most recent segment and discard the values in older segments. 2. You no longer need to keep an index of all the keys in memory. For a key like <code>handiwork</code>, when you know the offsets for the keys <code>handback</code> and <code>handsome</code>, you know <code>handiwork</code> must appear between those two. You can jump to the offset for <code>handback</code> and scan from there until you find <code>handiwork</code>, if not, the key is not present. You still need an in-memory index to tell you the offsets for some of the keys. One key for every few kilobytes of segment file is sufficient. 3. Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk.</p> <p>How do we get the data sorted in the first place? With red-black trees or AVL trees, you can insert keys in any order and read them back in sorted order. * When a write comes in, add it to an in-memory balanced tree structure (memtable). * When the memtable gets bigger than some threshold (megabytes), write it out to disk as an SSTable file. Writes can continue to a new memtable instance. * On a read request, try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc. * From time to time, run merging and compaction in the background to discard overwritten and deleted values.</p> <p>If the database crashes, the most recent writes are lost. We can keep a separate log on disk to which every write is immediately appended. That log is not in sorted order, but that doesn't matter, because its only purpose is to restore the memtable after crash. Every time the memtable is written out to an SSTable, the log can be discarded.</p> <p>Storage engines that are based on this principle of merging and compacting sorted files are often called LSM structure engines (Log Structure Merge-Tree).</p> <p>Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a similar method for storing its term dictionary.</p> <p>LSM-tree algorithm can be slow when looking up keys that don't exist in the database. To optimise this, storage engines often use additional Bloom filters (a memory-efficient data structure for approximating the contents of a set).</p> <p>There are also different strategies to determine the order and timing of how SSTables are compacted and merged. Mainly two size-tiered and leveled compaction. LevelDB and RocksDB use leveled compaction, HBase use size-tiered, and Cassandra supports both. In size-tiered compaction, newer and smaller SSTables are successively merged into older and larger SSTables. In leveled compaction, the key range is split up into smaller SSTables and older data is moved into separate \"levels\", which allows the compaction to use less disk space.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#b-trees","title":"B-trees","text":"<p>This is the most widely used indexing structure. B-tress keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries.</p> <p>The log-structured indexes break the database down into variable-size segments typically several megabytes or more. B-trees break the database down into fixed-size blocks or pages, traditionally 4KB.</p> <p>One page is designated as the root and you start from there. The page contains several keys and references to child pages.</p> <p>If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk. If you want to add new key, find the page and add it to the page. If there isn't enough free space in the page to accommodate the new key, it is split in two half-full pages, and the parent page is updated to account for the new subdivision of key ranges.</p> <p>Trees remain balanced. A B-tree with n keys always has a depth of O(log n).</p> <p>The basic underlying write operation of a B-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page, all references to that page remain intact. This is a big contrast to log-structured indexes such as LSM-trees, which only append to files.</p> <p>Some operations require several different pages to be overwritten. When you split a page, you need to write the two pages that were split, and also overwrite their parent. If the database crashes after only some of the pages have been written, you end up with a corrupted index.</p> <p>It is common to include an additional data structure on disk: a write-ahead log (WAL, also know as the redo log).</p> <p>Careful concurrency control is required if multiple threads are going to access, typically done protecting the tree internal data structures with latches (lightweight locks).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#b-trees-and-lsm-trees","title":"B-trees and LSM-trees","text":"<p>LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-tress as they have to check several different data structures and SSTables at different stages of compaction.</p> <p>Advantages of LSM-trees: * LSM-trees are typically able to sustain higher write throughput than B-trees, party because they sometimes have lower write amplification: a write to the database results in multiple writes to disk. The more a storage engine writes to disk, the fewer writes per second it can handle. * LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. B-trees tend to leave disk space unused due to fragmentation.</p> <p>Downsides of LSM-trees: * Compaction process can sometimes interfere with the performance of ongoing reads and writes. B-trees can be more predictable. The bigger the database, the the more disk bandwidth is required for compaction. Compaction cannot keep up with the rate of incoming writes, if not configured properly you can run out of disk space. * On B-trees, each key exists in exactly one place in the index. This offers strong transactional semantics. Transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#other-indexing-structures","title":"Other indexing structures","text":"<p>We've only discussed key-value indexes, which are like primary key index. There are also secondary indexes.</p> <p>A secondary index can be easily constructed from a key-value index. The main difference is that in a secondary index, the indexed values are not necessarily unique. There are two ways of doing this: making each value in the index a list of matching row identifiers or by making a each entry unique by appending a row identifier to it.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#full-text-search-and-fuzzy-indexes","title":"Full-text search and fuzzy indexes","text":"<p>Indexes don't allow you to search for similar keys, such as misspelled words. Such fuzzy querying requires different techniques.</p> <p>Full-text search engines allow synonyms, grammatical variations, occurrences of words near each other.</p> <p>Lucene uses SSTable-like structure for its term dictionary. Lucene, the in-memory index is a finite state automaton, similar to a trie.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#keeping-everything-in-memory","title":"Keeping everything in memory","text":"<p>Disks have two significant advantages: they are durable, and they have lower cost per gigabyte than RAM.</p> <p>It's quite feasible to keep them entirely in memory, this has lead to in-memory databases.</p> <p>Key-value stores, such as Memcached are intended for cache only, it's acceptable for data to be lost if the machine is restarted. Other in-memory databases aim for durability, with special hardware, writing a log of changes to disk, writing periodic snapshots to disk or by replicating in-memory sate to other machines.</p> <p>When an in-memory database is restarted, it needs to reload its state, either from disk or over the network from a replica. The disk is merely used as an append-only log for durability, and reads are served entirely from memory.</p> <p>Products such as VoltDB, MemSQL, and Oracle TimesTime are in-memory databases. Redis and Couchbase provide weak durability.</p> <p>In-memory databases can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk.</p> <p>Another interesting area is that in-memory databases may provide data models that are difficult to implement with disk-based indexes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#transaction-processing-or-analytics","title":"Transaction processing or analytics?","text":"<p>A transaction is a group of reads and writes that form a logical unit, this pattern became known as online transaction processing (OLTP).</p> <p>Data analytics has very different access patterns. A query would need to scan over a huge number of records, only reading a few columns per record, and calculates aggregate statistics.</p> <p>These queries are often written by business analysts, and fed into reports. This pattern became known for online analytics processing (OLAP).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#data-warehousing","title":"Data warehousing","text":"<p>A data warehouse is a separate database that analysts can query to their heart's content without affecting OLTP operations. It contains read-only copy of the dat in all various OLTP systems in the company. Data is extracted out of OLTP databases (through periodic data dump or a continuous stream of update), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse (process Extract-Transform-Load or ETL).</p> <p>A data warehouse is most commonly relational, but the internals of the systems can look quite different.</p> <p>Amazon RedShift is hosted version of ParAccel. Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill. Some of them are based on ideas from Google's Dremel.</p> <p>Data warehouses are used in fairly formulaic style known as a star schema.</p> <p>Facts are captured as individual events, because this allows maximum flexibility of analysis later. The fact table can become extremely large.</p> <p>Dimensions represent the who, what, where, when, how and why of the event.</p> <p>The name \"star schema\" comes from the fact than when the table relationships are visualised, the fact table is in the middle, surrounded by its dimension tables, like the rays of a star.</p> <p>Fact tables often have over 100 columns, sometimes several hundred. Dimension tables can also be very wide.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#column-oriented-storage","title":"Column-oriented storage","text":"<p>In a row-oriented storage engine, when you do a query that filters on a specific field, the engine will load all those rows with all their fields into memory, parse them and filter out the ones that don't meet the requirement. This can take a long time.</p> <p>Column-oriented storage is simple: don't store all the values from one row together, but store all values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in a query, which can save a lot of work.</p> <p>Column-oriented storage often lends itself very well to compression as the sequences of values for each column look quite repetitive, which is a good sign for compression. A technique that is particularly effective in data warehouses is bitmap encoding.</p> <p>Bitmap indexes are well suited for all kinds of queries that are common in a data warehouse.</p> <p>Cassandra and HBase have a concept of column families, which they inherited from Bigtable.</p> <p>Besides reducing the volume of data that needs to be loaded from disk, column-oriented storage layouts are also good for making efficient use of CPU cycles (vectorised processing).</p> <p>Column-oriented storage, compression, and sorting helps to make read queries faster and make sense in data warehouses, where most of the load consist on large read-only queries run by analysts. The downside is that writes are more difficult.</p> <p>An update-in-place approach, like B-tree use, is not possible with compressed columns. If you insert a row in the middle of a sorted table, you would most likely have to rewrite all column files.</p> <p>It's worth mentioning materialised aggregates as some cache of the counts ant the sums that queries use most often. A way of creating such a cache is with a materialised view, on a relational model this is usually called a virtual view: a table-like object whose contents are the results of some query. A materialised view is an actual copy of the query results, written in disk, whereas a virtual view is just a shortcut for writing queries.</p> <p>When the underlying data changes, a materialised view needs to be updated, because it is denormalised copy of the data. Database can do it automatically, but writes would become more expensive.</p> <p>A common special case of a materialised view is know as a data cube or OLAP cube, a grid of aggregates grouped by different dimensions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#encoding-and-evolution","title":"Encoding and evolution","text":"<p>Change to an application's features also requires a change to data it stores.</p> <p>Relational databases conforms to one schema although that schema can be changed, there is one schema in force at any point in time. Schema-on-read (or schemaless) contain a mixture of older and newer data formats.</p> <p>In large applications changes don't happen instantaneously. You want to perform a rolling upgrade and deploy a new version to a few nodes at a time, gradually working your way through all the nodes without service downtime.</p> <p>Old and new versions of the code, and old and new data formats, may potentially all coexist. We need to maintain compatibility in both directions * Backward compatibility, newer code can read data that was written by older code. * Forward compatibility, older code can read data that was written by newer code.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#formats-for-encoding-data","title":"Formats for encoding data","text":"<p>Two different representations: * In memory * When you want to write data to a file or send it over the network, you have to encode it</p> <p>Thus, you need a translation between the two representations. In-memory representation to byte sequence is called encoding (serialisation or marshalling), and the reverse is called decoding (parsing, deserialisation or unmarshalling).</p> <p>Programming languages come with built-in support for encoding in-memory objects into byte sequences, but is usually a bad idea to use them. Precisely because of a few problems. * Often tied to a particular programming language. * The decoding process needs to be able to instantiate arbitrary classes and this is frequently a security hole. * Versioning * Efficiency</p> <p>Standardised encodings can be written and read by many programming languages.</p> <p>JSON, XML, and CSV are human-readable and popular specially as data interchange formats, but they have some subtle problems: * Ambiguity around the encoding of numbers and dealing with large numbers * Support of Unicode character strings, but no support for binary strings. People get around this by encoding binary data as Base64, which increases the data size by 33%. * There is optional schema support for both XML and JSON * CSV does not have any schema</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#binary-encoding","title":"Binary encoding","text":"<p>JSON is less verbose than XML, but both still use a lot of space compared to binary formats. There are binary encodings for JSON (MesagePack, BSON, BJSON, UBJSON, BISON and Smile), similar thing for XML (WBXML and Fast Infoset).</p> <p>Apache Thrift and Protocol Buffers (protobuf) are binary encoding libraries.</p> <p>Thrift offers two different protocols: * BinaryProtocol, there are no field names like <code>userName</code>, <code>favouriteNumber</code>. Instead the data contains field tags, which are numbers (<code>1</code>, <code>2</code>) * CompactProtocol, which is equivalent to BinaryProtocol but it packs the same information in less space. It packs the field type and the tag number into the same byte.</p> <p>Protocol Buffers are very similar to Thrift's CompactProtocol, bit packing is a bit different and that might allow smaller compression.</p> <p>Schemas inevitable need to change over time (schema evolution), how do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility changes?</p> <ul> <li>Forward compatible support. As with new fields you add new tag numbers, old code trying to read new code, it can simply ignore not recognised tags.</li> <li>Backwards compatible support. As long as each field has a unique tag number, new code can always read old data. Every field you add after initial deployment of schema must be optional or have a default value.</li> </ul> <p>Removing fields is just like adding a field with backward and forward concerns reversed. You can only remove a field that is optional, and you can never use the same tag again.</p> <p>What about changing the data type of a field? There is a risk that values will lose precision or get truncated.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#avro","title":"Avro","text":"<p>Apache Avro is another binary format that has two schema languages, one intended for human editing (Avro IDL), and one (based on JSON) that is more easily machine-readable.</p> <p>You go go through the fields in the order they appear in the schema and use the schema to tell you the datatype of each field. Any mismatch in the schema between the reader and the writer would mean incorrectly decoded data.</p> <p>What about schema evolution? When an application wants to encode some data, it encodes the data using whatever version of the schema it knows (writer's schema).</p> <p>When an application wants to decode some data, it is expecting the data to be in some schema (reader's schema).</p> <p>In Avro the writer's schema and the reader's schema don't have to be the same. The Avro library resolves the differences by looking at the writer's schema and the reader's schema.</p> <p>Forward compatibility means you can have a new version of the schema as writer and an old version of the schema as reader. Conversely, backward compatibility means that you can have a new version of the schema as reader and an old version as writer.</p> <p>To maintain compatibility, you may only add or remove a field that has a default value.</p> <p>If you were to add a field that has no default value, new readers wouldn't be able to read data written by old writers.</p> <p>Changing the datatype of a field is possible, provided that Avro can convert the type. Changing the name of a filed is tricky (backward compatible but not forward compatible).</p> <p>The schema is identified encoded in the data. In a large file with lots of records, the writer of the file can just include the schema at the beginning of the file. On a database with individually written records, you cannot assume all the records will have the same schema, so you have to include a version number at the beginning of every encoded record. While sending records over the network, you can negotiate the schema version on connection setup.</p> <p>Avro is friendlier to dynamically generated schemas (dumping into a file the database). You can fairly easily generate an Avro schema in JSON.</p> <p>If the database schema changes, you can just generate a new Avro schema for the updated database schema and export data in the new Avro schema.</p> <p>By contrast with Thrift and Protocol Buffers, every time the database schema changes, you would have to manually update the mappings from database column names to field tags.</p> <p>Although textual formats such as JSON, XML and CSV are widespread, binary encodings based on schemas are also a viable option. As they have nice properties: * Can be much more compact, since they can omit field names from the encoded data. * Schema is a valuable form of documentation, required for decoding, you can be sure it is up to date. * Database of schemas allows you to check forward and backward compatibility changes. * Generate code from the schema is useful, since it enables type checking at compile time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#modes-of-dataflow","title":"Modes of dataflow","text":"<p>Different process on how data flows between processes</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#via-databases","title":"Via databases","text":"<p>The process that writes to the database encodes the data, and the process that reads from the database decodes it.</p> <p>A value in the database may be written by a newer version of the code, and subsequently read by an older version of the code that is still running.</p> <p>When a new version of your application is deployed, you may entirely replace the old version with the new version within a few minutes. The same is not true in databases, the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it. Data outlives code.</p> <p>Rewriting (migrating) is expensive, most relational databases allow simple schema changes, such as adding a new column with a <code>null</code> default value without rewriting existing data. When an old row is read, the database fills in <code>null</code>s for any columns that are missing.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#via-service-calls","title":"Via service calls","text":"<p>You have processes that need to communicate over a network of clients and servers.</p> <p>Services are similar to databases, each service should be owned by one team. and that team should be able to release versions of the service frequently, without having to coordinate with other teams. We should expect old and new versions of servers and clients to be running at the same time.</p> <p>Remote procedure calls (RPC) tries to make a request to a remote network service look the same as calling a function or method in your programming language, it seems convenient at first but the approach is flawed: * A network request is unpredictable * A network request it may return without a result, due a timeout * Retrying will cause the action to be performed multiple times, unless you build a mechanism for deduplication (idempotence). * A network request is much slower than a function call, and its latency is wildly variable. * Parameters need to be encoded into a sequence of bytes that can be sent over the network and becomes problematic with larger objects. * The RPC framework must translate datatypes from one language to another, not all languages have the same types.</p> <p>There is no point trying to make a remote service look too much like a local object in your programming language, because it's a fundamentally different thing.</p> <p>New generation of RPC frameworks are more explicit about the fact that a remote request is different from a local function call. Fiangle and Rest.li use features (promises) to encapsulate asyncrhonous actions.</p> <p>RESTful API has some significant advantages like being good for experimentation and debugging.</p> <p>REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organisation, typically within the same datacenter.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#via-asynchronous-message-passing","title":"Via asynchronous message passing","text":"<p>In an asynchronous message-passing systems, a client's request (usually called a message) is delivered to another process with low latency. The message goes via an intermediary called a message broker (message queue or message-oriented middleware) which stores the message temporarily. This has several advantages compared to direct RPC: * It can act as a buffer if the recipient is unavailable or overloaded * It can automatically redeliver messages to a process that has crashed and prevent messages from being lost * It avoids the sender needing to know the IP address and port number of the recipient (useful in a cloud environment) * It allows one message to be sent to several recipients * Decouples the sender from the recipient</p> <p>The communication happens only in one direction. The sender doesn't wait for the message to be delivered, but simply sends it and then forgets about it (asynchronous).</p> <p>Open source implementations for message brokers are RabbitMQ, ActiveMQ, HornetQ, NATS, and Apache Kafka.</p> <p>One process sends a message to a named queue or topic and the broker ensures that the message is delivered to one or more consumers or subscribers to that queue or topic.</p> <p>Message brokers typically don't enforce a particular data model, you can use any encoding format.</p> <p>An actor model is a programming model for concurrency in a single process. Rather than dealing with threads (and their complications), logic is encapsulated in actors. Each actor typically represent one client or entity, it may have some local state, and it communicates with other actors by sending and receiving asynchronous messages. Message deliver is not guaranteed. Since each actor processes only one message at a time, it doesn't need to worry about threads.</p> <p>In distributed actor frameworks, this programming model is used to scale an application across multiple nodes. It basically integrates a message broker and the actor model into a single framework.</p> <ul> <li>Akka uses Java's built-in serialisation by default, which does not provide forward or backward compatibility. You can replace it with something like Protocol Buffers and the ability to do rolling upgrades.</li> <li>Orleans by default uses custom data encoding format that does not support rolling upgrade deployments.</li> <li>In Erlang OTP it is surprisingly hard to make changes to record schemas.</li> </ul> <p>What happens if multiple machines are involved in storage and retrieval of data?</p> <p>Reasons for distribute a database across multiple machines: * Scalability * Fault tolerance/high availability * Latency, having servers at various locations worldwide</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#replication","title":"Replication","text":"<p>Reasons why you might want to replicate data: * To keep data geographically close to your users * Increase availability * Increase read throughput</p> <p>The difficulty in replication lies in handling changes to replicated data. Popular algorithms for replicating changes between nodes: single-leader, multi-leader, and leaderless replication.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#leaders-and-followers","title":"Leaders and followers","text":"<p>Each node that stores a copy of the database is called a replica.</p> <p>Every write to the database needs to be processed by every replica. The most common solution for this is called leader-based replication (active/passive or master-slave replication). 1. One of the replicas is designated the leader (master or primary). Writes to the database must send requests to the leader. 2. Other replicas are known as followers (read replicas, slaves, secondaries or hot standbys). The leader sends the data change to all of its followers as part of a replication log or change stream. 3. Reads can then query the leader or any of the followers, while writes are only accepted on the leader.</p> <p>MySQL, Oracle Data Guard, SQL Server's AlwaysOn Availability Groups, MongoDB, RethinkDB, Espresso, Kafka and RabbitMQ are examples of these kind of databases.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#synchronous-vs-asynchronous","title":"Synchronous vs asynchronous","text":"<p>The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is that it the synchronous follower doesn't respond, the write cannot be processed.</p> <p>It's impractical for all followers to be synchronous. If you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous. This guarantees up-to-date copy of the data on at least two nodes (this is sometimes called semi-synchronous).</p> <p>Often, leader-based replication is asynchronous. Writes are not guaranteed to be durable, the main advantage of this approach is that the leader can continue processing writes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#setting-up-new-followers","title":"Setting up new followers","text":"<p>Copying data files from one node to another is typically not sufficient.</p> <p>Setting up a follower can usually be done without downtime. The process looks like: 1. Take a snapshot of the leader's database 2. Copy the snapshot to the follower node 3. Follower requests data changes that have happened since the snapshot was taken 4. Once follower processed the backlog of data changes since snapshot, it has caught up.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#handling-node-outages","title":"Handling node outages","text":"<p>How does high availability works with leader-based replication?</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#follower-failure-catchup-recovery","title":"Follower failure: catchup recovery","text":"<p>Follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#leader-failure-failover","title":"Leader failure: failover","text":"<p>One of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader and followers need to start consuming data changes from the new leader.</p> <p>Automatic failover consists: 1. Determining that the leader has failed. If a node does not respond in a period of time it's considered dead. 2. Choosing a new leader. The best candidate for leadership is usually the replica with the most up-to-date changes from the old leader. 3. Reconfiguring the system to use the new leader. The system needs to ensure that the old leader becomes a follower and recognises the new leader.</p> <p>Things that could go wrong: * If asynchronous replication is used, the new leader may have received conflicting writes in the meantime. * Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents. * It could happen that two nodes both believe that they are the leader (split brain). Data is likely to be lost or corrupted. * What is the right time before the leader is declared dead?</p> <p>For these reasons, some operation teams prefer to perform failovers manually, even if the software supports automatic failover.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#implementation-of-replication-logs","title":"Implementation of replication logs","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#statement-based-replication","title":"Statement-based replication","text":"<p>The leader logs every statement and sends it to its followers (every <code>INSERT</code>, <code>UPDATE</code> or <code>DELETE</code>).</p> <p>This type of replication has some problems: * Non-deterministic functions such as <code>NOW()</code> or <code>RAND()</code> will generate different values on replicas. * Statements that depend on existing data, like auto-increments, must be executed in the same order in each replica. * Statements with side effects may result on different results on each replica.</p> <p>A solution to this is to replace any nondeterministic function with a fixed return value in the leader.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#write-ahead-log-wal-shipping","title":"Write-ahead log (WAL) shipping","text":"<p>The log is an append-only sequence of bytes containing all writes to the database. The leader can send it to its followers. This way of replication is used in PostgresSQL and Oracle.</p> <p>The main disadvantage is that the log describes the data at a very low level (like which bytes were changed in which disk blocks), coupling it to the storage engine.</p> <p>Usually is not possible to run different versions of the database in leaders and followers. This can have a big operational impact, like making it impossible to have a zero-downtime upgrade of the database.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#logical-row-based-log-replication","title":"Logical (row-based) log replication","text":"<p>Basically a sequence of records describing writes to database tables at the granularity of a row: * For an inserted row, the new values of all columns. * For a deleted row, the information that uniquely identifies that column. * For an updated row, the information to uniquely identify that row and all the new values of the columns.</p> <p>A transaction that modifies several rows, generates several of such logs, followed by a record indicating that the transaction was committed. MySQL binlog uses this approach.</p> <p>Since logical log is decoupled from the storage engine internals, it's easier to make it backwards compatible.</p> <p>Logical logs are also easier for external applications to parse, useful for data warehouses, custom indexes and caches (change data capture).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#trigger-based-replication","title":"Trigger-based replication","text":"<p>There are some situations were you may need to move replication up to the application layer.</p> <p>A trigger lets you register custom application code that is automatically executed when a data change occurs. This is a good opportunity to log this change into a separate table, from which it can be read by an external process.</p> <p>Main disadvantages is that this approach has greater overheads, is more prone to bugs but it may be useful due to its flexibility.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#problems-with-replication-lag","title":"Problems with replication lag","text":"<p>Node failures is just one reason for wanting replication. Other reasons are scalability and latency.</p> <p>In a read-scaling architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this only realistically works on asynchronous replication. The more nodes you have, the likelier is that one will be down, so a fully synchronous configuration would be unreliable.</p> <p>With an asynchronous approach, a follower may fall behind, leading to inconsistencies in the database (eventual consistency).</p> <p>The replication lag could be a fraction of a second or several seconds or even minutes.</p> <p>The problems that may arise and how to solve them.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#reading-your-own-writes","title":"Reading your own writes","text":"<p>Read-after-write consistency, also known as read-your-writes consistency is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves.</p> <p>How to implement it: * When reading something that the user may have modified, read it from the leader. For example, user profile information on a social network is normally only editable by the owner. A simple rule is always read the user's own profile from the leader. * You could track the time of the latest update and, for one minute after the last update, make all reads from the leader. * The client can remember the timestamp of the most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp. * If your replicas are distributed across multiple datacenters, then any request needs to be routed to the datacenter that contains the leader.</p> <p>Another complication is that the same user is accessing your service from multiple devices, you may want to provide cross-device read-after-write consistency.</p> <p>Some additional issues to consider: * Remembering the timestamp of the user's last update becomes more difficult. The metadata will need to be centralised. * If replicas are distributed across datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. You may need to route requests from all of a user's devices to the same datacenter.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#monotonic-reads","title":"Monotonic reads","text":"<p>Because of followers falling behind, it's possible for a user to see things moving backward in time.</p> <p>When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward.</p> <p>Make sure that each user always makes their reads from the same replica. The replica can be chosen based on a hash of the user ID. If the replica fails, the user's queries will need to be rerouted to another replica.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#consistent-prefix-reads","title":"Consistent prefix reads","text":"<p>If a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.</p> <p>This is a particular problem in partitioned (sharded) databases as there is no global ordering of writes.</p> <p>A solution is to make sure any writes casually related to each other are written to the same partition.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#solutions-for-replication-lag","title":"Solutions for replication lag","text":"<p>Transactions exist so there is a way for a database to provide stronger guarantees so that the application can be simpler.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#multi-leader-replication","title":"Multi-leader replication","text":"<p>Leader-based replication has one major downside: there is only one leader, and all writes must go through it.</p> <p>A natural extension is to allow more than one node to accept writes (multi-leader, master-master or active/active replication) where each leader simultaneously acts as a follower to the other leaders.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#use-cases-for-multi-leader-replication","title":"Use cases for multi-leader replication","text":"<p>It rarely makes sense to use multi-leader setup within a single datacenter.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#multi-datacenter-operation","title":"Multi-datacenter operation","text":"<p>You can have a leader in each datacenter. Within each datacenter, regular leader-follower replication is used. Between datacenters, each datacenter leader replicates its changes to the leaders in other datacenters.</p> <p>Compared to a single-leader replication model deployed in multi-datacenters * Performance. With single-leader, every write must go across the internet to wherever the leader is, adding significant latency. In multi-leader every write is processed in the local datacenter and replicated asynchronously to other datacenters. The network delay is hidden from users and perceived performance may be better. * Tolerance of datacenter outages. In single-leader if the datacenter with the leader fails, failover can promote a follower in another datacenter. In multi-leader, each datacenter can continue operating independently from others. * Tolerance of network problems. Single-leader is very sensitive to problems in this inter-datacenter link as writes are made synchronously over this link. Multi-leader with asynchronous replication can tolerate network problems better.</p> <p>Multi-leader replication is implemented with Tungsten Replicator for MySQL, BDR for PostgreSQL or GoldenGate for Oracle.</p> <p>It's common to fall on subtle configuration pitfalls. Autoincrementing keys, triggers and integrity constraints can be problematic. Multi-leader replication is often considered dangerous territory and avoided if possible.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#clients-with-offline-operation","title":"Clients with offline operation","text":"<p>If you have an application that needs to continue to work while it is disconnected from the internet, every device that has a local database can act as a leader, and there will be some asynchronous multi-leader replication process (imagine, a Calendar application).</p> <p>CouchDB is designed for this mode of operation.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#collaborative-editing","title":"Collaborative editing","text":"<p>Real-time collaborative editing applications allow several people to edit a document simultaneously. Like Etherpad or Google Docs.</p> <p>The user edits a document, the changes are instantly applied to their local replica and asynchronously replicated to the server and any other user.</p> <p>If you want to avoid editing conflicts, you must the lock the document before a user can edit it.</p> <p>For faster collaboration, you may want to make the unit of change very small (like a keystroke) and avoid locking.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#handling-write-conflicts","title":"Handling write conflicts","text":"<p>The biggest problem with multi-leader replication is when conflict resolution is required. This problem does not happen in a single-leader database.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#synchronous-vs-asynchronous-conflict-detection","title":"Synchronous vs asynchronous conflict detection","text":"<p>In single-leader the second writer can be blocked and wait the first one to complete, forcing the user to retry the write. On multi-leader if both writes are successful, the conflict is only detected asynchronously later in time.</p> <p>If you want synchronous conflict detection, you might as well use single-leader replication.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#conflict-avoidance","title":"Conflict avoidance","text":"<p>The simplest strategy for dealing with conflicts is to avoid them. If all writes for a particular record go through the sae leader, then conflicts cannot occur.</p> <p>On an application where a user can edit their own data, you can ensure that requests from a particular user are always routed to the same datacenter and use the leader in that datacenter for reading and writing.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#converging-toward-a-consistent-state","title":"Converging toward a consistent state","text":"<p>On single-leader, the last write determines the final value of the field.</p> <p>In multi-leader, it's not clear what the final value should be.</p> <p>The database must resolve the conflict in a convergent way, all replicas must arrive a the same final value when all changes have been replicated.</p> <p>Different ways of achieving convergent conflict resolution. * Five each write a unique ID (timestamp, long random number, UUID, or a has of the key and value), pick the write with the highest ID as the winner and throw away the other writes. This is known as last write wins (LWW) and it is dangerously prone to data loss. * Give each replica a unique ID, writes that originated at a higher-numbered replica always take precedence. This approach also implies data loss. * Somehow merge the values together. * Record the conflict and write application code that resolves it a to some later time (perhaps prompting the user).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#custom-conflict-resolution","title":"Custom conflict resolution","text":"<p>Multi-leader replication tools let you write conflict resolution logic using application code.</p> <ul> <li>On write. As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler.</li> <li>On read. All the conflicting writes are stored. On read, multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict. CouchDB works this way.</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#multi-leader-replication-topologies","title":"Multi-leader replication topologies","text":"<p>A replication topology describes the communication paths along which writes are propagated from one node to another.</p> <p>The most general topology is all-to-all in which every leader sends its writes to every other leader. MySQL uses circular topology, where each nodes receives writes from one node and forwards those writes to another node. Another popular topology has the shape of a star, one designated node forwards writes to all of the other nodes.</p> <p>In circular and star topologies a write might need to pass through multiple nodes before they reach all replicas. To prevent infinite replication loops each node is given a unique identifier and the replication log tags each write with the identifiers of the nodes it has passed through. When a node fails it can interrupt the flow of replication messages.</p> <p>In all-to-all topology fault tolerance is better as messages can travel along different paths avoiding a single point of failure. It has some issues too, some network links may be faster than others and some replication messages may \"overtake\" others. To order events correctly. there is a technique called version vectors. PostgresSQL BDR does not provide casual ordering of writes, and Tungsten Replicator for MySQL doesn't even try to detect conflicts.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#leaderless-replication","title":"Leaderless replication","text":"<p>Simply put, any replica can directly accept writes from clients. Databases like look like Amazon's in-house Dynamo datastore. Riak, Cassandra and Voldemort follow the Dynamo style.</p> <p>In a leaderless configuration, failover does not exist. Clients send the write to all replicas in parallel.</p> <p>Read requests are also sent to several nodes in parallel. The client may get different responses. Version numbers are used to determine which value is newer.</p> <p>Eventually, all the data is copied to every replica. After a unavailable node come back online, it has two different mechanisms to catch up: * Read repair. When a client detect any stale responses, write the newer value back to that replica. * Anti-entropy process. There is a background process that constantly looks for differences in data between replicas and copies any missing data from one replica to he other. It does not copy writes in any particular order.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#quorums-for-reading-and-writing","title":"Quorums for reading and writing","text":"<p>If there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. As long as w + r &gt; n, we expect to get an up-to-date value when reading. r and w values are called quorum reads and writes. Are the minimum number of votes required for the read or write to be valid.</p> <p>A common choice is to make n and odd number (typically 3 or 5) and to set w = r = (n + 1)/2 (rounded up).</p> <p>Limitations: * Sloppy quorum, the w writes may end up on different nodes than the r reads, so there is no longer a guaranteed overlap. * If two writes occur concurrently, and is not clear which one happened first, the only safe solution is to merge them. Writes can be lost due to clock skew. * If a write happens concurrently with a read, the write may be reflected on only some of the replicas. * If a write succeeded on some replicas but failed on others, it is not rolled back on the replicas where it succeeded. Reads may or may not return the value from that write. * If a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas storing the new value may break the quorum condition.</p> <p>Dynamo-style databases are generally optimised for use cases that can tolerate eventual consistency.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#sloppy-quorums-and-hinted-handoff","title":"Sloppy quorums and hinted handoff","text":"<p>Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads.</p> <p>It's likely that the client won't be able to connect to some database nodes during a network interruption. * Is it better to return errors to all requests for which we cannot reach quorum of w or r nodes? * Or should we accept writes anyway, and write them to some nodes that are reachable but aren't among the n nodes on which the value usually lives?</p> <p>The latter is known as sloppy quorum: writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n \"home\" nodes for a value.</p> <p>Once the network interruption is fixed, any writes are sent to the appropriate \"home\" nodes (hinted handoff).</p> <p>Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#multi-datacenter-operation_1","title":"Multi-datacenter operation","text":"<p>Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#detecting-concurrent-writes","title":"Detecting concurrent writes","text":"<p>In order to become eventually consistent, the replicas should converge toward the same value. If you want to avoid losing data, you application developer, need to know a lot about the internals of your database's conflict handling.</p> <ul> <li>Last write wins (discarding concurrent writes). Even though the writes don' have a natural ordering, we can force an arbitrary order on them. We can attach a timestamp to each write and pick the most recent. There are some situations such caching on which lost writes are acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution.</li> <li>The \"happens-before\" relationship and concurrency. Whether one operation happens before another operation is the key to defining what concurrency means. We can simply say that to operations are concurrent if neither happens before the other. Either A happened before B, or B happened before A, or A and B are concurrent.</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#capturing-the-happens-before-relationship","title":"Capturing the happens-before relationship","text":"<p>The server can determine whether two operations are concurrent by looking at the version numbers. * The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along the value written. * Client reads a key, the server returns all values that have not been overwrite, as well as the latest version number. A client must read a key before writing. * Client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read. * Server receives a write with a particular version number, it can overwrite all values with that version number or below, but it must keep all values with a higher version number.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#merging-concurrently-written-values","title":"Merging concurrently written values","text":"<p>No data is silently dropped. It requires clients do some extra work, they have to clean up afterward by merging the concurrently written values. Riak calls these concurrent values siblings.</p> <p>Merging sibling values is the same problem as conflict resolution in multi-leader replication. A simple approach is to just pick one of the values on a version number or timestamp (last write wins). You may need to do something more intelligent in application code to avoid losing data.</p> <p>If you want to allow people to remove things, union of siblings may not yield the right result. An item cannot simply be deleted from the database when it is removed, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings (tombstone).</p> <p>Merging siblings in application code is complex and error-prone, there are efforts to design data structures that can perform this merging automatically (CRDTs).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#version-vectors","title":"Version vectors","text":"<p>We need a version number per replica as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas.</p> <p>The collection of version numbers from all the replicas is called a version vector.</p> <p>Version vector are sent from the database replicas to clients when values are read, and need to be sent back to the database when a value is subsequently written. Riak calls this casual context. Version vectors allow the database to distinguish between overwrites and concurrent writes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning","title":"Partitioning","text":"<p>Replication, for very large datasets or very high query throughput is not sufficient, we need to break the data up into partitions (sharding).</p> <p>Basically, each partition is a small database of its own.</p> <p>The main reason for wanting to partition data is scalability, query load can be load cabe distributed across many processors. Throughput can be scaled by adding more nodes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning-and-replication","title":"Partitioning and replication","text":"<p>Each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance.</p> <p>A node may store more than one partition.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partition-of-key-value-data","title":"Partition of key-value data","text":"<p>Our goal with partitioning is to spread the data and the query load evenly across nodes.</p> <p>If partition is unfair, we call it skewed. It makes partitioning much less effective. A partition with disproportionately high load is called a hot spot.</p> <p>The simplest approach is to assign records to nodes randomly. The main disadvantage is that if you are trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partition-by-key-range","title":"Partition by key range","text":"<p>Assign a continuous range of keys, like the volumes of a paper encyclopaedia. Boundaries might be chose manually by an administrator, or the database can choose them automatically. On each partition, keys are in sorted order so scans are easy.</p> <p>The downside is that certain access patterns can lead to hot spots.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning-by-hash-of-key","title":"Partitioning by hash of key","text":"<p>A good hash function takes skewed data and makes it uniformly distributed. There is no need to be cryptographically strong (MongoDB uses MD5 and Cassandra uses Murmur3). You can assign each partition a range of hashes. The boundaries can be evenly spaced or they can be chosen pseudorandomly (consistent hashing).</p> <p>Unfortunately we lose the ability to do efficient range queries. Keys that were once adjacent are now scattered across all the partitions. Any range query has to be sent to all partitions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#skewed-workloads-and-relieving-hot-spots","title":"Skewed workloads and relieving hot spots","text":"<p>You can't avoid hot spots entirely. For example, you may end up with large volume of writes to the same key.</p> <p>It's the responsibility of the application to reduce the skew. A simple technique is to add a random number to the beginning or end of the key.</p> <p>Splitting writes across different keys, makes reads now to do some extra work and combine them.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning-and-secondary-indexes","title":"Partitioning and secondary indexes","text":"<p>The situation gets more complicated if secondary indexes are involved. A secondary index usually doesn't identify the record uniquely. They don't map neatly to partitions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning-secondary-indexes-by-document","title":"Partitioning secondary indexes by document","text":"<p>Each partition maintains its secondary indexes, covering only the documents in that partition (local index).</p> <p>You need to send the query to all partitions, and combine all the results you get back (scatter/gather). This is prone to tail latency amplification and is widely used in MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud and VoltDB.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning-secondary-indexes-by-term","title":"Partitioning secondary indexes by term","text":"<p>We construct a global index that covers data in all partitions. The global index must also be partitioned so it doesn't become the bottleneck.</p> <p>It is called the term-partitioned because the term we're looking for determines the partition of the index.</p> <p>Partitioning by term can be useful for range scans, whereas partitioning on a hash of the term gives a more even distribution load.</p> <p>The advantage is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. The downside of a global index is that writes are slower and complicated.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#rebalancing-partitions","title":"Rebalancing partitions","text":"<p>The process of moving load from one node in the cluster to another.</p> <p>Strategies for rebalancing: * How not to do it: Hash mod n. The problem with mod N is that if the number of nodes N changes, most of the keys will need to be moved from one node to another. * Fixed number of partitions. Create many more partitions than there are nodes and assign several partitions to each node. If a node is added to the cluster, we can steal a few partitions from every existing node until partitions are fairly distributed once again. The number of partitions does not change, nor does the assignment of keys to partitions. The only thing that change is the assignment of partitions to nodes. This is used in Riak, Elasticsearch, Couchbase, and Voldemport. You need to choose a high enough number of partitions to accomodate future growth. Neither too big or too small. * Dynamic partitioning. The number of partitions adapts to the total data volume. An empty database starts with an empty partition. While the dataset is small, all writes have to processed by a single node while the others nodes sit idle. HBase and MongoDB allow an initial set of partitions to be configured (pre-splitting). * Partitioning proportionally to nodes. Cassandra and Ketama make the number of partitions proportional to the number of nodes. Have a fixed number of partitions per node. This approach also keeps the size of each partition fairly stable.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#automatic-versus-manual-rebalancing","title":"Automatic versus manual rebalancing","text":"<p>Fully automated rebalancing may seem convenient but the process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.</p> <p>It can be good to have a human in the loop for rebalancing. You may avoid operational surprises.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#request-routing","title":"Request routing","text":"<p>This problem is also called service discovery. There are different approaches: 1. Allow clients to contact any node and make them handle the request directly, or forward the request to the appropriate node. 2. Send all requests from clients to a routing tier first that acts as a partition-aware load balancer. 3. Make clients aware of the partitioning and the assignment of partitions to nodes.</p> <p>In many cases the problem is: how does the component making the routing decision learn about changes in the assignment of partitions to nodes?</p> <p>Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. The routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. HBase, SolrCloud and Kafka use ZooKeeper to track partition assignment. MongoDB relies on its own config server. Cassandra and Riak take a different approach: they use a gossip protocol.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#parallel-query-execution","title":"Parallel query execution","text":"<p>Massively parallel processing (MPP) relational database products are much more sophisticated in the types of queries they support.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#transactions","title":"Transactions","text":"<p>Implementing fault-tolerant mechanisms is a lot of work.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-slippery-concept-of-a-transaction","title":"The slippery concept of a transaction","text":"<p>Transactions have been the mechanism of choice for simplifying these issues. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails (abort, rollback).</p> <p>The application is free to ignore certain potential error scenarios and concurrency issues (safety guarantees).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#acid","title":"ACID","text":"<ul> <li>Atomicity. Is not about concurrency. It is what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. Abortability would have been a better term than atomicity.</li> <li>Consistency. Invariants on your data must always be true. The idea of consistency depends on the application's notion of invariants. Atomicity, isolation, and durability are properties of the database, whereas consistency (in an ACID sense) is a property of the application.</li> <li>Isolation. Concurrently executing transactions are isolated from each other. It's also called serializability, each transaction can pretend that it is the only transaction running on the entire database, and the result is the same as if they had run serially (one after the other).</li> <li>Durability. Once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes. In a single-node database this means the data has been written to nonvolatile storage. In a replicated database it means the data has been successfully copied to some number of nodes.</li> </ul> <p>Atomicity can be implemented using a log for crash recovery, and isolation can be implemented using a lock on each object, allowing only one thread to access an object at any one time.</p> <p>A transaction is a mechanism for grouping multiple operations on multiple objects into one unit of execution.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#handling-errors-and-aborts","title":"Handling errors and aborts","text":"<p>A key feature of a transaction is that it can be aborted and safely retried if an error occurred.</p> <p>In datastores with leaderless replication is the application's responsibility to recover from errors.</p> <p>The whole point of aborts is to enable safe retries.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#weak-isolation-levels","title":"Weak isolation levels","text":"<p>Concurrency issues (race conditions) come into play when one transaction reads data that is concurrently modified by another transaction, or when two transactions try to simultaneously modify the same data.</p> <p>Databases have long tried to hide concurrency issues by providing transaction isolation.</p> <p>In practice, is not that simple. Serializable isolation has a performance cost. It's common for systems to use weaker levels of isolation, which protect against some concurrency issues, but not all.</p> <p>Weak isolation levels used in practice:</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#read-committed","title":"Read committed","text":"<p>It makes two guarantees: 1. When reading from the database, you will only see data that has been committed (no dirty reads). Writes by a transaction only become visible to others when that transaction commits. 2. When writing to the database, you will only overwrite data that has been committed (no dirty writes). Dirty writes are prevented usually by delaying the second write until the first write's transaction has committed or aborted.</p> <p>Most databases prevent dirty writes by using row-level locks that hold the lock until the transaction is committed or aborted. Only one transaction can hold the lock for any given object.</p> <p>On dirty reads, requiring read locks does not work well in practice as one long-running write transaction can force many read-only transactions to wait. For every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#snapshot-isolation-and-repeatable-read","title":"Snapshot isolation and repeatable read","text":"<p>There are still plenty of ways in which you can have concurrency bugs when using this isolation level.</p> <p>Nonrepeatable read or read skew, when you read at the same time you committed a change you may see temporal and inconsistent results.</p> <p>There are some situations that cannot tolerate such temporal inconsistencies: * Backups. During the time that the backup process is running, writes will continue to be made to the database. If you need to restore from such a backup, inconsistencies can become permanent. * Analytic queries and integrity checks. You may get nonsensical results if they observe parts of the database at different points in time.</p> <p>Snapshot isolation is the most common solution. Each transaction reads from a consistent snapshot of the database.</p> <p>The implementation of snapshots typically use write locks to prevent dirty writes.</p> <p>The database must potentially keep several different committed versions of an object (multi-version concurrency control or MVCC).</p> <p>Read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction.</p> <p>How do indexes work in a multi-version database? One option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction.</p> <p>Snapshot isolation is called serializable in Oracle, and repeatable read in PostgreSQL and MySQL.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#preventing-lost-updates","title":"Preventing lost updates","text":"<p>This might happen if an application reads some value from the database, modifies it, and writes it back. If two transactions do this concurrently, one of the modifications can be lost (later write clobbers the earlier write).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#atomic-write-operations","title":"Atomic write operations","text":"<p>A solution for this it to avoid the need to implement read-modify-write cycles and provide atomic operations such us</p> <pre><code>UPDATE counters SET value = value + 1 WHERE key = 'foo';\n</code></pre> <p>MongoDB provides atomic operations for making local modifications, and Redis provides atomic operations for modifying data structures.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#explicit-locking","title":"Explicit locking","text":"<p>The application explicitly lock objects that are going to be updated.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#automatically-detecting-lost-updates","title":"Automatically detecting lost updates","text":"<p>Allow them to execute in parallel, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.</p> <p>MySQL/InnoDB's repeatable read does not detect lost updates.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#compare-and-set","title":"Compare-and-set","text":"<p>If the current value does not match with what you previously read, the update has no effect.</p> <pre><code>UPDATE wiki_pages SET content = 'new content'\n  WHERE id = 1234 AND content = 'old content';\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#conflict-resolution-and-replication","title":"Conflict resolution and replication","text":"<p>With multi-leader or leaderless replication, compare-and-set do not apply.</p> <p>A common approach in replicated databases is to allow concurrent writes to create several conflicting versions of a value (also know as siblings), and to use application code or special data structures to resolve and merge these versions after the fact.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#write-skew-and-phantoms","title":"Write skew and phantoms","text":"<p>Imagine Alice and Bob are two on-call doctors for a particular shift. Imagine both the request to leave because they are feeling unwell. Unfortunately they happen to click the button to go off call at approximately the same time.</p> <pre><code>ALICE                                   BOB\n\n\u250c\u2500 BEGIN TRANSACTION                    \u250c\u2500 BEGIN TRANSACTION\n\u2502                                       \u2502\n\u251c\u2500 currently_on_call = (                \u251c\u2500 currently_on_call = (\n\u2502   select count(*) from doctors        \u2502    select count(*) from doctors\n\u2502   where on_call = true                \u2502    where on_call = true\n\u2502   and shift_id = 1234                 \u2502    and shift_id = 1234\n\u2502  )                                    \u2502  )\n\u2502  // now currently_on_call = 2         \u2502  // now currently_on_call = 2\n\u2502                                       \u2502\n\u251c\u2500 if (currently_on_call  2) {          \u2502\n\u2502    update doctors                     \u2502\n\u2502    set on_call = false                \u2502\n\u2502    where name = 'Alice'               \u2502\n\u2502    and shift_id = 1234                \u251c\u2500 if (currently_on_call &gt;= 2) {\n\u2502  }                                    \u2502    update doctors\n\u2502                                       \u2502    set on_call = false\n\u2514\u2500 COMMIT TRANSACTION                   \u2502    where name = 'Bob'  \n                                        \u2502    and shift_id = 1234\n                                        \u2502  }\n                                        \u2502\n                                        \u2514\u2500 COMMIT TRANSACTION\n</code></pre> <p>Since database is using snapshot isolation, both checks return 2. Both transactions commit, and now no doctor is on call. The requirement of having at least one doctor has been violated.</p> <p>Write skew can occur if two transactions read the same objects, and then update some of those objects. You get a dirty write or lost update anomaly.</p> <p>Ways to prevent write skew are a bit more restricted: * Atomic operations don't help as things involve more objects. * Automatically prevent write skew requires true serializable isolation. * The second-best option in this case is probably to explicitly lock the rows that the transaction depends on.   ```sql   BEGIN TRANSACTION;</p> <p>SELECT * FROM doctors   WHERE on_call = true   AND shift_id = 1234 FOR UPDATE;</p> <p>UPDATE doctors   SET on_call = false   WHERE name = 'Alice'   AND shift_id = 1234;</p> <p>COMMIT;   ```</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#serializability","title":"Serializability","text":"<p>This is the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without concurrency. Basically, the database prevents all possible race conditions.</p> <p>There are three techniques for achieving this: * Executing transactions in serial order * Two-phase locking * Serializable snapshot isolation.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#actual-serial-execution","title":"Actual serial execution","text":"<p>The simplest way of removing concurrency problems is to remove concurrency entirely and execute only one transaction at a time, in serial order, on a single thread. This approach is implemented by VoltDB/H-Store, Redis and Datomic.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#encapsulating-transactions-in-stored-procedures","title":"Encapsulating transactions in stored procedures","text":"<p>With interactive style of transaction, a lot of time is spent in network communication between the application and the database.</p> <p>For this reason, systems with single-threaded serial transaction processing don't allow interactive multi-statement transactions. The application must submit the entire transaction code to the database ahead of time, as a stored procedure, so all the data required by the transaction is in memory and the procedure can execute very fast.</p> <p>There are a few pros and cons for stored procedures: * Each database vendor has its own language for stored procedures. They usually look quite ugly and archaic from today's point of view, and they lack the ecosystem of libraries. * It's harder to debug, more awkward to keep in version control and deploy, trickier to test, and difficult to integrate with monitoring.</p> <p>Modern implementations of stored procedures include general-purpose programming languages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis uses Lua.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioning_1","title":"Partitioning","text":"<p>Executing all transactions serially limits the transaction throughput to the speed of a single CPU.</p> <p>In order to scale to multiple CPU cores you can potentially partition your data and each partition can have its own transaction processing thread. You can give each CPU core its own partition.</p> <p>For any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions. They will be vastly slower than single-partition transactions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#two-phase-locking-2pl","title":"Two-phase locking (2PL)","text":"<p>Two-phase locking (2PL) sounds similar to two-phase commit (2PC) but be aware that they are completely different things.</p> <p>Several transactions are allowed to concurrently read the same object as long as nobody is writing it. When somebody wants to write (modify or delete) an object, exclusive access is required.</p> <p>Writers don't just block other writers; they also block readers and vice versa. It protects against all the race conditions discussed earlier.</p> <p>Blocking readers and writers is implemented by a having lock on each object in the database. The lock is used as follows: * if a transaction want sot read an object, it must first acquire a lock in shared mode. * If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. * If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. * After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). First phase is when the locks are acquired, second phase is when all the locks are released.</p> <p>It can happen that transaction A is stuck waiting for transaction B to release its lock, and vice versa (deadlock).</p> <p>The performance for transaction throughput and response time of queries are significantly worse under two-phase locking than under weak isolation.</p> <p>A transaction may have to wait for several others to complete before it can do anything.</p> <p>Databases running 2PL can have unstable latencies, and they can be very slow at high percentiles. One slow transaction, or one transaction that accesses a lot of data and acquires many locks can cause the rest of the system to halt.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#predicate-locks","title":"Predicate locks","text":"<p>With phantoms, one transaction may change the results of another transaction's search query.</p> <p>In order to prevent phantoms, we need a predicate lock. Rather than a lock belonging to a particular object, it belongs to all objects that match some search condition.</p> <p>Predicate locks applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#index-range-locks","title":"Index-range locks","text":"<p>Predicate locks do not perform well. Checking for matching locks becomes time-consuming and for that reason most databases implement index-range locking.</p> <p>It's safe to simplify a predicate by making it match a greater set of objects.</p> <p>These locks are not as precise as predicate locks would be, but since they have much lower overheads, they are a good compromise.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#serializable-snapshot-isolation-ssi","title":"Serializable snapshot isolation (SSI)","text":"<p>It provides full serializability and has a small performance penalty compared to snapshot isolation. SSI is fairly new and might become the new default in the future.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#pesimistic-versus-optimistic-concurrency-control","title":"Pesimistic versus optimistic concurrency control","text":"<p>Two-phase locking is called pessimistic concurrency control because if anything might possibly go wrong, it's better to wait.</p> <p>Serial execution is also pessimistic as is equivalent to each transaction having an exclusive lock on the entire database.</p> <p>Serializable snapshot isolation is optimistic concurrency control technique. Instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. The database is responsible for checking whether anything bad happened. If so, the transaction is aborted and has to be retried.</p> <p>If there is enough spare capacity, and if contention between transactions is not too high, optimistic concurrency control techniques tend to perform better than pessimistic ones.</p> <p>SSI is based on snapshot isolation, reads within a transaction are made from a consistent snapshot of the database. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.</p> <p>The database knows which transactions may have acted on an outdated premise and need to be aborted by: * Detecting reads of a stale MVCC object version. The database needs to track when a transaction ignores another transaction's writes due to MVCC visibility rules. When a transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted. * Detecting writes that affect prior reads. As with two-phase locking, SSI uses index-range locks except that it does not block other transactions. When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. It simply notifies the transactions that the data they read may no longer be up to date.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#performance-of-serializable-snapshot-isolation","title":"Performance of serializable snapshot isolation","text":"<p>Compared to two-phase locking, the big advantage of SSI is that one transaction doesn't need to block waiting for locks held by another transaction. Writers don't block readers, and vice versa.</p> <p>Compared to serial execution, SSI is not limited to the throughput of a single CPU core. Transactions can read and write data in multiple partitions while ensuring serializable isolation.</p> <p>The rate of aborts significantly affects the overall performance of SSI. SSI requires that read-write transactions be fairly short (long-running read-only transactions may be okay).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-trouble-with-distributed-systems","title":"The trouble with distributed systems","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#faults-and-partial-failures","title":"Faults and partial failures","text":"<p>A program on a single computer either works or it doesn't. There is no reason why software should be flaky (non deterministic).</p> <p>In a distributed systems we have no choice but to confront the messy reality of the physical world. There will be parts that are broken in an unpredictable way, while others work. Partial failures are nondeterministic. Things will unpredicably fail.</p> <p>We need to accept the possibility of partial failure and build fault-tolerant mechanism into the software. We need to build a reliable system from unreliable components.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#unreliable-networks","title":"Unreliable networks","text":"<p>Focusing on shared-nothing systems the network is the only way machines communicate.</p> <p>The internet and most internal networks are asynchronous packet networks. A message is sent and the network gives no guarantees as to when it will arrive, or whether it will arrive at all. Things that could go wrong: 1. Request lost 2. Request waiting in a queue to be delivered later 3. Remote node may have failed 4. Remote node may have temporarily stoped responding 5. Response has been lost on the network 6. The response has been delayed and will be delivered later</p> <p>If you send a request to another node and don't receive a response, it is impossible to tell why.</p> <p>The usual way of handling this issue is a timeout: after some time you give up waiting and assume that the response is not going to arrive.</p> <p>Nobody is immune to network problems. You do need to know how your software reacts to network problems to ensure that the system can recover from them. It may make sense to deliberately trigger network problems and test the system's response.</p> <p>If you want to be sure that a request was successful, you need a positive response from the application itself.</p> <p>If something has gone wrong, you have to assume that you will get no response at all.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#timeouts-and-unbounded-delays","title":"Timeouts and unbounded delays","text":"<p>A long timeout means a long wait until a node is declared dead. A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead (when it could be a slowdown).</p> <p>Premature declaring a node is problematic, if the node is actually alive the action may end up being performed twice.</p> <p>When a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and the network.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#network-congestion-and-queueing","title":"Network congestion and queueing","text":"<ul> <li>Different nodes try to send packets simultaneously to the same destination, the network switch must queue them and feed them to the destination one by one. The switch will discard packets when filled up.</li> <li>If CPU cores are busy, the request is queued by the operative system, until applications are ready to handle it.</li> <li>In virtual environments, the operative system is often paused while another virtual machine uses a CPU core. The VM queues the incoming data.</li> <li>TCP performs flow control, in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node. This means additional queuing at the sender.</li> </ul> <p>You can choose timeouts experimentally by measuring the distribution of network round-trip times over an extended period.</p> <p>Systems can continually measure response times and their variability (jitter), and automatically adjust timeouts according to the observed response time distribution.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#synchronous-vs-ashynchronous-networks","title":"Synchronous vs ashynchronous networks","text":"<p>A telephone network estabilishes a circuit, we say is synchronous even as the data passes through several routers as it does not suffer from queing. The maximum end-to-end latency of the network is fixed (bounded delay).</p> <p>A circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established, whereas packets of a TCP connection opportunistically use whatever network bandwidth is available.</p> <p>Using circuits for bursty data transfers wastes network capacity and makes transfer unnecessary slow. By contrast, TCP dinamycally adapts the rate of data transfer to the available network capacity.</p> <p>We have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there's no \"correct\" value for timeouts, they need to be determined experimentally.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#unreliable-clocks","title":"Unreliable clocks","text":"<p>The time when a message is received is always later than the time when it is sent, we don't know how much later due to network delays. This makes difficult to determine the order of which things happened when multiple machines are involved.</p> <p>Each machine on the network has its own clock, slightly faster or slower than the other machines. It is possible to synchronise clocks with Network Time Protocol (NTP).</p> <ul> <li>Time-of-day clocks. Return the current date and time according to some calendar (wall-clock time). If the local clock is toof ar ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. This makes it is unsuitable for measuring elapsed time.</li> <li>Monotonic clocks. Peg: <code>System.nanoTime()</code>. They are guaranteed to always move forward. The difference between clock reads can tell you how much time elapsed beween two checks. The absolute value of the clock is meaningless. NTP allows the clock rate to be speeded up or slowed down by up to 0.05%, but NTP cannot cause the monotonic clock to jump forward or backward. In a distributed system, using a monotonic clock for measuring elapsed time (peg: timeouts), is usually fine.</li> </ul> <p>If some piece of sofware is relying on an accurately synchronised clock, the result is more likely to be silent and subtle data loss than a dramatic crash.</p> <p>You need to carefully monitor the clock offsets between all the machines.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#timestamps-for-ordering-events","title":"Timestamps for ordering events","text":"<p>It is tempting, but dangerous to rely on clocks for ordering of events across multiple nodes. This usually imply that last write wins (LWW), often used in both multi-leader replication and leaderless databases like Cassandra and Riak, and data-loss may happen.</p> <p>The definition of \"recent\" also depends on local time-of-day clock, which may well be incorrect.</p> <p>Logical clocks, based on counters instead of oscillating quartz crystal, are safer alternative for ordering events. Logical clocks do not measure time of the day or elapsed time, only relative ordering of events. This contrasts with time-of-the-day and monotic clocks (also known as physical clocks).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#clock-readings-have-a-confidence-interval","title":"Clock readings have a confidence interval","text":"<p>It doesn't make sense to think of a clock reading as a point in time, it is more like a range of times, within a confidence internval: for example, 95% confident that the time now is between 10.3 and 10.5.</p> <p>The most common implementation of snapshot isolation requires a monotonically increasing transaction ID.</p> <p>Spanner implements snapshot isolation across datacenters by using clock's confidence interval. If you have two confidence internvals where</p> <pre><code>A = [A earliest, A latest]\nB = [B earliest, B latest]\n</code></pre> <p>And those two intervals do not overlap (<code>A earliest</code> &lt; <code>A latest</code> &lt; <code>B earliest</code> &lt; <code>B latest</code>), then B definetively happened after A.</p> <p>Spanner deliberately waits for the length of the confidence interval before commiting a read-write transaction, so their confidence intervals do not overlap.</p> <p>Spanner needs to keep the clock uncertainty as small as possible, that's why Google deploys a GPS receiver or atomic clock in each datacenter.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#process-pauses","title":"Process pauses","text":"<p>How does a node know that it is still leader?</p> <p>One option is for the leader to obtain a lease from other nodes (similar ot a lock with a timeout). It will be the leader until the lease expires; to remain leader, the node must periodically renew the lease. If the node fails, another node can takeover when it expires.</p> <p>We have to be very careful making assumptions about the time that has passed for processing requests (and holding the lease), as there are many reasons a process would be paused: * Garbage collector (stop the world) * Virtual machine can be suspended * In laptops execution may be suspended * Operating system context-switches * Synchronous disk access * Swapping to disk (paging) * Unix process can be stopped (<code>SIGSTOP</code>)</p> <p>You cannot assume anything about timing</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#response-time-guarantees","title":"Response time guarantees","text":"<p>There are systems that require software to respond before a specific deadline (real-time operating system, or RTOS).</p> <p>Library functions must document their worst-case execution times; dynamic memory allocation may be restricted or disallowed and enormous amount of testing and measurement must be done.</p> <p>Garbage collection could be treated like brief planned outages. If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node and perform GC while no requests are in progress.</p> <p>A variant of this idea is to use the garbage collector only for short-lived objects and to restart the process periodically.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#knowledge-truth-and-lies","title":"Knowledge, truth and lies","text":"<p>A node cannot necessarily trust its own judgement of a situation. Many distributed systems rely on a quorum (voting among the nodes).</p> <p>Commonly, the quorum is an absolute majority of more than half of the nodes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#fencing-tokens","title":"Fencing tokens","text":"<p>Assume every time the lock server grant sa lock or a lease, it also returns a fencing token, which is a number that increases every time a lock is granted (incremented by the lock service). Then we can require every time a client sends a write request to the storage service, it must include its current fencing token.</p> <p>The storage server remembers that it has already processed a write with a higher token number, so it rejects the request with the last token.</p> <p>If ZooKeeper is used as lock service, the transaciton ID <code>zcid</code> or the node version <code>cversion</code> can be used as a fencing token.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#byzantine-faults","title":"Byzantine faults","text":"<p>Fencing tokens can detect and block a node that is inadvertently acting in error.</p> <p>Distributed systems become much harder if there is a risk that nodes may \"lie\" (byzantine fault).</p> <p>A system is Byzantine fault-tolerant if it continues to operate correctly even if some of the nodes are malfunctioning. * Aerospace environments * Multiple participating organisations, some participants may attempt ot cheat or defraud others</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#consistency-and-consensus","title":"Consistency and consensus","text":"<p>The simplest way of handling faults is to simply let the entire service fail. We need to find ways of tolerating faults.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#consistency-guarantees","title":"Consistency guarantees","text":"<p>Write requests arrive on different nodes at different times.</p> <p>Most replicated databases provide at least eventual consistency. The inconsistency is temporary, and eventually resolves itself (convergence).</p> <p>With weak guarantees, you need to be constantly aware of its limitations. Systems with stronger guarantees may have worse performance or be less fault-tolerant than systems with weaker guarantees.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#linearizability","title":"Linearizability","text":"<p>Make a system appear as if there were only one copy of the data, and all operaitons on it are atomic.</p> <ul> <li><code>read(x) =&gt; v</code> Read from register x, database returns value v.</li> <li><code>write(x,v) =&gt; r</code> r could be ok or error.</li> </ul> <p>If one client read returns the new value, all subsequent reads must also return the new value.</p> <ul> <li><code>cas(x_old, v_old, v_new) =&gt; r</code> an atomic compare-and-set operation. If the value of the register x equals v_old, it is atomically set to v_new. If <code>x != v_old</code> the registers is unchanged and it returns an error.</li> </ul> <p>Serializability: Transactions behave the same as if they had executed some serial order.</p> <p>Linearizability: Recency guarantee on reads and writes of a register (individual object).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#locking-and-leader-election","title":"Locking and leader election","text":"<p>To ensure that there is indeed only one leader, a lock is used. It must be linearizable: all nodes must agree which nodes owns the lock; otherwise is useless.</p> <p>Apache ZooKeepr and etcd are often used for distributed locks and leader election.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#constraints-and-uniqueness-guarantees","title":"Constraints and uniqueness guarantees","text":"<p>Unique constraints, like a username or an email address require a situation similiar to a lock.</p> <p>A hard uniqueness constraint in relational databases requires linearizability.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#implementing-linearizable-systems","title":"Implementing linearizable systems","text":"<p>The simplest approach would be to have a single copy of the data, but this would not be able to tolerate faults.</p> <ul> <li>Single-leader repolication is potentially linearizable.</li> <li>Consensus algorithms is linearizable.</li> <li>Multi-leader replication is not linearizable.</li> <li>Leaderless replication is probably not linearizable.</li> </ul> <p>Multi-leader replication is often a good choice for multi-datacenter replication. On a network interruption betwen data-centers will force a choice between linearizability and availability.</p> <p>With multi-leader configuraiton, each data center can operate normally with interruptions.</p> <p>With single-leader replication, the leader must be in one of the datacenters. If the application requires linearizable reads and writes, the network interruption causes the application to become unavailable.</p> <ul> <li> <p>If your applicaiton requires linearizability, and some replicas are disconnected from the other replicas due to a network problem, the some replicas cannot process request while they are disconnected (unavailable).</p> </li> <li> <p>If your application does not require, then it can be written in a way tha each replica can process requests independently, even if it is disconnected from other replicas (peg: multi-leader), becoming available.</p> </li> </ul> <p>If an application does not require linearizability it can be more tolerant of network problems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-unhelpful-cap-theorem","title":"The unhelpful CAP theorem","text":"<p>CAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. Or being said in another way either Consistency or Available when Partitioned.</p> <p>CAP only considers one consistency model (linearizability) and one kind of fault (network partitions, or nodes that are alive but disconnected from each other). It doesn't say anything about network delays, dead nodes, or other trade-offs. CAP has been historically influential, but nowadays has little practical value for designing systems.</p> <p>The main reason for dropping linearizability is performance, not fault tolerance. Linearizabilit is slow and this is true all the time, not on only during a network fault.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#ordering-guarantees","title":"Ordering guarantees","text":"<p>Cause comes before the effect. Causal order in the system is what happened before what (causally consistent).</p> <p>Total order allows any two elements to be compared. Peg, natural numbers are totally ordered.</p> <p>Some cases one set is greater than another one.</p> <p>Different consistency models:</p> <ul> <li>Linearizablity. total order of operations: if the system behaves as if there is only a single copy of the data.</li> <li>Causality. Two events are ordered if they are causally related. Causality defines a partial order, not a total one (incomparable if they are concurrent).</li> </ul> <p>Linearizability is not the only way of preserving causality. Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p> <p>You need to know which operation happened before.</p> <p>In order to determine the causal ordering, the database needs to know which version of the data was read by the application. The version number from the prior operation is passed back to the database on a write.</p> <p>We can create sequence numbers in a total order that is consistent with causality.</p> <p>With a single-leader replication, the leader can simply increment a counter for each operation, and thus assign a monotonically increasing sequence number to each operation in the replication log.</p> <p>If there is not a single leader (multi-leader or leaderless database): * Each node can generate its own independent set of sequence numbers. One node can generate only odd numbers and the other only even numbers. * Attach a timestamp from a time-of-day clock. * Preallocate blocks of sequence numbers.</p> <p>The only problem is that the sequence numbers they generate are not consistent with causality. They do not correctly capture ordering of operations across different nodes.</p> <p>There is simple method for generating sequence numbers that is consistent with causality: Lamport timestamps.</p> <p>Each node has a unique identifier, and each node keeps a counter of the number of operations it has processed. The lamport timestamp is then simply a pair of (counter, node ID). It provides total order, as if you have two timestamps one with a greater counter value is the greater timestamp. If the counter values are the same, the one with greater node ID is the greater timestamp.</p> <p>Every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request. When a node receives a request of response with a maximum counter value greater than its own counter value, it inmediately increases its own counter to that maximum.</p> <p>As long as the maximum counter value is carried along with every operation, this scheme  ensure that the ordering from the lamport timestamp is consistent with causality.</p> <p>Total order of oepration only emerges after you have collected all of the operations.</p> <p>Total order broadcast: * Reliable delivery: If a message is delivered to one node, it is delivered to all nodes. * Totally ordered delivery: Mesages are delivered to every node in the same order.</p> <p>ZooKeeper and etcd implement total order broadcast.</p> <p>If every message represents a write to the database, and every replica processes the same writes in the same order, then the replcias will remain consistent with each other (state machine replication).</p> <p>A node is not allowed to retroactgively insert a message into an earlier position in the order if subsequent messages have already been dlivered.</p> <p>Another way of looking at total order broadcast is that it is a way of creating a log. Delivering a message is like appending to the log.</p> <p>If you have total order broadcast, you can build linearizable storage on top of it.</p> <p>Because log entries are delivered to all nodes in the same order, if therer are several concurrent writes, all nodes will agree on which one came first. Choosing the first of the conflicting writes as the winner and aborting later ones ensures that all nodes agree on whether a write was commited or aborted.</p> <p>This procedure ensures linearizable writes, it doesn't guarantee linearizable reads.</p> <p>To make reads linearizable: * You can sequence reads through the log by appending a message, reading the log, and performing the actual read when the message is delivered back to you (etcd works something like this). * Fetch the position of the latest log message in a linearizable way, you can query that position to be delivered to you, and then perform the read (idea behind ZooKeeper's <code>sync()</code>). * You can make your read from a replica that is synchronously updated on writes.</p> <p>For every message you want to send through total order broadcast, you increment-and-get the linearizable integer and then attach the value you got from the register as a sequence number to the message. YOu can send the message to all nodes, and the recipients will deliver the message consecutively by sequence number.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#distributed-transactions-and-consensus","title":"Distributed transactions and consensus","text":"<p>Basically getting several nodes to agree on something.</p> <p>There are situations in which it is important for nodes to agree: * Leader election: All nodes need to agree on which node is the leader. * Atomic commit: Get all nodes to agree on the outcome of the transacction, either they all abort or roll back.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#atomic-commit-and-two-phase-commit-2pc","title":"Atomic commit and two-phase commit (2PC)","text":"<p>A transaction either succesfully commit, or abort. Atomicity prevents half-finished results.</p> <p>On a single node, transaction commitment depends on the order in which data is writen to disk: first the data, then the commit record.</p> <p>2PC uses a coordinartor (transaction manager). When the application is ready to commit, the coordinator begins phase 1: it sends a prepare request to each of the nodes, asking them whether are able to commit.</p> <ul> <li>If all participants reply \"yes\", the coordinator sends out a commit request in phase 2, and the commit takes place.</li> <li>If any of the participants replies \"no\", the coordinator sends an abort request to all nodes in phase 2.</li> </ul> <p>When a participant votes \"yes\", it promises that it will definitely be able to commit later; and once the coordiantor decides, that decision is irrevocable. Those promises ensure the atomicity of 2PC.</p> <p>If one of the participants or the network fails during 2PC (prepare requests fail or time out), the coordinator aborts the transaction. If any of the commit or abort request fail, the coordinator retries them indefinitely.</p> <p>If the coordinator fails before sending the prepare requests, a participant can safely abort the transaction.</p> <p>The only way 2PC can complete is by waiting for the coordinator to revover in case of failure. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort requests to participants.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#three-phase-commit","title":"Three-phase commit","text":"<p>2PC is also called a blocking atomic commit protocol, as 2Pc can become stuck waiting for the coordinator to recover.</p> <p>There is an alternative called three-phase commit (3PC) that requires a perfect failure detector.</p> <p>Distributed transactions carry a heavy performance penalty due the disk forcing in 2PC required for crash recovery and additional network round-trips.</p> <p>XA (X/Open XA for eXtended Architecture) is a standard for implementing two-phase commit across heterogeneous technologies. Supported by many traditional relational databases (PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and message brokers (ActiveMQ, HornetQ, MSQMQ, and IBM MQ).</p> <p>The problem with locking is that database transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty writes.</p> <p>While those locks are held, no other transaction can modify those rows.</p> <p>When a coordinator fails, orphaned in-doubt transactions do ocurr, and the only way out is for an administrator to manually decide whether to commit or roll back the transaction.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#fault-tolerant-consensus","title":"Fault-tolerant consensus","text":"<p>One or more nodes may propose values, and the consensus algorithm decides on those values.</p> <p>Consensus algorithm must satisfy the following properties: * Uniform agreement: No two nodes decide differently. * Integrity: No node decides twice. * Validity: If a node decides the value v, then v was proposed by some node. * Termination: Every node that does not crash eventually decides some value.</p> <p>If you don't care about fault tolerance, then satisfying the first three properties is easy: you can just hardcode one node to be the \"dictator\" and let that node make all of the decisions.</p> <p>The termination property formalises the idea of fault tolerance. Even if some nodes fail, the other nodes must still reach a decision. Termination is a liveness property, whereas the other three are safety properties.</p> <p>The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft and Zab.</p> <p>Total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes.</p> <p>So total order broadcast is equivalent to repeated rounds of consensus: * Due to agreement property, all nodes decide to deliver the same messages in the same order. * Due to integrity, messages are not duplicated. * Due to validity, messages are not corrupted. * Due to termination, messages are not lost.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#single-leader-replication-and-consensus","title":"Single-leader replication and consensus","text":"<p>All of the consensus protocols dicussed so far internally use a leader, but they don't guarantee that the lader is unique. Protocols define an epoch number (ballot number in Paxos, view number in Viewstamped Replication, and term number in Raft). Within each epoch, the leader is unique.</p> <p>Every time the current leader is thought to be dead, a vote is started among the nodes to elect a new leader. This election is given an incremented epoch number, and thus epoch numbers are totallly ordered and monotonically increasing. If there is a conflic, the leader with the higher epoch number prevails.</p> <p>A node cannot trust its own judgement. It must collect votes from a quorum of nodes. For every decision that a leader wants to make, it must send the proposed value to the other nodes and wait for a quorum of nodes to respond in favor of the proposal.</p> <p>There are two rounds of voting, once to choose a leader, and second time to vote on a leader's proposal. The quorums for those two votes must overlap.</p> <p>The biggest difference with 2PC, is that 2PC requires a \"yes\" vote for every participant.</p> <p>The benefits of consensus come at a cost. The process by which nodes vote on proposals before they are decided is kind of synchronous replication.</p> <p>Consensus always require a strict majority to operate.</p> <p>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can't just add or remove nodes in the cluster. Dynamic membership extensions are much less well understood than static membership algorithms.</p> <p>Consensus systems rely on timeouts to detect failed nodes. In geographically distributed systems, it often happens that a node falsely believes the leader to have failed due to a network issue. This implies frequest leader elecctions resulting in terrible performance, spending more time choosing a leader than doing any useful work.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#membership-and-coordination-services","title":"Membership and coordination services","text":"<p>ZooKeeper or etcd are often described as \"distributed key-value stores\" or \"coordination and configuration services\".</p> <p>They are designed to hold small amounts of data that can fit entirely in memory, you wouldn't want to store all of your application's data here. Data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm.</p> <p>ZooKeeper is modeled after Google's Chubby lock service and it provides some useful features: * Linearizable atomic operations: Usuing an atomic compare-and-set operation, you can implement a lock. * Total ordering of operations: When some resource is protected by a lock or lease, you need a fencing token to prevent clients from conflicting with each other in the case of a process pause. The fencing token is some number that monotonically increases every time the lock is acquired. * Failure detection: Clients maintain a long-lived session on ZooKeeper servers. When a ZooKeeper node fails, the session remains active. When ZooKeeper declares the session to be dead all locks held are automatically released. * Change notifications: Not only can one client read locks and values, it can also watch them for changes.</p> <p>ZooKeeper is super useful for distributed coordination.</p> <p>ZooKeeper/Chubby model works well when you have several instances of a process or service, and one of them needs to be chosen as a leader or primary. If the leader fails, one of the other nodes should take over. This is useful for single-leader databases and for job schedulers and similar stateful systems.</p> <p>ZooKeeper runs on a fixed number of nodes, and performs its majority votes among those nodes while supporting a potentially large number of clients.</p> <p>The kind of data managed by ZooKeeper is quite slow-changing like \"the node running on 10.1.1.23 is the leader for partition 7\". It is not intended for storing the runtime state of the application. If application state needs to be replicated there are other tools (like Apache BookKeeper).</p> <p>ZooKeeper, etcd, and Consul are also often used for service discovery, find out which IP address you need to connect to in order to reach a particular service. In cloud environments, it is common for virtual machines to continually come an go, you often don't know the IP addresses of your services ahead of time. Your services when they start up they register their network endpoints ina  service registry, where they can then be found by other services.</p> <p>ZooKeeper and friends can be seen as part of a long history of research into membership services, determining which nodes are currently active and live members of a cluster.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#batch-processing","title":"Batch processing","text":"<ul> <li>Service (online): waits for a request, sends a response back</li> <li>Batch processing system (offline): takes a large amount of input data, runs a job to process it, and produces some output.</li> <li>Stream processing systems (near-real-time): a stream processor consumes input and produces outputs. A stream job operates on events shortly after they happen.</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#batch-processing-with-unix-tools","title":"Batch processing with Unix tools","text":"<p>We can build a simple log analysis job to get the five most popular pages on your site</p> <pre><code>cat /var/log/nginx/access.log |\n  awk '{print $7}' |\n  sort             |\n  uniq -c          |\n  sort -r -n       |\n  head -n 5        |\n</code></pre> <p>You could write the same thing with a simpel program.</p> <p>The difference is that with Unix commands automatically handle larger-than-memory datasets and automatically paralelizes sorting across multiple CPU cores.</p> <p>Programs must have the same data format to pass information to one another. In Unix, that interface is a file (file descriptor), an ordered sequence of bytes.</p> <p>By convention Unix programs treat this sequence of bytes as ASCII text.</p> <p>The unix approach works best if a program simply uses <code>stdin</code> and <code>stdout</code>. This allows a shell user to wire up the input and output in whatever way they want; the program doesn't know or care where the input is coming from and where the output is going to.</p> <p>Part of what makes Unix tools so successful is that they make it quite easy to see what is going on.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#map-reduce-and-distributed-filesystems","title":"Map reduce and distributed filesystems","text":"<p>A single MapReduce job is comparable to a single Unix process.</p> <p>Running a MapReduce job normally does not modify the input and does not have any side effects other than producing the output.</p> <p>While Unix tools use <code>stdin</code> and <code>stdout</code> as input and output, MapReduce jobs read and write files on a distributed filesystem. In Hadoop, that filesystem is called HDFS (Haddoop Distributed File System).</p> <p>HDFS is based on the shared-nothing principe. Implemented by centralised storage appliance, often using custom hardware and special network infrastructure.</p> <p>HDFS consists of a daemon process running on each machine, exposing a network service that allows other nodes to access files stored on that machine. A central server called the NameNode keeps track of which file blocks are stored on which machine.</p> <p>File blocks are replciated on multiple machines. Reaplication may mean simply several copies of the same data on multiple machines, or an erasure coding scheme such as Reed-Solomon codes, which allow lost data to be recovered.</p> <p>MapReduce is a programming framework with which you can write code to process large datasets in a distributed filesystem like HDFS. 1. Read a set of input files, and break it up into records. 2. Call the mapper function to extract a key and value from each input record. 3. Sort all of the key-value pairs by key. 4. Call the reducer function to iterate over the sorted key-value pairs.</p> <ul> <li>Mapper: Called once for every input record, and its job is to extract the key and value from the input record.</li> <li>Reducer: Takes the key-value pairs produced by the mappers, collects all the values belonging to the same key, and calls the reducer with an interator over that collection of vaues.</li> </ul> <p>MapReduce can parallelise a computation across many machines, without you having ot write code to explicitly handle the parallelism. THe mapper and reducer only operate on one record at a time; they don't need to know where their input is coming from or their output is going to.</p> <p>In Hadoop MapReduce, the mapper and reducer are each a Java class that implements a particular interface.</p> <p>The MapReduce scheduler tries to run each mapper on one of the machines that stores a replica of the input file, putting the computation near the data.</p> <p>The reduce side of the computation is also partitioned. While the number of map tasks is determined by the number of input file blocks, the number of reduce tasks is configured by the job author. To ensure that all key-value pairs with the same key end up in the same reducer, the framework uses a hash of the key.</p> <p>The dataset is likely too large to be sorted with a conventional sorting algorithm on a single machine. Sorting is performed in stages.</p> <p>Whenever a mapper finishes reading its input file and writing its sorted output files, the MapReduce scheduler notifies the reducers that they can start fetching the output files from that mapper. The reducers connect to each of the mappers and download the files of sorted key-value pairs for their partition. Partitioning by reducer, sorting and copying data partitions from mappers to reducers is called shuffle.</p> <p>The reduce task takes the files from the mappers and merges them together, preserving the sort order.</p> <p>MapReduce jobs can be chained together into workflows, the output of one job becomes the input to the next job. In Hadoop this chaining is done implicitly by directory name: the first job writes its output to a designated directory in HDFS, the second job reads that same directory name as its input.</p> <p>Compared with the Unix example, it could be seen as in each sequence of commands each command output is written to a temporary file, and the next command reads from the temporary file.</p> <p>It is common in datasets for one record to have an association with another record: a foreign key in a relational model, a document reference in a document model, or an edge in graph model.</p> <p>If the query involves joins, it may require multiple index lookpus. MapReduce has no concept of indexes.</p> <p>When a MapReduce job is given a set of files as input, it reads the entire content of all of those files, like a full table scan.</p> <p>In analytics it is common to want to calculate aggregates over a large number of records. Scanning the entire input might be quite reasonable.</p> <p>In order to achieve good throughput in a batch process, the computation must be local to one machine. Requests over the network are too slow and nondeterministic. Queries to other database for example would be prohibitive.</p> <p>A better approach is to take a copy of the data (peg: the database) and put it in the same distributed filesystem.</p> <p>MapReduce programming model has separated the physical network communication aspects of the computation (getting the data to the right machine) from the application logic (processing the data once you have it).</p> <p>In an example of a social network, small number of celebrities may have many millions of followers. Such disproportionately active database records are known as linchpin objects or hot keys.</p> <p>A single reducer can lead to significant skew that is, one reducer that must process significantly more records than the others.</p> <p>The skewed join method in Pig first runs a sampling job to determine which keys are hot and then records related to the hot key need to be replicated to all reducers handling that key.</p> <p>Handling the hot key over several reducers is called shared join method. In Crunch is similar but requires the hot keys to be specified explicitly.</p> <p>Hive's skewed join optimisation requries hot keys to be specified explicitly and it uses map-side join. If you can make certain assumptions about your input data, it is possible to make joins faster. A MapReducer job with no reducers and no sorting, each mapper simply reads one input file and writes one output file.</p> <p>The output of a batch process is often not a report, but some other kind of structure.</p> <p>Google's original use of MapReduce was to build indexes for its search engine. Hadoop MapReduce remains a good way of building indexes for Lucene/Solr.</p> <p>If you need to perform a full-text search, a batch process is very effective way of building indexes: the mappers partition the set of documents as needed, each reducer builds the index for its partition, and the index files are written to the distributed filesystem. It pararellises very well.</p> <p>Machine learning systems such as clasifiers and recommendation systems are a common use for batch processing.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#key-value-stores-as-batch-process-output","title":"Key-value stores as batch process output","text":"<p>The output of those batch jobs is often some kind of database.</p> <p>So, how does the output from the batch process get back into a database?</p> <p>Writing from the batch job directly to the database server is a bad idea: * Making a network request for every single record is magnitude slower than the normal throughput of a batch task. * Mappers or reducers concurrently write to the same output database an it can be easily overwhelmed. * You have to worry about the results from partially completed jobs being visible to other systems.</p> <p>A much better solution is to build a brand-new database inside the batch job an write it as files to the job's output directory, so it can be loaded in bulk into servers that handle read-only queries. Various key-value stores support building database files in MapReduce including Voldemort, Terrapin, ElephanDB and HBase bulk loading.</p> <p>By treating inputs as immutable and avoiding side effects (such as writing to external databases), batch jobs not only achieve good performance but also become much easier to maintain.</p> <p>Design principles that worked well for Unix also seem to be working well for Hadoop.</p> <p>The MapReduce paper was not at all new. The sections we've seen had been already implemented in so-called massively parallel processing (MPP) databases.</p> <p>The biggest difference is that MPP databases focus on parallel execution of analytic SQL queries on a cluster of machines, while the combination of MapReduce and a distributed filesystem provides something much more like a general-purpose operating system that can run arbitraty programs.</p> <p>Hadoop opened up the possibility of indiscriminately dumpint data into HDFS. MPP databases typically require careful upfront modeling of the data and query patterns before importing data into the database's proprietary storage format.</p> <p>In MapReduce instead of forcing the producer of a dataset to bring it into a standarised format, the interpretation of the data becomes the consumer's problem.</p> <p>If you have HDFS and MapReduce, you can build a SQL query execution engine on top of it, and indeed this is what the Hive project did.</p> <p>If a node crashes while a query is executing, most MPP databases abort the entire query. MPP databases also prefer to keep as much data as possible in memory.</p> <p>MapReduce can tolerate the failure of a map or reduce task without it affecting the job. It is also very eager to write data to disk, partly for fault tolerance, and partly because the dataset might not fit in memory anyway.</p> <p>MapReduce is more appropriate for larger jobs.</p> <p>At Google, a MapReduce task that runs for an hour has an approximately 5% risk of being terminated to make space for higher-priority process.</p> <p>Ths is why MapReduce is designed to tolerate frequent unexpected task termination.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#beyond-mapreduce","title":"Beyond MapReduce","text":"<p>In response to the difficulty of using MapReduce directly, various higher-level programming models emerged on top of it: Pig, Hive, Cascading, Crunch.</p> <p>MapReduce has poor performance for some kinds of processing. It's very robust, you can use it to process almost arbitrarily large quantities of data on an unreliable multi-tenant system with frequent task terminations, and it will still get the job done.</p> <p>The files on the distributed filesystem are simply intermediate state: a means of passing data from one job to the next.</p> <p>The process of writing out the intermediate state to files is called materialisation.</p> <p>MapReduce's approach of fully materialising state has some downsides compared to Unix pipes:</p> <ul> <li>A MapReduce job can only start when all tasks in the preceding jobs have completed, whereas rocesses connected by a Unix pipe are started at the same time.</li> <li>Mappers are often redundant: they just read back the same file that was just written by a reducer.</li> <li>Files are replicated across several nodes, which is often overkill for such temporary data.</li> </ul> <p>To fix these problems with MapReduce, new execution engines for distributed batch computations were developed, Spark, Tez and Flink. These new ones can handle an entire workflow as one job, rather than breaking it up into independent subjobs (dataflow engines).</p> <p>These functions need not to take the strict roles of alternating map and reduce, they are assembled in flexible ways, in functions called operators.</p> <p>Spark, Flink, and Tex avoid writing intermediate state to HDFS, so they take a different approach to tolerating faults: if a machine fails and the intermediate state on that machine is lost, it is recomputed from other data that is still available.</p> <p>The framework must keep track of how a given piece of data was computed. Spark uses the resilient distributed dataset (RDD) to track ancestry data, while Flink checkpoints operator state, allowing it to resume running an operator that ran into a fault during its execution.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#graphs-and-iterative-processing","title":"Graphs and iterative processing","text":"<p>It's interesting to look at graphs in batch processing context, where the goal is to perform some kind of offline processing or analysis on an entire graph. This need often arises in machine learning applications such as recommednation engines, or in ranking systems.</p> <p>\"repeating until done\" cannot be expressed in plain MapReduce as it runs in a single pass over the data and some extra trickery is necessary.</p> <p>An optimisation for batch processing graphs, the bulk synchronous parallel (BSP) has become popular. It is implemented by Apache Giraph, Spark's GraphX API, and Flink's Gelly API (_Pregel model, as Google Pregel paper popularised it).</p> <p>One vertex can \"send a message\" to another vertex, and typically those messages are sent along the edges in a graph.</p> <p>The difference from MapReduce is that a vertex remembers its state in memory from one iteration to the next.</p> <p>The fact that vertices can only communicate by message passing helps improve the performance of Pregel jobs, since messages can be batched.</p> <p>Fault tolerance is achieved by periodically checkpointing the state of all vertices at the end of an interation.</p> <p>The framework may partition the graph in arbitrary ways.</p> <p>Graph algorithms often have a lot of cross-machine communication overhead, and the intermediate state is often bigger than the original graph.</p> <p>If your graph can fit into memory on a single computer, it's quite likely that a single-machine algorithm will outperform a distributed batch process. If the graph is too big to fit on a single machine, a distributed approach such as Pregel is unavoidable.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#stream-processing","title":"Stream processing","text":"<p>We can run the processing continuously, abandoning the fixed time slices entirely and simply processing every event as it happens, that's the idea behind stream processing. Data that is incrementally made available over time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#transmitting-event-streams","title":"Transmitting event streams","text":"<p>A record is more commonly known as an event. Something that happened at some point in time, it usually contains a timestamp indicating when it happened acording to a time-of-day clock.</p> <p>An event is generated once by a producer (publisher or sender), and then potentially processed by multiple consumers (subcribers or recipients). Related events are usually grouped together into a topic or a stream.</p> <p>A file or a database is sufficient to connect producers and consumers: a producer writes every event that it generates to the datastore, and each consumer periodically polls the datastore to check for events that have appeared since it last ran.</p> <p>However, when moving toward continual processing, polling becomes expensive. It is better for consumers to be notified when new events appear.</p> <p>Databases offer triggers but they are limited, so specialised tools have been developed for the purpose of delivering event notifications.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#messaging-systems","title":"Messaging systems","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#direct-messaging-from-producers-to-consumers","title":"Direct messaging from producers to consumers","text":"<p>Within the publish/subscribe model, we can differentiate the systems by asking two questions: 1. What happens if the producers send messages faster than the consumers can process them? The system can drop messages, buffer the messages in a queue, or apply backpressure (flow control, blocking the producer from sending more messages). 2. What happens if nodes crash or temporarily go offline, are any messages lost? Durability may require some combination of writing to disk and/or replication.</p> <p>A number of messaging systems use direct communication between producers and consumers without intermediary nodes: * UDP multicast, where low latency is important, application-level protocols can recover lost packets. * Brokerless messaging libraries such as ZeroMQ * StatsD and Brubeck use unreliable UDP messaging for collecting metrics * If the consumer expose a service on the network, producers can make a direct HTTP or RPC request to push messages to the consumer. This is the idea behind webhooks, a callback URL of one service is registered with another service, and makes a request to that URL whenever an event occurs</p> <p>These direct messaging systems require the application code to be aware of the possibility of message loss. The faults they can tolerate are quite limited as they assume that producers and consumers are constantly online.</p> <p>If a consumer if offline, it may miss messages. Some protocols allow the producer to retry failed message deliveries, but it may break down if the producer crashes losing the buffer or messages.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#message-brokers","title":"Message brokers","text":"<p>An alternative is to send messages via a message broker (or message queue), which is a kind of database that is optimised for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. Producers write messages to the broker, and consumers receive them by reading them from the broker.</p> <p>By centralising the data, these systems can easily tolerate clients that come and go, and the question of durability is moved to the broker instead. Some brokers only keep messages in memory, while others write them down to disk so that they are not lost inc ase of a broker crash.</p> <p>A consequence of queueing is that consuemrs are generally asynchronous: the producer only waits for the broker to confirm that it has buffered the message and does not wait for the message to be processed by consumers.</p> <p>Some brokers can even participate in two-phase commit protocols using XA and JTA. This makes them similar to databases, aside some practical differences: * Most message brokers automatically delete a message when it has been successfully delivered to its consumers. This makes them not suitable for long-term storage. * Most message brokers assume that their working set is fairly small. If the broker needs to buffer a lot of messages, each individual message takes longer to process, and the overall throughput may degrade. * Message brokers often support some way of subscribing to a subset of topics matching some pattern. * Message brokers do not support arbitrary queries, but they do notify clients when data changes.</p> <p>This is the traditional view of message brokers, encapsulated in standards like JMS and AMQP, and implemented in RabbitMQ, ActiveMQ, HornetQ, Qpid, TIBCO Enterprise Message Service, IBM MQ, Azure Service Bus, and Google Cloud Pub/Sub.</p> <p>When multiple consumers read messages in the same topic, to main patterns are used: * Load balancing: Each message is delivered to one of the consumers. The broker may assign messages to consumers arbitrarily. * Fan-out: Each message is delivered to all of the consumers.</p> <p>In order to ensure that the message is not lost, message brokers use acknowledgements: a client must explicitly tell the broker when it has finished processing a message so that the broker can remove it from the queue.</p> <p>The combination of laod balancing with redelivery inevitably leads to messages being reordered. To avoid this issue, youc an use a separate queue per consumer (not use the load balancing feature).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#partitioned-logs","title":"Partitioned logs","text":"<p>A key feature of barch process is that you can run them repeatedly without the risk of damaging the input. This is not the case with AMQP/JMS-style messaging: receiving a message is destructive if the acknowledgement causes it to be deleted from the broker.</p> <p>If you add a new consumer to a messaging system, any prior messages are already gone and cannot be recovered.</p> <p>We can have a hybrid, combining the durable storage approach of databases with the low-latency notifications facilities of messaging, this is the idea behind log-based message brokers.</p> <p>A log is simply an append-only sequence of records on disk. The same structure can be used to implement a message broker: a producer sends a message by appending it to the end of the log, and consumer receives messages by reading the log sequentially. If a consumer reaches the end of the log, it waits for a notification that a new message has been appended.</p> <p>To scale to higher throughput than a single disk can offer, the log can be partitioned. Different partitions can then be hosted on different machines. A topic can then be defined as a group of partitions that all carry messages of the same type.</p> <p>Within each partition, the broker assigns monotonically increasing sequence number, or offset, to every message.</p> <p>Apache Kafka, Amazon Kinesis Streams, and Twitter's DistributedLog, are log-based message brokers that work like this.</p> <p>The log-based approach trivially supports fan-out messaging, as several consumers can independently read the log reading without affecint each other. Reading a message does not delete it from the log. To eachieve load balancing the broker can assign entire partitions to nodes in the consumer group. Each client then consumes all the messages in the partition it has been assigned. This approach has some downsides. * The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic. * If a single message is slow to process, it holds up the processing of subsequent messages in that partition.</p> <p>In situations where messages may be expensive to process and you want to pararellise processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. In situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.</p> <p>It is easy to tell which messages have been processed: al messages with an offset less than a consumer current offset have already been processed, and all messages with a greater offset have not yet been seen.</p> <p>The offset is very similar to the log sequence number that is commonly found in single-leader database replication. The message broker behaves like a leader database, and the consumer like a follower.</p> <p>If a consumer node fails, another node in the consumer group starts consuming messages at the last recorded offset. If the consumer had processed subsequent messages but not yet recorded their offset, those messages will be processed a second time upon restart.</p> <p>If you only ever append the log, you will eventually run out of disk space. From time to time old segments are deleted or moved to archive.</p> <p>If a slow consumer cannot keep with the rate of messages, and it falls so far behind that its consumer offset poitns to a deleted segment, it will miss some of the messages.</p> <p>The throughput of a log remains more or less constant, since every message is written to disk anyway. This is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: systems are fast when queues are short and become much slower when they start writing to disk, throughput depends on the amount of history retained.</p> <p>If a consumer cannot keep up with producers, the consumer can drop messages, buffer them or applying backpressure.</p> <p>You can monitor how far a consumer is behind the head of the log, and raise an alert if it falls behind significantly.</p> <p>If a consumer does fall too far behind and start missing messages, only that consumer is affected.</p> <p>With AMQP and JMS-style message brokers, processing and acknowledging messages is a destructive operation, since it causes the messages to be deleted on the broker. In a log-based message broker, consuming messages is more like reading from a file.</p> <p>The offset is under the consumer's control, so you can easily be manipulated if necessary, like for replaying old messages.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#databases-and-streams","title":"Databases and streams","text":"<p>A replciation log is a stream of a database write events, produced by the leader as it processes transactions. Followers apply that stream of writes to their own copy of the database and thus end up with an accurate copy of the same data.</p> <p>If periodic full database dumps are too slow, an alternative that is sometimes used is dual writes. For example, writing to the database, then updating the search index, then invalidating the cache.</p> <p>Dual writes have some serious problems, one of which is race conditions. If you have concurrent writes, one value will simply silently overwrite another value.</p> <p>One of the writes may fail while the other succeeds and two systems will become inconsistent.</p> <p>The problem with most databases replication logs is that they are considered an internal implementation detail, not a public API.</p> <p>Recently there has been a growing interest in change data capture (CDC), which is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other systems.</p> <p>For example, you can capture the changes in a database and continually apply the same changes to a search index.</p> <p>We can call log consumers derived data systems: the data stored in the search index and the data warehouse is just another view. Change data capture is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems.</p> <p>Change data capture makes one database the leader, and turns the others into followers.</p> <p>Database triggers can be used to implement change data capture, but they tend to be fragile and have significant performance overheads. Parsing the replication log can be a more robust approach.</p> <p>LinkedIn's Databus, Facebook's Wormhole, and Yahoo!'s Sherpa use this idea at large scale. Bottled Watter implements CDC for PostgreSQL decoding the write-ahead log, Maxwell and Debezium for something similar for MySQL by parsing the binlog, Mongoriver reads the MongoDB oplog, and GoldenGate provide similar facilities for Oracle.</p> <p>Keeping all changes forever would require too much disk space, and replaying it would take too long, so the log needs to be truncated.</p> <p>You can start with a consistent snapshot of the database, and it must correspond to a known position or offset in the change log.</p> <p>The storage engine periodically looks for log records with the same key, throws away any duplicates, and keeps only the most recent update for each key.</p> <p>An update with a special null value (a tombstone) indicates that a key was deleted.</p> <p>The same idea works in the context of log-based mesage brokers and change data capture.</p> <p>RethinkDB allows queries to subscribe to notifications, Firebase and CouchDB provide data synchronisation based on change feed.</p> <p>Kafka Connect integrates change data capture tools for a wide range of database systems with Kafka.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#event-sourcing","title":"Event sourcing","text":"<p>There are some parallels between the ideas we've discussed here and event sourcing.</p> <p>Similarly to change data capture, event sourcing involves storing all changes to the application state as a log of change events. Event sourcing applyies the idea at a different level of abstraction.</p> <p>Event sourcing makes it easier to evolve applications over time, helps with debugging by making it easier to understand after the fact why something happened, and guards against application bugs.</p> <p>Specialised databases such as Event Store have been developed to support applications using event sourcing.</p> <p>Applications that use event sourcing need to take the log of evetns and transform it into application state that is suitable for showing to a user.</p> <p>Replying the event log allows you to reconstruct the current state of the system.</p> <p>Applications that use event sourcing typically have some mechanism for storing snapshots.</p> <p>Event sourcing philosophy is careful to distinguis between events and commands. When a request from a user first arrives, it is initially a command: it may still fail (like some integrity condition is violated). If the validation is successful, it becomes an event, which is durable and immutable.</p> <p>A consumer of the event stream is not allowed to reject an event: Any validation of a command needs to happen synchronously, before it becomes an event. For example, by using a serializable transaction that atomically validates the command and publishes the event.</p> <p>Alternatively, the user request to serve a seat could be split into two events: first a tentative reservation, and then a separate confirmation event once the reservation has been validated. This split allows the validation to take place in an asynchronous process.</p> <p>Whenever you have state changes, that state is the result of the events that mutated it over time.</p> <p>Mutable state and an append-only log of immutable events do not contradict each other.</p> <p>As an example, financial bookkeeping is recorded as an append-only ledger. It is a log of events describing money, good, or services that have changed hands. Profit and loss or the balance sheet are derived from the ledger by adding them up.</p> <p>If a mistake is made, accountants don't erase or change the incorrect transaction, instead, they add another transaction that compensates for the mistake.</p> <p>If buggy code writes bad data to a database, recovery is much harder if the code is able to destructively overwrite data.</p> <p>Immutable events also capture more information than just the current state. If you persisted a cart into a regular database, deleting an item would effectively loose that event.</p> <p>You can derive views from the same event log, Druid ingests directly from Kafka, Pistachio is a distributed key-value sotre that uses Kafka as a commit log, Kafka Connect sinks can export data from Kafka to various different databases and indexes.</p> <p>Storing data is normally quite straightforward if you don't have to worry about how it is going to be queried and accessed. You gain a lot of flexibility by separating the form in which data is written from the form it is read, this idea is known as command query responsibility segregation (CQRS).</p> <p>There is this fallacy that data must be written in the same form as it will be queried.</p> <p>The biggest downside of event sourcing and change data capture is that consumers of the event log are usually asynchronous, a user may make a write to the log, then read from a log derived view and find that their write has not yet been reflected.</p> <p>The limitations on immutable event history depends on the amount of churn in the dataset. Some workloads mostly add data and rarely update or delete; they are wasy to make immutable. Other workloads have a high rate of updates and deletes on a comparaively small dataset; in these cases immutable history becomes an issue because of fragmentation, performance compaction and garbage collection.</p> <p>There may also be circumstances in which you need data to be deleted for administrative reasons.</p> <p>Sometimes you may want to rewrite history, Datomic calls this feature excision.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#processing-streams","title":"Processing Streams","text":"<p>What you can do with the stream once you have it: 1. You can take the data in the events and write it to the database, cache, search index, or similar storage system, from where it can thenbe queried by other clients. 2. You can push the events to users in some way, for example by sending email alerts or push notifications, or to a real-time dashboard. 3. You can process one or more input streams to produce one or more output streams.</p> <p>Processing streams to produce other, derived streams is what an operator job does. The one crucial difference to batch jobs is that a stream never ends.</p> <p>Complex event processing (CEP) is an approach for analising event streams where you can specify rules to search for certain patterns of events in them.</p> <p>When a match is found, the engine emits a complex event.</p> <p>Queries are stored long-term, and events from the input streams continuously flow past them in search of a query that matches an event pattern.</p> <p>Implementations of CEP include Esper, IBM InfoSphere Streams, Apama, TIBCO StreamBase, and SQLstream.</p> <p>The boundary between CEP and stream analytics is blurry, analytics tends to be less interested in finding specific event sequences and is more oriented toward aggregations and statistical metrics.</p> <p>Frameworks with analytics in mind are: Apache Storm, Spark Streaming, Flink, Concord, Samza, and Kafka Streams. Hosted services include Google Cloud Dataflow and Azure Stream Analytics.</p> <p>Sometimes there is a need to search for individual events continually, such as full-text search queries over streams.</p> <p>Message-passing ystems are also based on messages and events, we normally don't think of them as stream processors.</p> <p>There is some crossover area between RPC-like systems and stream processing. Apache Storm has a feature called distributed RPC.</p> <p>In a batch process, the time at which the process is run has nothing to do with the time at which the events actually occurred.</p> <p>Many stream processing frameworks use the local system clock on the processing machine (processing time) to determine windowing. It is a simple approach that breaks down if there is any significant processing lag.</p> <p>Confusing event time and processing time leads to bad data. Processing time may be unreliable as the stream processor may queue events, restart, etc. It's better to take into account the original event time to count rates.</p> <p>You can never be sure when you have received all the events.</p> <p>You can time out and declare a window ready after you have not seen any new events for a while, but it could still happen that some events are delayed due a network interruption. You need to be able to handle such stranggler events that arrive after the window has already been declared complete.</p> <ol> <li>You can ignore the stranggler events, tracking the number of dropped events as a metric.</li> <li>Publish a correction, an updated value for the window with stranglers included. You may also need to retrat the previous output.</li> </ol> <p>To adjust for incofrrect device clocks, one approach is to log three timestamps: * The time at which the event occurred, according to the device clock * The time at which the event was sent to the server, according to the device clock * The time at which the event was received by the server, according to the server clock.</p> <p>You can estimate the offset between the device clock and the server clock, then apply that offset to the event timestamp, and thus estimate the true time at which the event actually ocurred.</p> <p>Several types of windows are in common use: * Tumbling window: Fixed length. If you have a 1-minute tumbling window, all events between 10:03:00 and 10:03:59 will be grouped in one window, next window would be 10:04:00-10:04:59 * Hopping window: Fixed length, but allows windows to overlap in order to provide some smoothing. If you have a 5-minute window with a hop size of 1 minute, it would contain the events between 10:03:00 and 10:07:59, next window would cover 10:04:00-10:08:59 * Sliding window: Events that occur within some interval of each other. For example, a 5-minute sliding window would cover 10:03:39 and 10:08:12 because they are less than 4 minutes apart. * Session window: No fixed duration. All events for the same user, the window ends when the user has been inactive for some time (30 minutes). Common in website analytics</p> <p>The fact that new events can appear anytime on a stream makes joins on stream challenging.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#stream-stream-joins","title":"Stream-stream joins","text":"<p>You want to detect recent trends in searched-for URLs. You log an event containing the query. Someone clicks one of the search results, you log another event recording the click. You need to bring together the events for the search action and the click action.</p> <p>For this type of join, a stream processor needs to maintain state: All events that occurred in the last hour, indexed by session ID. Whenever a search event or click event occurs, it is added to the appropriate index, and the stream processor also checks the other index to see if another event for the same session ID has already arrived. If there is a matching event, you emit an event saying search result was clicked.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#stream-table-joins","title":"Stream-table joins","text":"<p>Sometimes know as enriching the activity events with information from the database.</p> <p>Imagine two datasets: a set of usr activity events, and a database of user profiles. Activity events include the user ID, and the the resulting stream should have the augmented profile information based upon the user ID.</p> <p>The stream process needs to look at one activity event at a time, look up the event's user ID in the database, and add the profile information to the activity event. THe database lookup could be implemented by querying a remote database., however this would be slow and risk overloading the database.</p> <p>Another approach is to load a copy of the database into the stream processor so that it can be queried locally without a network round-trip. The stream processor's local copy of the database needs to be kept up to date; this can be solved with change data capture.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#table-table-join","title":"Table-table join","text":"<p>The stream process needs to maintain a database containing the set of followers for each user so it knows which timelines need to be updated when a new tweet arrives.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#time-dependence-join","title":"Time-dependence join","text":"<p>The previous three types of join require the stream processor to maintain some state.</p> <p>If state changes over time, and you join with some state, what point in time do you use for the join?</p> <p>If the ordering of events across streams is undetermined, the join becomes nondeterministic.</p> <p>This issue is known as slowly changing dimension (SCD), often addressed by using a unique identifier for a particular version of the joined record. For example, we can turn the system deterministic if every time the tax rate changes, it is given a new identifier, and the invoice includes the identifier for the tax rate at the time of sale. But as a consequence makes log compation impossible.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#fault-tolerance","title":"Fault tolerance","text":"<p>Batch processing frameworks can tolerate faults fairly easy:if a task in a MapReduce job fails, it can simply be started again on another machine, input files are immutable and the output is written to a separate file.</p> <p>Even though restarting tasks means records can be processed multiple times, the visible effect in the output is as if they had only been processed once (exactly-once-semantics or effectively-once).</p> <p>With stream processing waiting until a tasks if finished before making its ouput visible is not an option, stream is infinite.</p> <p>One solution is to break the stream into small blocks, and treat each block like a minuature batch process (micro-batching). This technique is used in Spark Streaming, and the batch size is typically around one second.</p> <p>An alternative approach, used in Apache Flint, is to periodically generate rolling checkpoints of state and write them to durable storage. If a stream operator crashes, it can restart from its most recent checkpoint.</p> <p>Microbatching and chekpointing approaches provide the same exactly-once semantics as batch processing. However, as soon as output leaves the stream processor, the framework is no longer able to discard the output of a failed batch.</p> <p>In order to give appearance of exactly-once processing, things either need to happen atomically or none of must happen. Things should not go out of sync of each other. Distributed transactions and two-phase commit can be used.</p> <p>This approach is used in Google Cloud Dataflow and VoltDB, and there are plans to add similar features to Apache Kafka.</p> <p>Our goal is to discard the partial output of failed tasks so that they can be safely retired without taking effect twice. Distributed transactions are one way of achieving that goal, but another way is to rely on idempotence.</p> <p>An idempotent operation is one that you can perform multiple times, and it has the same effect as if you performed it only once.</p> <p>Even if an operation is not naturally idempotent, it can often be made idempotent with a bit of extra metadata. You can tell wether an update has already been applied.</p> <p>Idempotent operations can be an effective way of achieving exactly-once semantics with only a small overhead.</p> <p>Any stream process that requires state must ensure tha this state can be recovered after a failure.</p> <p>One option is to keep the state in a remote datastore and replicate it, but it is slow.</p> <p>An alternative is to keep state local to the stream processor and replicate it periodically.</p> <p>Flink periodically captures snapshots and writes them to durable storage such as HDFS; Samza and Kafka Streams replicate state changes by sending them to a dedicated Kafka topic with log compaction. VoltDB replicates state by redundantly processing each input message on several nodes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-future-of-data-systems","title":"The future of data systems","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#data-integration","title":"Data integration","text":"<p>Updating a derived data system based on an event log can often be made determinisitic and idempotent.</p> <p>Distributed transactions decide on an ordering of writes by using locks for mutual exclusion, while CDC and event sourcing use a log for ordering. Distributed transactions use atomic commit to ensure exactly once semantics, while log-based systems are based on deterministic retry and idempotence.</p> <p>Transaction systems provide linearizability, useful guarantees as reading your own writes. On the other hand, derived systems are often updated asynchronously, so they do not by default offer the same timing guarantees.</p> <p>In the absence of widespread support for a good distributed transaction protocol, log-based derived data is the most promising approach for integrating different data systems.</p> <p>However, as systems are scaled towards bigger and more coplex worloads, limitiations emerge: * Constructing a totally ordered log requires all events to pass through a single leader node that decides on the ordering. * An undefined ordering of events that originate on multiple datacenters. * When two events originate in different services, there is no defined order for those events. * Some applications maintain client-side state. Clients and servers are very likely to see events in different orders.</p> <p>Deciding on a total order of events is known as total order broadcast, which is equivalent to consensus. It is still an open research problem to design consensus algorithms that can scale beyond the throughput of a single node.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#batch-and-stream-processing","title":"Batch and stream processing","text":"<p>The fundamental difference between batch processors and batch processes is that the stream processors operate on unbounded datasets whereas batch processes inputs are of a known finite size.</p> <p>Spark performs stream processing on top of batch processing. Apache Flink performs batch processing in top of stream processing.</p> <p>Batch processing has a quite strong functional flavour. The output depends only on the input, there are no side-effects. Stream processing is similar but it allows managed, fault-tolerant state.</p> <p>Derived data systems could be maintained synchronously. However, asynchrony is what makes systems based on event logs robust: it allows a fault in one part of the system to be contained locally.</p> <p>Stream processing allows changes in the input to be reflected in derived views with low delay, whereas batch processing allows large amounts of accumulated historical data to be reprocessed in order to derive new views onto an existing dataset.</p> <p>Derived views allow gradual evolution. If you want to restructure a dataset, you do not need to perform the migration as a sudden switch. Instead, you can maintain the old schema and the new schema side by side as two independent derived views onto the same underlying data, eventually you can drop the old view.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#lambda-architecture","title":"Lambda architecture","text":"<p>The whole idea behind lambda architecture is that incoming data should be recorded by appending immutable events to an always-growing dataset, similarly to event sourcing. From these events, read-optimised vuews are derived. Lambda architecture proposes running two different systems in parallel: a batch processing system such as Hadoop MapReduce, and a stream-processing system as Storm.</p> <p>The stream processor produces an approximate update to the view: the batch processor produces a corrected version of the derived view.</p> <p>The stream process can use fast approximation algorithms while the batch process uses slower exact algorithms.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#unbundling-databases","title":"Unbundling databases","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#creating-an-index","title":"Creating an index","text":"<p>Batch and stream processors are like elaborate implementations of triggers, stored procedures, and materialised view maintenance routines. The derived data systems they maintain are like different index types.</p> <p>There are two avenues by which different storate and processing tools can nevertheless be composed into a cohesive system: * Federated databases: unifying reads. It is possible to provide a unified query interface to a wide variety of underlying storate engines and processing methods, this is known as federated database or polystore. An example is PostgreSQL's foreign data wrapper. * Unbundled databases: unifying writes. When we compose several storage systems, we need to ensure that all data changes end up in all the right places, even in the face of faults, it is like unbundling a database's index-maintenance features in a way that can synchronise writes across disparate technologies.</p> <p>Keeping the writes to several storage systems in sync is the harder engineering problem.</p> <p>Synchronising writes requires distributed transactions across heterogeneous storage systems which may be the wrong solution. An asynchronous event log with idempotent writes is a much more robust and practical approach.</p> <p>The big advantage is loose coupling between various components: 1. Asynchronous event streams make the system as a whole more robust to outages or performance degradation of individual components. 2. Unbundling data systems allows different software components and services to be developed, improved and maintained independently from each other by different teams.</p> <p>If there is a single technology that does everything you need, you're most likely best off simply using that product rather than trying to reimplement it yourself from lower-level components. The advantages of unbundling and composition only come into the picture when there is no single piece of software that satisfies all your requirements.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#separation-of-application-code-and-state","title":"Separation of application code and state","text":"<p>It makes sense to have some parts of a system that specialise in durable data storage, and other parts that specialise in running application code. The two can interact while still remaining independent.</p> <p>The trend has been to keep stateless application logic separate from state management (databases): not putting application logic in the database and not putting persistent state in the application.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#dataflow-interplay-between-state-changes-and-application-code","title":"Dataflow, interplay between state changes and application code","text":"<p>Instead of treating the database as a passive variable that is manipulated by the application, application code responds to state changes in one place by triggering state changes in another place.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#stream-processors-and-services","title":"Stream processors and services","text":"<p>A customer is purchasing an item that is priced in one currency but paid in another currency. In order to perform the currency conversion, you need to know the current exchange rate.</p> <p>This could be implemented in two ways: * Microservices approach, the code that processes the purchase would probably wuery an exchange-rate service or a database in order to obtain the current rate for a particular currency. * Dataflow approach, the code that processes purchases would subscribe to a stream of exchange rate updates ahead of time, and record the current rate in a local database whenever it changes. When it comes to processing the purchase, it only needs to query the local database.</p> <p>The dataflow is not only faster, but it is also more robust to the failure of another service.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#observing-derived-state","title":"Observing derived state","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#materialised-views-and-caching","title":"Materialised views and caching","text":"<p>A full-text search index is a good example: the write path updates the index, and the read path searches the index for keywords.</p> <p>If you don't have an index, a search query would have to scan over all documents, which is very expensive. No index means less work on the write path (no index to update), but a lot more work on the read path.</p> <p>Another option would be to precompute the search results for only a fixed set of the most common queries. The uncommon queries can still be served from the inxed. This is what we call a cache although it could also be called a materialised view.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#read-are-events-too","title":"Read are events too","text":"<p>It is also possible to represent read requests as streams of events, and send both the read events and write events through a stream processor; the processor responds to read events by emiting the result of the read to an output stream.</p> <p>It would allow you to reconstruct what the user saw before they made a particular decision.</p> <p>Enables better tracking of casual dependencies.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#aiming-for-correctness","title":"Aiming for correctness","text":"<p>If your application can tolerate occasionally corrupting or losing data in unpredictable ways, life is a lot simpler. If you need stronger assurances of correctness, the serializability and atomic commit are established approaches.</p> <p>While traditional transaction approach is not going away, there are some ways of thinking about correctness in the context of dataflow architectures.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#the-end-to-end-argument-for-databases","title":"The end-to-end argument for databases","text":"<p>Bugs occur, and people make mistakes. Favour of immutable and append-only data, because it is easier to recover from such mistakes.</p> <p>We've seen the idea of exactly-once (or effectively-once) semantics. If something goes wrong while processing a message, you can either give up or try again. If you try again, there is the risk that it actually succeeded the first time, the message ends up being processed twice.</p> <p>Exactly-once means arranging the computation such that the final effect is the same as if no faults had occurred.</p> <p>One of the most effective approaches is to make the operation idempotent, to ensure that it has the same effect, no matter whether it is executed once or multiple times. Idempotence requires some effort and care: you may need to maintain some additional metadata (operation IDs), and ensure fencing when failing over from one node to another.</p> <p>Two-phase commit unfortunately is not sufficient to ensure that the transaction will only be executed once.</p> <p>You need to consider end-to-end flow of the request.</p> <p>You can generate a unique identifier for an operation (such as a UUID) and include it as a hidden form field in the client application, or calculate a hash of all the relevant form fields to derive the operation ID. If the web browser submits the POST request twice, the two requests will have the same operation ID. You can then pass that operation ID all the way through to the database and check that you only ever execute one operation with a given ID. You can then save those requests to be processed, uniquely identified by the operation ID.</p> <p>Is not enough to prevent a user from submitting a duplicate request if the first one times out. Solving the problem requires an end-to-end solution: a transaction indentifier that is passed all the way from the end-user client to the database.</p> <p>Low-level reliability mechanisms such as those in TCP, work quite well, and so the remaining higher-level faults occur fairly rarely.</p> <p>Transactions have long been seen as a good abstraction, they are useful but not enough.</p> <p>It is worth exploring F=fault-tolerance abstractions that make it easy to provide application-specific end-to-end correctness properties, but also maintain good performance and good operational characteristics.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#enforcing-constraints","title":"Enforcing constraints","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#uniqueness-constraints-require-consensus","title":"Uniqueness constraints require consensus","text":"<p>The most common way of achieving consensus is to make a single node the leadder, and put it in charge of making all decisions. If you need to tolerate the leader failing, you're back at the consensus problem again.</p> <p>Uniqueness checking can be scaled out by partitioning based on the value that needs to be unique. For example, if you need usernames to be unique, you can partition by hash or username.</p> <p>Asynchronous multi-master replication is ruled out as different masters concurrently may accept conflicting writes, so values are no longer unique. If you want to be able to immediately reject any writes that would violate the constraint, synchronous coordination is unavoidable.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#uniqueness-in-log-based-messaging","title":"Uniqueness in log-based messaging","text":"<p>A stream processor consumes all the messages in a log partition sequentially on a single thread. A stream processor can unambiguously and deterministically decide which one of several conflicting operations came first. 1. Every request for a username is encoded as a message. 2. A stream processor sequentially reads the requests in the log. For every request for a username tht is available, it records the name as taken and emits a success message to an output stream. For every request for a username that is already taken, it emits a rejection message to an output stream. 3. The client waits for a success or rejection message corresponding to its request.</p> <p>The approach works not only for uniqueness constraints, but also for many other kinds of constraints.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#multi-partition-request-processing","title":"Multi-partition request processing","text":"<p>There are potentially three partitions: the one containing the request ID, the one containing the payee account, and one containing the payer account.</p> <p>The traditional approach to databases, executing this transaction would require an atomic commit across all three partitions.</p> <p>Equivalent correctness can be achieved with partitioned logs, and without an atomic commit.</p> <ol> <li>The request to transfer money from account A to account B is given a unique request ID by the client, and appended to a log partition based on the request ID.</li> <li>A stream processor reads the log of requests. For each request message it emits two messages to output streams: a debit instruction to the payer account A (partitioned by A), and a credit instruction to the payee account B (partitioned by B). The original request ID is included in those emitted messages.</li> <li>Further processors consume the streams of credit and debit instructions, deduplicate by request ID, and apply the chagnes to the account balances.</li> </ol>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#timeliness-and-integrity","title":"Timeliness and integrity","text":"<p>Consumers of a log are asynchronous by design, so a sender does not wait until its message has been proccessed by consumers. However, it is possible for a client to wait for a message to appear on an output stream.</p> <p>Consistency conflates two different requirements: * Timeliness: users observe the system in an up-to-date state. * Integrity: Means absence of corruption. No data loss, no contradictory or false data. The derivation must be correct.</p> <p>Violations of timeless are \"eventual consistency\" whereas violations of integrity are \"perpetual inconsistency\".</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#correctness-and-dataflow-systems","title":"Correctness and dataflow systems","text":"<p>When processing event streams asynchronously, there is no guarantee of timeliness, unless you explicitly build consumers that wait for a message to arrive before returning. But integrity is in fact central to streaming systems.</p> <p>Exactly-once or effectively-once semantics is a mechanism for preserving integrity. Fault-tolerant message delivery and duplicate supression are important for maintaining the integrity of a data system in the face of faults.</p> <p>Stream processing systems can preserve integrity without requireing distributed transactions and an atomic commit protocol, which means they can potentially achieve comparable correctness with much better performance and operational robustness. Integrity can be achieved through a combination of mechanisms: * Representing the content of the write operation as a single message, this fits well with event-sourcing * Deriving all other state updates from that single message using deterministic derivation functions * Passing a client-generated request ID, enabling end-to-end duplicate supression and idempotence * Making messages immutable and allowing derived data to be reprocessed from time to time</p> <p>In many businesses contexts, it is actually acceptable to temporarily violate a constraint and fix it up later apologising. The cost of the apology (money or reputation), it is often quite low.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#coordination-avoiding-data-systems","title":"Coordination-avoiding data-systems","text":"<ol> <li>Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizability, or synchronous cross-partition coordination.</li> <li>Although strict uniqueness constraints require timeliness and coordination, many applications are actually fine with loose constraints than may be temporarily violated and fixed up later.</li> </ol> <p>Dataflow systems can provide the data management services for many applications without requiring coordination, while still giving strong integrity guarantees. Coordination-avoiding data systems can achieve better performance and fault tolerance than systems that need to perform synchronous coordination.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#trust-but-verify","title":"Trust, but verify","text":"<p>Checking the integrity of data is know as auditing.</p> <p>If you want to be sure that your data is still there, you have to actually read it and check. It is important to try restoring from your backups from time to time. Don't just blindly trust that it is working.</p> <p>Self-validating or self-auditing systems continually check their own integrity.</p> <p>ACID databases has led us toward developing applications on the basis of blindly trusting technology, neglecting any sort of auditability in the process.</p> <p>By contrast, event-based systems can provide better auditability (like with event sourcing).</p> <p>Cryptographic auditing and integrity checking often relies on Merkle trees. Outside of the hype for cryptocurrencies, certificate transparency is a security technology that relies on Merkle trees to check the validity of TLS/SSL certificates.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#doing-the-right-thing","title":"Doing the right thing","text":"<p>Many datasets are about people: their behaviour, their interests, their identity. We must treat such data with humanity and respect. Users are humans too, and human dignitity is paramount.</p> <p>There are guidelines to navigate these issues such as ACM's Software Engineering Code of Ethics and Professional Practice</p> <p>It is not sufficient for software engineers to focus exclusively on the technology and ignore its consequences: the ethical responsibility is ours to bear also.</p> <p>In countries that respect human rights, the criminal justice system presumes innocence until proven guilty; on the other hand, automated systems can systematically and artbitrarily exclude a person from participating in society without any proof of guilt, and with little chance of appeal.</p> <p>If there is a systematic bias in the input to an algorithm, the system will most likely learn and amplify bias in its output.</p> <p>It seems ridiculous to believe that an algorithm could somehow take biased data as input and produce fair and impartial output from it. Yet this believe often seems to be implied by proponents of data-driven decision making.</p> <p>If we want the future to be better than the past, moral imagination is required, and that's something only humans can provide. Data and models should be our tools, not our masters.</p> <p>If a human makes a mistake, they can be held accountable. Algorithms make mistakes too, but who is accountable if they go wrong?</p> <p>A credit score summarises \"How did you behave in the past?\" whereas predictive analytics usually work on the basis of \"Who is similar to you, and how did people like you behave in the past?\" Drawing parallels to others' behaviour implies stereotyping people.</p> <p>We will also need to figure outhow to prevent data being used to harm people, and realise its positive potential instead, this power could be used to focus aid an support to help people who most need it.</p> <p>When services become good at predicting what content users want to se, they may end up showing people only opinions they already agree with, leading to echo chambers in which stereotypes, misinformation and polaristaion can breed.</p> <p>Many consequences can be predicted by thinking about the entire system (not just the computerised parts), an approach known as systems thinking.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20Someguy/#privacy-and-tracking","title":"Privacy and tracking","text":"<p>When a system only stores data that a user has explicitly entered, because they want the system to store and process it in a certain way, the system is performing a service for the user: the user is the customer.</p> <p>But when a user's activity is tracked and logged as a side effect of other things they are doing, the relationship is less clear. The service no longer just does what the users tells it to do, but it takes on interests of its own, which may conflict with the user's interest.</p> <p>If the service is funded through advertising, the advertirsers are the actual customers, and the users' interests take second place.</p> <p>The user is given a free service and is coaxed into engaging with it as much as possible. The tracking of the user serves the needs of the advertirses who are funding the service. This is basically surveillance.</p> <p>As a thougt experiment, try replacing the word data with surveillance.</p> <p>Even themost totalitarian and repressive regimes could only dream of putting a microphone in every room and forcing every person to constantly carry a device capable of tracking their location and movements. Yet we apparently voluntarily, even enthusiastically, throw ourselves into this world of total surveillance. The difference is just that the data is being collected by corporations rather than government agencies.</p> <p>Perhaps you feel you have nothing to hide, you are totally in line with existing power structures, you are not a marginalised minority, and you needn't fear persecution. Not everyone is so fortunate.</p> <p>Without understanding what happens to their data, users cannot give any meaningful consent. Often, data from one user also says things about other people who are not users of the service and who have not agreed to any terms.</p> <p>For a user who does not consent to surveillance, the only real alternative is simply to not user the service. But this choice is not free either: if a service is so popular that it is \"regarded by most people as essential for basic social participation\", then it is not reasonable to expect people to opt out of this service. Especially when a service has network effects, there is a social cost to people choosing not to use it.</p> <p>Declining to use a service due to its tracking of users is only an option for the small number of people who are priviledged enough to have the time and knowledge to understand its privacy policy, and who can affort to potentially miss out on social participation or professional opportunities that may have arisen if they ahd participated in the service. For people in a less priviledged position, there is no meaningful freedom of choice: surveillance becomes inescapable.</p> <p>Having privacy does not mean keeping everything secret; it means having the freedom to choose which things to reveal to whom, what to make public, and what to keep secret.</p> <p>Companies that acquire data essentially say \"trust us to do the right thing with your data\" which means that the right to decide what to reveal and what to keep secret is transferred from the individual to the company.</p> <p>Even if the service promises not to sell the data to third parties, it usually grants itself unrestricted rights to process and analyse the data internally, often going much further than what is overtly visible to users.</p> <p>If targeted advertising is what pays for a service, then behavioral data about people is the service's core asset.</p> <p>When collecting data, we need to consider not just today's political environment, but also future governments too. There is no guarantee that every government elected in the future will respect human rights and civil liberties, so \"it is poor civic hygiene to install technologies that could someday facilitate a police state\".</p> <p>To scrutinise others while avoiding scrutiny oneself is one of the most important forms of power.</p> <p>In the industrial revolution tt took a long time before safeguards were established, such as environmental protection regulations, safety protocols for workplaces, outlawing child labor, and health inspections for food. Undoubtedly the cost of doing business increased when factories could no longer dump their waste into rivers, sell tainted foods, or exploit workers. But society as a whole benefited hugely, and few of us would want to return to a time before those regulations.</p> <p>We should stop regarding users as metrics to be optimised, and remember that they are humans who deserve respect, dignity, and agency. We should self-regulate our data collection and processing practices in order to establish an maintain the trust of the people who depend on our software. And we should take it upon ourselves to educate end users about how their data is used, rather than keeping them in the dark.</p> <p>We should allow each individual to maintain their privacy, their control over their own data, and not steal that control from them through surveillance.</p> <p>We should not retain data forever, but purge it as soon as it is no longer needed.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/","title":"Designing Data Intensive Applications","text":"<p>From moyano83 on github</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-1-reliable-scalable-and-maintainable-applications","title":"Chapter 1: Reliable, Scalable, and Maintainable Applications","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#thinking-about-data-systems","title":"Thinking About Data Systems","text":"<p>The three concerns that are important in most software systems are:</p> <ul> <li>Reliability: The system should continue to work correctly (even performant) even in the face of adversity </li> <li>Scalability: Scalability is the term we use to describe a system's ability to cope with increased load</li> <li>Maintainability: Different people should be able to work productively on the system</li> </ul>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reliability","title":"Reliability","text":"<p>The things that can go wrong are called faults, and systems that anticipate faults and can cope with them are called fault-tolerant or resilient, although it only makes sense to talk about tolerating certain types of faults. A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#hardware-faults","title":"Hardware Faults","text":"<p>Typical approach to this problem is to add redundancy to the individual hardware components in order to reduce the failure rate of the system.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#human-errors","title":"Human Errors","text":"<p>To alleviate this problem, design systems in a way that minimizes opportunities for error decouple the places where people make the most mistakes from the places where they can cause failures, test thoroughly at all levels, set up detailed and clear monitoring and implement good management practices and training.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#scalability","title":"Scalability","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#describing-load","title":"Describing Load","text":"<p>Load can be described with a few numbers which we call load parameters (request per second, reads/writes ration, concurrent users...)</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#describing-performance","title":"Describing Performance","text":"<p>Look at performance in two ways:</p> <pre><code>* When a load parameter changes, how is the performance affected?\n* How much do you need to increase the resources to keep performance unchanged if you increase a load parameter?\n</code></pre> <p>Service level objectives (SLOs) and service level agreements (SLAs) are often defined through the performance percentiles around request time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#approaches-for-coping-with-load","title":"Approaches for Coping with Load","text":"<p>There is two ways to cope with increasing load: scaling up (more powerful machines) and scaling out (adding more nodes to the cluster).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#maintainability","title":"Maintainability","text":"<p>Software can be designed in such a way that it minimizes maintenance pain. We will pay particular attention to three design principles for software systems:</p> <pre><code>* Operability: Make it easy for operations teams to keep the system running smoothly\n* Simplicity: Make it easy for new engineers to understand the system, by removing as much complexity as possible from the system\n* Evolvability: Make it easy for engineers to change the system in the future, adapting it for unanticipated use cases asrequirements change\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#operability-making-life-easy-for-operations","title":"Operability: Making Life Easy for Operations","text":"<p>An operations team typically is responsible for the monitorization of the service, tracking down the cause of problems, keeping the software and platforms updated, keeping tabs on how different systems affect each other, doing preventive work, setting best practices and tools, setting security, defining process and documenting all of the above. Good operability means making routine tasks easy, allowing the operations team to focus on other activities</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#simplicity-managing-complexity","title":"Simplicity: Managing Complexity","text":"<p>Reducing complexity greatly improves the maintainability of software, and thus simplicity should be a key goal for the systems we build. Abstraction removes unnecesary complexity: it can hide a great deal of implementation detail behind a clean, simple-to-understand facade.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#evolvability-making-change-easy","title":"Evolvability: Making Change Easy","text":"<p>The ease with which you can modify a data system, and adapt it to changing requirements, is closely linked to its simplicity and its abstractions: simple and easy-to-understand systems are usually easier to modify than complex ones.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-2-data-models-and-query-languages","title":"Chapter 2: Data Models and Query Languages","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#relational-model-versus-document-model","title":"Relational Model Versus Document Model","text":"<p>In SQL data is organized into relations called tables, where each relation is an unordered collection of tuples (rows).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-birth-of-nosql","title":"The Birth of NoSQL","text":"<p>The adoption of NoSQL databases is driven by:</p> <pre><code>* A need for greater scalability than RDBMS, including very large datasets or very high write throughput\n* A widespread preference for free and open source software\n* Specialized query operations that are not well supported by the relational model\n* Frustration with the restrictiveness of relational schemas, desire for a more dynamic and expressive data model\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-object-relational-mismatch","title":"The Object-Relational Mismatch","text":"<p>The impedance mismatch is the disconnection between the relational model and the object model, which can be partially solved by the ORM frameworks. Later versions of SQL support structured data types and XML within a single row. JSON format can partially solve the impedance mismatch, it has better locality than multi-table schema as the data is stored as a tree structure, but the lack of schema can be either an advantage or a disadvantage.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#many-to-one-and-many-to-many-relationships","title":"Many-to-One and Many-to-Many Relationships","text":"<p>Normalized data requires many-to-one relationships which doesn't fit very well in the document model: support for joins in this model is often weak and you may need to perform this join in application code.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#are-document-databases-repeating-history","title":"Are Document Databases Repeating History?","text":"<p>The hierarchichal model (data is represented as a tree of records nested within records) has limitations that the relational and network model tries to solve.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-network-model","title":"The network model","text":"<p>Also named CODASYL model, is a generalization of the hierarchical model, although in this model, a record could have multiple parents. The links between records in the network model were not foreign keys, but pointers in a programming language (stored on disk). The only way of accessing a record was to follow a path (called access path) from a root record along these chains of links. A query in CODASYL was performed by moving a cursor through the database by iterating over lists of records and following access paths. If a record had multiple parents, the application code had to keep track of all the various relationships. This makes updates difficult and queries complicated to write.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-relational-model","title":"The relational model","text":"<p>In a relational database, the query optimizer automatically decides which parts of the query to execute in which order, and which indexes to use. This model makes querying more efficient, but adds complexity on the code optimizer.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#comparison-to-document-databases","title":"Comparison to document databases","text":"<p>Document databases are similar to the networking model in regards to storing nested records, but when it comes to representing many-to-one and many-to-many relation\u2010ships, relational and document databases references the joined table by a unique identifier, which is called a foreign key in the RBDMS and a document reference in the document model.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#relational-versus-document-databases-today","title":"Relational Versus Document Databases Today","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#which-data-model-leads-to-simpler-application-code","title":"Which data model leads to simpler application code?","text":"<p>If the data in your application has a document-like structure (i.e., a tree of one-to-many relationships, where typically the entire tree is loaded at once), use a document model. The document model still needs to have access patterns for nested structures. If many-to-many relationships are needed, you are most likely to use a relational model. Usually highly interconnected data is more efficiently represented with the relational model and graph models.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#schema-flexibility-in-the-document-model","title":"Schema flexibility in the document model","text":"<p>No schema means that arbitrary keys and values can be added to a document, and when reading, clients have no guarantees as to what fields the documents may contain. Document databases has some kind of implicit schema, but it is not enforced by the database (schema-on-read). The schema-on-read approach is advantageous if the items in the collection don't all have the same structure, which could happen due to have different types of objects (and hence is not practical to put each type in its own table), or the structure of the table is determined by external systems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#data-locality-for-queries","title":"Data locality for queries","text":"<p>If your application often needs to access an entire document, there is a performance advantage to this storage locality. The locality advantage only applies if you need large parts of the document at the same time. Updates usually requires to write the whole document (only modifications that don't change the encoded size of a document can easily be performed in place), it is generally recommended that you keep documents fairly small and avoid writes that increase the size of a document.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#convergence-of-document-and-relational-databases","title":"Convergence of document and relational databases","text":"<p>Most relational database systems has support for XML, including functions to make local modifications to XML documents and the ability to index and query inside XML documents (same applies to JSON). On the document database side support for relational-like joins (performing a client-side joins) have been added, although this is likely to be slower than a join performed in the database since it requires additional network round-trips and is less optimized.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#query-languages-for-data","title":"Query Languages for Data","text":"<p>SQL is a declarative query language, whereas IMS and CODASYL queried the database using imperative code. The fact that SQL is more limited in functionality gives the database much more room for automatic optimizations. Declarative languages are easier to parallelize.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#mapreduce-querying","title":"MapReduce Querying","text":"<p>MapReduce is neither a declarative query language nor a fully imperative query API, but somewhere in between: the logic of the query is expressed with snippets of code, which are called repeatedly by the processing framework. Some document databases like MongoDB supports a limited form of map reduce. The map and reduce functions are somewhat restricted in what they are allowed to do. They must be pure functions, which means they only use the data that is passed to them as input, they cannot perform additional database queries, and they must not have any side effects.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#graph-like-data-models","title":"Graph-Like Data Models","text":"<p>A graph consists of vertices (also known as nodes or entities) and edges (also known as relationships or arcs). Many kinds of data can be modeled as a graph:</p> <pre><code>* Social graphs: Vertices are people, and edges indicate which people know each other\n* The web graph: Vertices are web pages, and edges indicate HTML links to other pages\n* Road or rail networks: Vertices are junctions, and edges represent the roads or railway lines between them\n</code></pre> <p>An equally powerful use of graphs is to provide a consistent way of storing completely different types of objects in a single datastore.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#property-graphs","title":"Property Graphs","text":"<p>In the property graph model, each vertex consists of:</p> <pre><code>* A unique identifier\n* A set of outgoing edges\n* A set of incoming edges\n* A collection of properties (key-value pairs)\n</code></pre> <p>Each edge consists of:</p> <pre><code>* A unique identifier\n* The vertex at which the edge starts (the tail vertex)\n* The vertex at which the edge ends (the head vertex)\n* A label to describe the kind of relationship between the two vertices\n* A collection of properties (key-value pairs)\n</code></pre> <p>Modelling this into a relational model would be a two table model with:</p> <pre><code>CREATE TABLE vertices (vertex_id integer PRIMARYKEY,\n    properties json);\n\nCREATE TABLE edges (edge_id integer PRIMARY KEY,\n    properties json,\n    label text,\n    tail_vertex integer REFERENCES vertices (vertex_id),\n    head_vertex integer REFERENCES vertices (vertex_id));\n</code></pre> <p>Worth notice the many to many relationship established between the vertices through the edges, and how having labels for different kind of relationships allows for the storage of different kinds of information in a single graph. Graphs can be easily extended to accommodate changes in the application's data structures.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-cypher-query-language","title":"The Cypher Query Language","text":"<p>Cypher is a declarative query language for property graphs. The following example creates 3 vertices and two edges between them each one with the label 'WITHIN':</p> <pre><code>CREATE\n  (NAmerica:Location {name:'North America', type:'continent'}),\n  (USA:Location      {name:'United States', type:'country'  }),\n  (Idaho:Location    {name:'Idaho',         type:'state'    }),\n  (Idaho) -[:WITHIN]-&gt;  (USA)  -[:WITHIN]-&gt; (NAmerica)\n</code></pre> <p>Which allow us to traverse the graph to ask questions like \"give me all the people that were born in Idaho\". As it is a declarative query language, you don't need to specify execution details when writing the query (chosen by the query optimizer). An example of that type of query is:</p> <p>```text MATCH (person) -[:BORN_IN]-&gt;  () -[:WITHIN0..]-&gt; (us:Location {name:'United States'}), (person) -[:LIVES_IN]-&gt; () -[:WITHIN0..]-&gt; (eu:Location {name:'Europe'}) RETURN person.name</p> <pre><code>\nThe `-[:WITHIN*0..]-&gt;` expresses \"follow a WITHIN edge, zero or more times.\" It is like the * operator in a regular expression. As suggested before,\nwe can also query graph data by using SQL if we put it in a relational structure. Although in SQL you usually know in advance which joins you need in\nyour query while in graph databases is not.\n\n#### Triple-Stores and SPARQL\n\nIn a triple-store, all information is stored in the form of very simple three-part statements: (subject, predicate, object). The subject here is\nsimilar to a vertex in a graph, the object is either a primitive value (string, number)\nequivalent to the key and value of a property on the subject vertex, or another vertex in the graph (tail vertex).\n\n##### The semantic web\n\nThe Resource Description Framework (RDF) was intended as a mechanism for different websites to publish data in a consistent format, allowing data from\ndifferent websites to be automatically combined into a web of data\u2014a kind of internet-wide \"database of everything.\". Triples can be a good internal\ndata model to represent it.\n\n##### The RDF data model\n\nOn the RDF data model, the subject, predicate, and object of a triple are often URIs (like &lt;http://my-company.com/namespace#lives_in&gt; instead of\nWITHIN). The reasoning behind this design is that you should be able to combine your data with someone else's data, and if they attach a different\nmeaning to the word within or `lives_in`, you won't get a conflict because their predicates are different. Something like avoiding collisions by using\ndifferent namespaces.\n\n##### The SPARQL query language\n\nSPARQL is a query language for triple-stores using the RDF data model (similar to Cypher):\n\n```text\nPREFIX : &lt;urn:example:&gt;\nSELECT ?personName WHERE {\n  ?person :name ?personName.\n  ?person :bornIn  / :within* / :name \"United States\".\n  ?person :livesIn / :within* / :name \"Europe\".\n}\n</code></pre> <p>Because RDF doesn't distinguish between properties and edges but just uses predicates for both, you can use the same syntax for matching properties.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-foundation-datalog","title":"The Foundation: Datalog","text":"<p>Datalog's data model is similar to the triple-store model. Instead of writing a triple as (subject, predicate, object), we write it as predicate (subject, object). In Datalog, we define rules that tell the database about new predicates. These predicates aren't triples stored in the database, but instead they are derived from data or from other rules. Rules can refer to other rules.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-3-storage-and-retrieval","title":"Chapter 3: Storage and Retrieval","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#data-structures-that-power-your-database","title":"Data Structures That Power Your Database","text":"<p>Many databases internally use a log, which is an append-only data file. In order to efficiently find the value for a particular key in the database, we need a different data structure: an index. If you want to search the same data in several different ways, you may need several different indexes on different parts of the data. There is an important trade-off in storage systems: well-chosen indexes speed up read queries, but every index slows down writes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#hash-indexes","title":"Hash Indexes","text":"<p>A simple possible strategy to implement indexes for key-value data is to keep an in-memory hash map where every key is mapped to a byte offset in the data file. Bitcask is a storage engine that does this, stores all the keys in memory and values are either in memory (cache) or on disk, only a disk seek has to be done to load a value. This is well suited for frequently updated keys. The log can be break into segments of certain size (segments are closed when they reach certain size), once a segment is closed, compaction (throwing away duplicate keys keeping the most up to date value) is performed and a new segment file is opened. After compaction, the segments can be merged if they are small enough (segments are not modified so they are written to a new file), this is done in the background by a separate thread. Some performance considerations are:</p> <pre><code>* File format: binary preferred\n* Deleting records: a new append must be done to indicate the deletion and would be deleted on next merge\n* Crash recovery: Data needs to be reloaded in memory which can take time if segments are large\n* Partially written records: Checksums for records prevent corrupted parts of the logs being used\n* Concurrency control: A common implementation is to have a single writer thread per segment\n</code></pre> <p>Append only allows for multiple concurrent reads, is usually faster and merging segments avoids data file fragmentation over time. Although it is limited by the memory and range queries are not efficient.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#sstables-and-lsm-trees","title":"SSTables and LSM-Trees","text":"<p>If instead of having the key-value pairs in the log in the order they were written we have the keys sorted by keys, we have what is called a Sorted String Table (SSTable). Keys must appear only once within each merged segment file. This is beneficial due to:</p> <pre><code>* Merging files is simpler and efficient, mergesort algorithm (taking several segment and writting the lowest and most recent key to the \n  segment in each step) can be used\n* No need for indexing all keys (you can jump to another close position and scan from there), sparse key indexes can be used\n* Since read request needs to scan over several key-value pairs for request, it is possible to group and compress those records in a block \n  before writting it, which reduces I/O bandwidth\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#constructing-and-maintaining-sstables","title":"Constructing and maintaining SSTables","text":"<p>Maintaining a sorted structure on disk is possible, but maintaining it in memory is much easier. The storage engine can work as follows:</p> <pre><code>* When a write comes in, add it to the in-memory balanced tree (called memtable)\n* If memtable size &gt; threshold, write it to disk, while this happens new writes are written to a new memtable instance\n* On request, try to find the key in the memtable, if it fails go to the most recent disk segment, if it fails go to the second, and so on\n* Run merging an compaction in the background regularly\n</code></pre> <p>The only problem with this is that it may lead to data lost if the server goes down and memtable has not been written to disk (which can be solved by appending to an unsorted separate log every write as it happens). This log can be discarded every time the memtable is written to disk.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#making-an-lsm-tree-out-of-sstables","title":"Making an LSM-tree out of SSTables","text":"<p>Some key-value storage engines are designed to be embedded into other applications. This index structure is called Log-Structured Merge-Tree or LSM-Tree. A similar concept is used for full text search in Lucene.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#performance-optimizations","title":"Performance optimizations","text":"<p>the LSM-tree algorithm can be slow if the key does not exists (as it does a full scan). Storage engines often use Bloom filters, which are used to tell you if a key does not appear in the database. Other concerns are when and how the compaction happens, the algorithm used are size-tiered(newer and smaller SSTables are merged into older and larger ones), and leveled compaction (key range is split up into smaller SSTables and older data is moved into separate levels).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#b-trees","title":"B-Trees","text":"<p>The most widely used indexing structure is the B-tree, which keep key-value pairs sorted by key, allowing for efficient key-value lookups and range queries. B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time. Each page can be identified using an address or location, which allows one page to refer to another (similar to a pointer), but on disk instead of in memory and this structure can be used to construct a tree of pages. One page is designated as the root of the B-tree, which is used to start looking for a key. The page contains several keys and references to child pages. Each child is responsible for a continuous range of keys, and the keys between the references indicate where the boundaries between those ranges lie. A leaf page contains individual keys, which contains either the value for the key or the reference to the page where the value is. The number of references to child pages in one page of the B-tree is called the branching factor.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#making-b-trees-reliable","title":"Making B-trees reliable","text":"<p>The basic write operation is to overwrite pages on disk. This can fail, for example if you split a page because an insertion caused it to be overfull, you need to write the two pages that were split, and also overwrite their parent page to update the references to the two child pages (can be solved partially by adding an append only write-ahead log named WAL). Pages has typically latches (lightweight locks), to deal with concurrency problems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#b-tree-optimizations","title":"B-tree optimizations","text":"<p>Some optimizations can be:</p> <pre><code>* Use a copy-on-write scheme instead of WAL: A modified page is written to a different location, and a new version of the parent pages in the tree \n  is created, pointing at the new location\n* Abreviatting the keys to save space, specially in interior pages which only needs to act as boundaries\n* Many B-tree implementations therefore try to lay out the tree so that leaf pages appear in sequential order on disk\n* Additional pointer to sibling pages might be added to speed scanning\n* Fractal trees variants\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#comparing-b-trees-and-lsm-trees","title":"Comparing B-Trees and LSM-Trees","text":"<p>LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads, although you need to test systems with your particular workload in order to make a valid comparison.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#advantages-of-lsm-trees","title":"Advantages of LSM-trees","text":"<p>A B-tree index must write every piece of data at least twice: once to the write-ahead log, and once to the tree page itself. Log-structured indexes also rewrite data multiple times due to repeated compaction and merging of SSTables. This multiple write is called write amplification, LSM-trees are typically able to sustain higher write throughput than B-trees, partly because they sometimes have lower write amplification. LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#downsides-of-lsm-trees","title":"Downsides of LSM-trees","text":"<p>A downside of log-structured storage is that the compaction process can sometimes interfere with the performance of ongoing reads and writes. Also, the disk's finite write bandwidth needs to be shared between the initial write (logging and flushing a memtable to disk) and the compaction threads running in the background, the bigger the database gets, the more disk bandwidth is required for compaction. In B-trees, each key exists in exactly one place in the index, whereas a log-structured storage engine may have multiple copies of the same key in different segments (advantage on strong transactional storages).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#other-indexing-structures","title":"Other Indexing Structures","text":"<p>Aside of the key-value (like PK for a RDBMS or a document ID for a document database...), there are other structures like FK or secondary indexes. The main difference between the primary and secondary indexes is that in a secondary index, the indexed values are not necessarily unique; that is, there might be many rows (documents, vertices) under the same index entry.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#storing-values-within-the-index","title":"Storing values within the index","text":"<p>The value that a key points to, can be one of two things: it could be the actual row, or it could be a reference to the row stored elsewhere (the place where rows are stored is known as a heap file and it stores unordered data). The heap file avoids duplicating data when multiple secondary indexes are present: each index just references a location in the heap file, and the actual data is kept in one place. In some cases, it can be desirable to store the indexed row directly within an index, known as a clustered index (storing all row data within the index). Covering index or index with included columns stores some of a table's columns within the index, and some are references to other locations.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-column-indexes","title":"Multi-column indexes","text":"<p>Multi-dimensional indexes are a more general way of querying several columns at once, the most common being the concatenated index, which combines several fields into one key by appending one column to another.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#full-text-search-and-fuzzy-indexes","title":"Full-text search and fuzzy indexes","text":"<p>Full-text search engines commonly allow a search for one word to be expanded to include synonyms of the word, to ignore grammatical variations of words, and to search for occurrences of words near each other in the same document, and support various other features that depend on linguistic analysis of the text.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#keeping-everything-in-memory","title":"Keeping everything in memory","text":"<p>Many datasets are simply not that big, so it's quite feasible to keep them entirely in memory, potentially distributed across several machines. When an in-memory database is restarted, it needs to reload its state, either from disk or over the network from a replica. The performance advantage of in-memory databases is due to not having to deal with overheads of encoding in-memory data structures in a form that can be written to disk.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#transaction-processing-or-analytics","title":"Transaction Processing or Analytics?","text":"<p>A transaction in DBs is a group of reads and writes that form a logical unit. The typical access pattern became known as online transaction processing (OLTP) and usually involves a small number of records per query, fetched by key. A different access pattern is used for analytics, usually an analytic query needs to scan over a huge number of records, only reading a few columns per record, and calculates aggregate statistics. The pattern for analytics is called online analytic processing (OLAP).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#data-warehousing","title":"Data Warehousing","text":"<p>A data warehouse is a database that analysts can query without affecting OLTP operations, it contains a read-only copy of the data in all the various OLTP systems in a company or organization. Data is extracted from OLTP databases (using periodic data dump or a continuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse. This is known as Extract\u2013Transform\u2013Load (ETL). Data warehouses and a relational OLTP database both have a SQL query interface but they are optimized for very different query patterns.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stars-and-snowflakes-schemas-for-analytics","title":"Stars and Snowflakes: Schemas for Analytics","text":"<p>The star schema or dimensional modeling is usually the formula used in analytics. It contains a fact table to which dimension tables (which stores events) points to. The event tables represents an event that occurred at a particular time. Facts are captured as individual events, because this allows maximum flexibility of analysis later. Some of the columns in the fact table are attributes and others are foreign key references to the dimension tables. As each row in the fact table represents an event, the dimensions represent the who, what, where, when, how, and why of the event. A variation of this template is known as the snowflake schema, where dimensions are further broken down into subdimensions. Typical data warehouse tables are often very wide: fact tables often have several hundred columns.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#column-oriented-storage","title":"Column-Oriented Storage","text":"<p>A typical data warehouse query usually accesses 4 or 5 columns at one time (<code>SELECT * ...</code> is not usually required for analytics). In most OLTP databases, storage is laid out in a row-oriented fashion: all the values from one row of a table are stored next to each other. In column-oriented storage, stores all the values from each column together, the column-oriented storage layout relies on each column file containing the rows in the same order.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#column-compression","title":"Column Compression","text":"<p>Column-oriented storage often lends itself very well to compression. One technique that is particularly effective in data warehouses is bitmap encoding (often, the number of distinct values in a column is small compared to the number of rows).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#sort-order-in-column-storage","title":"Sort Order in Column Storage","text":"<p>In a column store it usually doesn't make a difference the order of the data (it is the insertion order), but we can impose an order. The administrator of the database can choose the columns by which the table should be sorted, which is usually dependent on the query pattern most commonly used (i.e. by date and by country). This can help with compression of columns</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#several-different-sort-orders","title":"Several different sort orders","text":"<p>Different queries benefit from different sort orders, storing the same data sorted in several different ways is sometimes used. Having multiple sort orders in a column-oriented store is similar to multiple secondary indexes in a row-oriented store, although row-oriented store keeps every row in one place (in the heap file or a clustered index), and secondary indexes just contain pointers to the matching rows.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#writing-to-column-oriented-storage","title":"Writing to Column-Oriented Storage","text":"<p>An update-in-place approach, like B-trees use, is not possible with compressed columns. If you wanted to insert a row in the middle of a sorted table, you would most likely have to rewrite all the column files. As rows are identified by their position within a column, the insertion has to update all columns consistently. An option is to use LSM-trees. All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn't matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written t o new files in bulk. Queries need to examine both the column data on disk and the recent writes in memory, and combine the two (done by the query optimizer).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#aggregation-data-cubes-and-materialized-views","title":"Aggregation: Data Cubes and Materialized Views","text":"<p>Data warehouse queries often involve an aggregate function (SUM, AVG...), which if they are used often, might have sense to cache the results of such functions. One way of creating such a cache is a materialized view: similar to virtual views (data as result of a query), but written to disk. When the underlying data changes, a materialized view needs to be updated, because it is a denormalized copy of the data. A common special case of a materialized view is known as a data cube or OLAP cube which is a grid of aggregates grouped by different dimensions. In these cubes, performance is gain at the cost of flexibility in querying (therefore this type of aggregation is often limited to some particular queries).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-4-encoding-and-evolution","title":"Chapter 4: Encoding and Evolution","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#formats-for-encoding-data","title":"Formats for Encoding Data","text":"<p>Programs usually work with data in (at least) two different representations:</p> <pre><code>* In memory: optimized for efficient access and manipulation by the CPU (lists, hashmaps, sets...)\n* Encoded in a particular sequence of bytes: There are no pointers (woldn't make sense), JSON, text...\n</code></pre> <p>The translation from the in-memory representation to a byte sequence is called encoding, and the reverse is called decoding.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#language-specific-formats","title":"Language-Specific Formats","text":"<p>Many libraries came with a built-in support for in-memory encoding-decoding, but at the cost of:</p> <pre><code>* Often, the encoding is tied to a particular programming language\n* In order to restore data in the same object types, the decoding process needs to be able to instantiate arbitrary classes, which might lead \n  to security vulnerabilities\n* Versioning data can be a source of problems\n* CPU efficiency can be a source of performance problems\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#json-xml-and-binary-variants","title":"JSON, XML, and Binary Variants","text":"<p>JSON, XML, and CSV are textual formats, that comes with some remarkable problems:</p> <pre><code>* Lot of ambiguity around the encoding of numbers: XML doesn't distinguish between strings containing numbers and numbers, and JSON can't \n  specify the precission of numbers\n* JSON and XML have good support for Unicode character strings, but they don't support binary strings. These strings are usually encoded using \n  Base64, which increases the data size\n* There is optional schema support for XML and JSON, but it is complicated to implement\n* CSV does not have any schema, up to the application to interpret the data. Additions are complicated to handle\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#binary-encoding","title":"Binary encoding","text":"<p>Data encoding matters for very big datasets due to performance reasons. Binary encoding of JSON or XML exists, although the gains are not very big in terms of space, so might not worth the loss of human readability.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#thrift-and-protocol-buffers","title":"Thrift and Protocol Buffers","text":"<p>Both Thrift and Protocol Buffers require a schema for any data that is encoded. Thrift and Protocol Buffers each come with a code generation tool that takes a schema definition, and produces classes that implement the schema in various programming languages. Thrift has two different binary encoding formats, called BinaryProtocol and CompactProtocol. In the thrift binary protocol, the data contains type, length of data, the data itself, and field tags (instead of field names), which are numbers identified the field names in the schema definition (like aliases). In the thrift compact protocol, the field type and tag numbers are combined in one, and fields are encoded using variable length integers (the top bytes are used to indicate whether there are still more bytes to come). Protocol Buffers is very similar to Thrift's CompactProtocol.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#field-tags-and-schema-evolution","title":"Field tags and schema evolution","text":"<p>In Thrift and protocol buffers each field is identified by its tag number and annotated with a datatype, if a field value is not set, it is simply omitted from the encoded record. You can change the name of a field in the schema, but you cannot change a field's tag. You can add new fields to the schema, provided that you give each field a new tag number (Old code would skip a field if it has a tag that doesn't recognize). For backwards compatibility, if a new field is added, can't be made mandatory as it would invalidate the old schemas (or have a default value).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#datatypes-and-schema-evolution","title":"Datatypes and schema evolution","text":"<p>In data type changes, there is a risk that values will lose precision or get truncated.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#avro","title":"Avro","text":"<p>Avro also uses a schema to specify the structure of the data being encoded. It has two schema languages: one (Avro IDL) intended for human editing, and one (based on JSON) that is more easily machine-readable. Values are concatenated together, to parse the binary data, you go through the fields in the order that they appear in the schema and use the schema to tell the datatype of each field (schema mismatch would result in invalid data read).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-writers-schema-and-the-readers-schema","title":"The writer's schema and the reader's schema","text":"<p>With Avro, when an application wants to encode some data, it encodes the data using whatever version of the schema it knows about which might be compiled into the application. This is known as the writer's schema. On data decoding, it needs the data to be in some schema, known as reader's schema, which don't have to be the same than the writer's schema (but needs to be compatible). The Avro library resolves the differences by looking at the writer's schema and the reader's schema side by side and translating the data from the writer's schema into the reader's schema.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#schema-evolution-rules","title":"Schema evolution rules","text":"<p>You may only add or remove a field that has a default value. In Avro, if you want to allow a field to be null, you have to use a union type: union { null, long, string } field; indicates that field can be a number, or a string, or null. Changing the datatype of a field is possible, provided that Avro can convert the type.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#but-what-is-the-writers-schema","title":"But what is the writer's schema?","text":"<p>The schema of an Avro file can't be included in every record, therefore:</p> <pre><code>* Large file with lots of records: Usually the schema is at the beginning of the file. Avro specifies a file format (object container file) to do \n  this\n* Database with individually written records: different records may be written at different points in time using different writer's schemas. A \n  version number indicating the schema is included at the beginning of each record\n* Sending records over a network connection: Two endpoints in a communication can negotiate the schema version on connection setup (like in \n  the Avro RPC protocol)\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dynamically-generated-schemas","title":"Dynamically generated schemas","text":"<p>Avro doesn't contain tag numbers like in Protocol Buffers, and Thrift which is friendlier to the dynamically generated schemas, with Thrift or Protocol Buffers, the field tags would likely have to be assigned by hand every time the schema changes (from DB columns to field tags).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#code-generation-and-dynamically-typed-languages","title":"Code generation and dynamically typed languages","text":"<p>Protocol Buffers and Thrift allows efficient in-memory structures to be used for decoded data after a schema has been defined. There is no point on doing this for dynamically typed languages. Avro provided optional code generation for statically typed languages.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-merits-of-schemas","title":"The Merits of Schemas","text":"<p>Binary encodings based on schemas have a number of nice properties:</p> <pre><code>* They can be much more compact than the various \u201cbinary JSON\u201d variants, as they can omit field names\n* The schema is a valuable form of documentation, which is always up to date\n* Keeping a database of schemas allows you to check forward and backward compatibility of schema changes\n* Code generation from the schema enables type checking at compile time\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#modes-of-dataflow","title":"Modes of Dataflow","text":"<p>Compatibility is a relationship between one process that encodes the data, and another process that decodes it.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dataflow-through-databases","title":"Dataflow Through Databases","text":"<p>In an environment where the application is changing, it is likely that some processes accessing the database will be running newer code, and some will be running older code, therefore forward compatibility is also often required for databases. Older clients usually left newly added fields untouched.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#different-values-written-at-different-times","title":"Different values written at different times","text":"<p>When you deploy a new version of your application, you may entirely replace the old version with the new version which is not true of database contents, this observation is sometimes summed up as data outlives code. Rewriting data into a new schema is certainly possible, but it's an expensive thing to do on a large dataset, simple schema changes, such as adding a new column with a null default value are supported in most relational databases.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#archival-storage","title":"Archival Storage","text":"<p>While archiving, the data dump will typically be encoded using the latest schema, even if the original encoding in the source database contained a mixture of schema versions from different eras.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dataflow-through-services-rest-and-rpc","title":"Dataflow Through Services: REST and RPC","text":"<p>When you have processes that need to communicate over a network, the most common arrangement is to have two roles: clients and servers. The servers expose an API over the network, and the clients make requests to that API (known as service). In some ways, services are similar to databases: they typically allow clients to submit and query data. As the queries a client can do are limited by the API, the services can impose fine-grained restrictions to what a client can do. A key design goal of a service-oriented/microservices architecture is to make the application easier to change and maintain by making services independently deployable and evolvable.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#web-services","title":"Web Services","text":"<p>When HTTP is used as the underlying protocol for talking to the service, it is called a web service. There are two popular approaches:</p> <pre><code>* REST: emphasizes simple data formats, using URLs for identifying resources and using HTTP features for cache control, authentication, and \n  content type negotiation\n* SOAP: XML-based protocol for making network API requests, the API of a SOAP web service is described using an XML-based language called the \n  Web Services Description Language, or WSDL\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-problems-with-remote-procedure-calls-rpcs","title":"The problems with remote procedure calls (RPCs)","text":"<p>The remote procedure call (RPC) model tries to make a request to a remote network service look the same as calling a function or method in your programming language, within the same process. A network request is very different from a local function call:</p> <pre><code>* Local calls are predictable (either succeeds or not), remote calls are not because involves factors out of our control like network problems\n* Local calls returns a result, an exception or never returns (infinite loop). A network call might return nothing due to timeouts, not \n  knowing what happened in the other end\n* Retrying a call might lead to unexpected results for non idempotent calls if the bit lost was the response from the server\n* Response time in local calls are almost always the same, a network call adds latency to it\n* You can pass references in a local call (pointers), all parameters needs to be encoded in the network call\n* Client and services might be implemented using different languages, with potential data type impedance\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#current-directions-for-rpc","title":"Current directions for RPC","text":"<p>The new generation of RPC frameworks is more explicit about the fact that a remote request is different from a local function call, including futures to encapsulate asynchronous calls, or streams, which are a series of requests and responses over time. REST seems to be the predominant style for public APIs.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#data-encoding-and-evolution-for-rpc","title":"Data encoding and evolution for RPC","text":"<p>Regarding evolvability, it is reasonable to assume that all the servers will be updated first, and all the clients second. Thus, you only need backward compatibility on requests, and forward compatibility on responses. Compatibility needs to be maintained for a long time, perhaps indefinitely. For RESTful APIs, common approaches are to use a version number in the URL or in the HTTP Accept header.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#message-passing-dataflow","title":"Message-Passing Dataflow","text":"<p>Asynchronous message-passing systems are similar to RPC in that a client's request is delivered to another process with low latency and similar to databases in that the message is not sent via a direct network connection, but goes via an intermediary called a message broker, which stores the message temporarily. This brings some advantages:</p> <pre><code>* It can act as a buffer if the recipient is unavailable or overloaded, improving system reliability\n* It can automatically redeliver messages to a process that has crashed, preventing messages from being lost\n* It avoids the sender needing to know the IP address and port number of the recipient\n* It allows one message to be sent to several recipients.\n* It logically decouples the sender from the recipient.\n</code></pre> <p>The sender doesn't usually expect to receive a reply.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#message-brokers","title":"Message Brokers","text":"<p>Message brokers are used as follows: one process sends a message to a named queue or topic, and the broker ensures that the message is delivered to one or more consumers of or subscribers to that queue or topic. A consumer may itself publish messages to another topic or to a reply queue that is consumed by the sender of the original message. Message brokers typically don't enforce any particular data model</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#distributed-actor-frameworks","title":"Distributed actor frameworks","text":"<p>The actor model is a programming model for concurrency in a single process. The logic is encapsulated in the actors, which might have some local non-shared state, and communicates with other actors by sending and receiving asynchronous messages, with message delivery not guaranteed. A distributed actor framework essentially integrates a message broker, and the actor programming model into a single framework.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-5-replication","title":"Chapter 5: Replication","text":"<p>If your dataset can fit in a single machine, all of the difficulty in replication lies in handling changes to replicated data.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#leaders-and-followers","title":"Leaders and Followers","text":"<p>The replicas of datastores needs to be updated to stay in sync with the latest data. The most common solution for this is called leader-based replication:</p> <pre><code>1. One replica is the leader (master or primary), which receives client request and writes new data to local storage\n2. The rest of replicas (followers, read replicas, slaves or secondaries), receives the data change from the \nleader as part of a replication log or change stream and updates their local copy of the data by applying the \nwrites in order\n3. A client can then query the leader or the replicas, although writes are only accepted in the leader\n</code></pre> <p>This architecture is used in RDBMS (MySQL, Oracle, PostgreSQL...) and NoSQL DB (MongoDB, Espresso...) and even message brokers (Kafka, RabbitMQ)</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#synchronous-versus-asynchronous-replication","title":"Synchronous Versus Asynchronous Replication","text":"<p>Replication can happen synchronously or asynchronously (often configurable in RDBMS). In synchronous replication the leader waits until the follower have confirmed the write before reporting success to the client and before making the write visible to other clients (guarantee of availability if the leader fails at the cost of latency), in asynchronous replication, the leader sends the write to the replica and does not wait (leading to potential data lost). Usually for sync replication, at least one follower needs to be in sync (this is called semi-synchronous replication).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#setting-up-new-followers","title":"Setting Up New Followers","text":"<p>The process of setting up a new follower looks like this:</p> <pre><code>1. Take a consistent snapshot of the leader's database at some point in time\n2. Copy the snapshot to the new follower node\n3. After the copy, request all the changes since the snapshot was taken (snapshot is associated with an exact \nposition on the replication log)\n4. One the follower has copied all the changes we say it has caught up\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#handling-node-outages","title":"Handling Node Outages","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#follower-failure-catch-up-recovery","title":"Follower failure: Catch-up recovery","text":"<p>The follower gets the last transaction it has processed and request to the leader all the changes since then. After the caught up, it can continue receiving a stream of data changes as before.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#leader-failure-failover","title":"Leader failure: Failover","text":"<p>One of the followers needs to be promoted to be the new leader, clients needs to send the writes to it and the other followers need to start consuming data changes from it. This process is called failover. An automatic failover steps are:</p> <pre><code>1. Determining that the leader has failed: Most of systems use a timeout, if a node does not reply in X seconds, \nthen it is assumed to be dead\n2. Elect a new leader: Either by election or be appointed by a previously elected controller node\n3. Reconfiguring the system to use the new leader: Clients needs to send request to the new leader, if the old \nleader comes back, he might still thinks it is the leader\n</code></pre> <p>Things that can go wrong:</p> <pre><code>* In async replication, the new leader might not be up to date with all the writes. If the old leader comes back,\n those not sync writes are usually discarded\n* Discarded writes are problematic for outsiders to the system, and a potential source of problems\n* In fault scenarios, two nodes might believe they are the leaders (this is called split brain), both accepting \nwrites, leading to inconsistency\n* Choosing the right timeout is problematic as it is a trade-off between availability and unnecessary failovers\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementation-of-replication-logs","title":"Implementation of Replication Logs","text":"<p>There are several leader-based replication methods:</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#statement-based-replication","title":"Statement-based replication","text":"<p>The leader logs every write request (statement) that it executes and sends that statement log to its followers. It is now being replaced due to:</p> <pre><code>* If there is a call to a non-deterministic function like NOW() or RAND(), which would generate unsync data\n* If statements use an autoincrementing column or they depend on existing data, the statements needs to be \nexecuted in order\n* Statements with side effects might result in different results in the replicas\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#write-ahead-log-wal-shipping","title":"Write-ahead log (WAL) shipping","text":"<p>Either for log structured storage engines or for B-trees, the log is an append-only sequence of bytes containing all writes to the DB. This log can be used to create another replica. The main disadvantage is that the log describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks, leading to problems with version upgrading.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#logical-row-based-log-replication","title":"Logical (row-based) log replication","text":"<p>An alternative is to use different log formats for replication and for the storage engine, which allows the replication log to be decoupled from the storage engine internals (this is called logical log). This log:</p> <pre><code>* For an inserted row, the log contains the new values of all columns\n* For a deleted row, the log contains enough information to uniquely identify the row that was deleted (PK or\nall columns need to be logged)\n* For an updated row, the log contains enough information to uniquely identify the updated row (same than before)\n</code></pre> <p>Logical log replication is decoupled from the underlying database software, and it is easier for an external application to parse.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#trigger-based-replication","title":"Trigger-based replication","text":"<p>Replication might be needed to be moved up to the application layer. There are tools to make this like Oracle GoldenGate but there are other features to do this: triggers and stored procedures. A trigger lets you register custom application code that is automatically executed when a data change occurs in a database system, but is more limited and prone to bugs.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#problems-with-replication-lag","title":"Problems with Replication Lag","text":"<p>For workloads that consist of mostly reads and only a small percentage of writes create many followers, and distribute the read requests across those followers. This approach only realistically works with asynchronous replication, but reads from asynchronous followers, may see outdated information if the follower has fallen behind. If you stop writing to the database and wait a while, the followers will eventually catch up and become consistent with the leader (known as eventual consistency). The delay between a write happening on the leader and being reflected on a follower (the _ replication lag_), can cause problems if the lag is big.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reading-your-own-writes","title":"Reading Your Own Writes","text":"<p>When a user needs to submit data and see the results, read-after-write consistency is needed, also known as read-your-writes consistency. This guarantees that if the user reloads the page, they will always see any updates they submitted themselves, but makes no promises about other users. There are various possible techniques:</p> <pre><code>* When reading something the user has modified, read it from the leader if possible, if not from one follower. \nThere should be a way to query certain type of information from the leader: i.e. the user profile\n* If most of things in the application are editable, you can select to read stuff that has been write since a \nperiod of time (i.e. one minute) from the leader\n* The client can remember the timestamp of its most recent write, the system can ensure that the replica serving\n any reads for that user reflects updates at least until that timestamp (can be a logic timestamp, like an offset)\n* If the replicas are splitted across datacenters, requests that needs to be served by the leader must be routed \nto the datacenter that contains the leader\n</code></pre> <p>If the user access the application from many devices, you need cross-device read-after-write consistency, consider:</p> <pre><code>* Metadata needs to be centralized if the timestamp is used to retrieve the updates (clocks of the devices might \ndiffer)\n* For distributed replicas, we can't assume that requests would be routed to the same datacenter (devices \nmight be in different networks), you need to ensure this programatically.\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#monotonic-reads","title":"Monotonic Reads","text":"<p>When a user makes several reads from different replicas, user might see things moving backward in time. Monotonic reads guarantees that this doesn't happen by making sure that each user always makes their reads from the same replica.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#consistent-prefix-reads","title":"Consistent Prefix Reads","text":"<p>If a user querys a replica that is far behind the leader, he might get information from a more updated one, making it looks like he received the answer before the query. Consistent prefix reads guarantees that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order. However, in many distribute systems different partitions operate independently, so there is no global ordering of writes. One solution is to make sure that any writes that are causally related to each other are written to the same partition.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#solutions-for-replication-lag","title":"Solutions for Replication Lag","text":"<p>Transactions exists in databases to provide stronger guarantees so that the application can be simpler, but this is easer to achieve in single node system than in distributed ones.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-leader-replication","title":"Multi-Leader Replication","text":"<p>Leader-based application are limited by the fact that there is a single leader. In multi-leader configuration (or master\u2013master or active/active replication), each leader simultaneously acts as a follower to the other leaders.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#use-cases-for-multi-leader-replication","title":"Use Cases for Multi-Leader Replication","text":"<p>This configuration rarely makes sense within a single datacenter, but it might make sense in certain situations:</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-datacenter-operation","title":"Multi-datacenter operation","text":"<p>In a multi-leader configuration, you can have a leader in each datacenter: each datacenter's leader replicates its changes to the leaders in other datacenters. Single-leader and multi-leader configurations in multidatacenter deployment differs in:</p> <pre><code>* Performance: In single-leader configuration every write must go over the internet to the datacenter with the \nleader, adding latency. In multi-leader configuration, every write can be processed in the local datacenter and\nis replicated asynchronously to the other datacenters, hidding the write lag to the final users\n* Tolerance of datacenter outages: Promoting a leader in a different datacenter has to happen in single leader \nconfiguration, in multi leader configuration, each datacenter can continue operating independently\n* Tolerance of network problems: multi leader configuration handles better network problems\n</code></pre> <p>Multi-leadership is usually implemented with external tools, and one of the issues is that it has to solve the problem of concurrent writing to the same data in different datacenters. Other problems that might arise are autoincrementing keys, triggers, and integrity constraints.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#clients-with-offline-operation","title":"Clients with offline operation","text":"<p>In the case where you need changes made by an application (possibly in multiple devices) when being offline, and you need the changes to be on sync in every device next time the device gets a connection, usually a local database that acts as a leader is maintain, and then a multi-leader replication process happens when the connection is established (each device is a datacenter).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#collaborative-editing","title":"Collaborative editing","text":"<p>In Real-time collaborative editing (like in Google docs), you need to guarantee no editing conflicts, so a lock on the document must be obtained first, other users have to wait to get the lock to make modifications on the document. This collaboration model is equivalent to single-leader replication with transactions on the leader. For faster collaboration, you may want to make the unit of change very small (e.g., a single keystroke) and avoid locking.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#handling-write-conflicts","title":"Handling Write Conflicts","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#synchronous-versus-asynchronous-conflict-detection","title":"Synchronous versus asynchronous conflict detection","text":"<p>In a multi-leader setup, conflicts are detected asynchronously unless you make it synchronous by waiting for the rest of the leader replicas to write the data, but this adds lag.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#conflict-avoidance","title":"Conflict avoidance","text":"<p>If the application can ensure that all writes for a particular record go through the same leader, then conflicts cannot occur. Sometimes you might want to change the designated leader for a record, so conflict avoidance breaks down, and you have to deal with the possibility of concurrent writes on different leaders.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#converging-toward-a-consistent-state","title":"Converging toward a consistent state","text":"<p>In a single-leader database, if there are several updates to the same field, the last write determines the final value of the field. In multi-leader writes, the order of writes is not defined from datacenter to datacenter, thus conflicts must be solver in a convergent way. Possible solutions are:</p> <pre><code>* Give each write a unique ID, pick the write with the highest ID as the winner, and throw away the other writes.\n (last write wins or LWW). This approach is prone to data loss\n* Give each replica a unique ID, and let writes that originated at a higher-numbered replica always take \nprecedence over writes that originated at a lower-numbered replica\n* Somehow merge the values together (application logic)\n* Record the conflict in an explicit data structure that preserves all information, and write application code \n that resolves the conflict at some later time.\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#custom-conflict-resolution-logic","title":"Custom conflict resolution logic","text":"<p>Two approaches:</p> <pre><code>* On write : Conflict handler is called as soon as the database system detects a conflict.\n* On read: When a conflict is detected, all the conflicting writes are stored. The next time the data is read, \nthese multiple versions of the data are returned to the application. The application may prompt the user or\nautomatically resolve the conflict, and write the result back to the database\n</code></pre> <p>Each conflict is considered separately for conflict resolution even if they are in the same transaction.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-leader-replication-topologies","title":"Multi-Leader Replication Topologies","text":"<p>A replication topology describes the communication paths along which writes are propagated from one node to another. With two leaders, each write from one leader has to be replicated to the other. With more than two, there are various options:</p> <pre><code>* All to all: Most common, every leader sents its writes to every other leader\n* Circular: Each node receives and sends writes from and to a single node, making a ring (MySQL uses this)\n* Star topology: One designated root node forwards writes to all of the other nodes (can be generalized to a tree)\n</code></pre> <p>To prevent infinite replication loos in star and circular topologies, each node is given a unique identifier, and in the replication log, each write is tagged with the identifiers of all the nodes it has passed through. In these topologies, if a node fails can cause problem to other nodes, although the topology can be reconfigured to ignore the failed node. All to all topologies can have problems with the different speed of the network links, therefore some writes may overtake others. To order write events correctly, version vectors can be used.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#leaderless-replication","title":"Leaderless Replication","text":"<p>Some data storage systems abandons the concept of a leader and allows replicas to directly accept writes from clients. In some implementations, the client directly sends its writes to several replicas, in others, a coordinator node does this on behalf of the client, that coordinator does not enforce a particular ordering of writes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#writing-to-the-database-when-a-node-is-down","title":"Writing to the Database When a Node Is Down","text":"<p>In a leaderless configuration, failover does not exist (client might choose to ignore a failure in writing to a replica). The client send read request to several nodes in parallel to avoid getting stale values in case a replica couldn't get a write in the past. Version numbers are used to determine which value is newer.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#read-repair-and-anti-entropy","title":"Read repair and anti-entropy","text":"<p>The replication scheme should ensure that eventually all the data is copied to every replica:</p> <pre><code>* Read repair: The client detect stale values and writes back the newer value to that replica\n* Anti-entropy process: A background process constantly looks for differences in replicas and copies missing data\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#quorums-for-reading-and-writing","title":"Quorums for reading and writing","text":"<p>If there are n replicas (usually an odd number), every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. As long as w + r &gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we're reading from must be up to date. Reads and writes that follows this rule are called quorum reads and writes:</p> <pre><code>* If w &lt; n, we can still process writes if a node is unavailable\n* If r &lt; n, we can still process reads if a node is unavailable\n* If fewer than the required w or r nodes are available, writes or reads return an error\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#limitations-of-quorum-consistency","title":"Limitations of Quorum Consistency","text":"<p>w and r might be set to smaller numbers, so that w + r \u2264 n: You are more likely to read stale values, but this allows lower latency and higher availability. Even with w + r &gt; n, there are edge cases where stale values are returned:</p> <pre><code>* In sloppy quorums, w writes may end up on different nodes than the r reads, there is no guarantee of overlap\n* If two writes occur concurrently (not clear which happened first), the only option is to merge the concurrent \nwrites. If a winner is picked based on a timestamp (LWW), writes can be lost due to clock skew\n* If a write happens concurrently with a read, the write may be reflected on only some of the replicas\n* If a write succeeded on some replicas but failed on others and overall succeeded on fewer than w replicas, it \nis not rolled back on the replicas where it succeeded. This means that if a write was reported as failed, \nsubsequent reads may or may not return the value from that write\n* If a node carrying a new value fails, and its data is restored from a replica carrying an old value, the \nnumber of replicas storing the new value may fall below w, breaking the quorum condition.\n* Unlucky timing can cause stale values to be read\n</code></pre> <p>Due to all of the above, stronger guarantees generally require transactions or consensus.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#monitoring-staleness","title":"Monitoring staleness","text":"<p>For leader-based replication, writes are applied to the leader and to followers in the same order, and each node has a position in the replication log. By subtracting a follower's current position from the leader's current position, you can measure the amount of replication lag. In systems with leaderless replication, if the database only uses read repair (no anti-entropy), there is no limit to how old a value might be.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#sloppy-quorums-and-hinted-handoff","title":"Sloppy Quorums and Hinted Handoff","text":"<p>Databases with leader\u2010less replication are appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads. In a large cluster it's likely that a client can connect to some database nodes during a network interruption, just not to the nodes that it needs to assemble a quorum for a particular value. In that case, database designers face a trade-off:</p> <pre><code>* Is it better to return errors to all requests for which we cannot reach a quorum of w or r nodes\n* It accepts writes anyway, and write them to some nodes that are reachable but aren't among the n nodes\n on which the value usually lives\n</code></pre> <p>The latter is called sloppy quorum, once the network interruption is fixed, any writes that one node temporarily accepted on behalf of another node are sent to the appropriate \u201chome\u201d nodes (called hinted handoff). A sloppy quorum is only an assurance of durability, namely that the data is stored on w nodes somewhere. There is no guarantee that a read of r nodes will see it until the hinted handoff has completed.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-datacenter-operation_1","title":"Multi-datacenter operation","text":"<p>Leaderless replication is also suitable for multi-datacenter operation, with replication between database clusters happening asynchronously in the background, in a style that is similar to multi-leader replication.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#detecting-concurrent-writes","title":"Detecting Concurrent Writes","text":"<p>Events may arrive in a different order at different nodes, due to variable network delays and partial failures in multi-leader replication, read repair or hinted handoff. Several approaches exist for achieving eventual convergence</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#last-write-wins-discarding-concurrent-writes","title":"Last write wins (discarding concurrent writes)","text":"<p>Newer values replaces older ones in each replica, therefore a way of unambiguously determining which write is more recent is needed. LWW can be used but is tricky cause even if they were reported as successful to the client (because they were written to w replicas), only one will survive and the others will be discarded silently. The only safe way of using a database with LWW is to ensure that a key is only written once and treated as immutable, avoiding any concurrent updates to the same key (i.e. giving each write operation a unique key).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-happens-before-relationship-and-concurrency","title":"The \u201chappens-before\u201d relationship and concurrency","text":"<p>A value B is causally dependent on value A if B depends on A and should therefore happen after. In the other hand we say that two operations are concurrent if neither happens before the other. A mechanism should exist to determine concurrency: two operations concurrent if they are both unaware of each other, regardless of the physical time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#capturing-the-happens-before-relationship","title":"Capturing the happens-before relationship","text":"<p>In order to determine if a concurrent write has happened, we can use the following algorithm:</p> <pre><code>* The server maintains a version number for every key, increments it every time that key is written, and stores \nthe new value along with the value written\n* When a client reads a key, the server returns all values that have not been over\u2010written, as well as the \nlatest version number. A client must read a key before writing\n* When a client writes a key, it must include the version number from the prior read, and it must merge together \nall values that it received in the prior read\n* When the server receives a write with a particular version number, it can over\u2010write all values with that \nversion number or below, but it must keep all values with a higher version number\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#merging-concurrently-written-values","title":"Merging concurrently written values","text":"<p>The previous algorithm ensures that no data is silently dropped, but adds complexity in the clients which has to merge written values. These values are sometimes called siblings. Merging values has similarities with multi-leader write merging, and can be done with LWW, or using the union of the two writes which can lead to problems in case of removing elements. For that purpose, usually a marker of deletion (called tombstone) with the version number is stored.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#version-vectors","title":"Version vectors","text":"<p>If more replicas without leader where added to the previous scenario, we need to use a version number per replica and per key, the collection of version numbers from all the replicas is called a version vector, and allows the database to distinguish between overwrites and concurrent writes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-6-partitioning","title":"Chapter 6: Partitioning","text":"<p>Partitions are like a small databases, but supports operations that touch multiple partitions at the same time. Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-of-key-value-data","title":"Partitioning of Key-Value Data","text":"<p>Our goal with partitioning is to spread the data and the query load evenly across nodes (if the partitioning is unfair so that some partitions have more data or queries than others, we call it skewed). The simplest approach for avoiding hot spots would be to assign records to nodes randomly.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-by-key-range","title":"Partitioning by Key Range","text":"<p>Partitions are assigned (manually or automatically) as a continuous range of keys (like a dictionary index). Ranges need not to be evenly spaced if the data is not evenly distributed. This approach is used by MongoDB, BigTable, HBase... Keys can be ordered within partitions. This approach can lead to hotspots</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-by-hash-of-key","title":"Partitioning by Hash of Key","text":"<p>A hash function is used to determine the partition for a given key. The hash function need not be cryptographically strong, you can assign each partition a range of hashes, and every key whose hash falls within a partition's range will be stored in that partition. A table in Cassandra can be declared with a compound primary key consisting of several columns. Only the first part of that key is hashed to determine the partition, but the other columns are used as a concatenated index for sorting the data in Cassandra's SSTables</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#skewed-workloads-and-relieving-hot-spots","title":"Skewed Workloads and Relieving Hot Spots","text":"<p>In the extreme case where all reads and writes are for the same key, you still end up with all requests being routed to the same partition (for example a celebrity twitter account). A common technique to solve this is if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. Extra work would have to be done around combining those keys and keeping track which keys were splitted in that way.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-and-secondary-indexes","title":"Partitioning and Secondary Indexes","text":"<p>A secondary index usually doesn't identify a record uniquely but rather is a way of searching for occurrences of a particular value. The problem with secondary indexes is that they don't map neatly to partitions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-secondary-indexes-by-document","title":"Partitioning Secondary Indexes by Document","text":"<p>If you have a list of documents distributed in partitions representing cars, and you want to allow users of that database to search for things like color or brand, each partition would have to have secondary indexes to map this features, covering only the cars in that partition. To do CRUD over a document, you have to interact with a single partition. A document-partitioned index is also known as a local index. Care has to be taken to avoid putting all potential documents that would be returned in a query (such as all the red cars) in a single partition. Querys are sent to every partition and combined back (known as scatter/gather), and they are prone to tail latency.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-secondary-indexes-by-term","title":"Partitioning Secondary Indexes by Term","text":"<p>We can also construct a global index that covers data in all partitions (namely term index). A global index must also be partitioned, but it can be partitioned differently from the primary key index: the term we're looking for determines the partition of the index, we can partition the index by the term itself, or using a hash of the term. It is more efficient than the scatter/gather approach as only one partition is queried, the downside of a global index is that writes are slower and more complicated (a write on a document needs to modify several partitions of the indexes). Updates in secondary indexes are usually asynchronous for this reason.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#rebalancing-partitions","title":"Rebalancing Partitions","text":"<p>The process of moving load from one node in the cluster to another is called rebalancing. Rebalancing should at least:</p> <pre><code>* Distribute the data load fairly between the nodes in the cluster\n* While rebalancing is happening, the database should continue accepting reads and writes\n* No more data than necessary should be moved between nodes\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#strategies-for-rebalancing","title":"Strategies for Rebalancing","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#how-not-to-do-it-hash-mod-n","title":"How not to do it: hash mod N","text":"<p>The problem with the mod N approach is that if the number of nodes N changes, most of the keys will need to be moved from one node to another.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#fixed-number-of-partitions","title":"Fixed number of partitions","text":"<p>A simple solution is to create more partitions than nodes, so if new nodes are added, whole partitions can be moved to that node, more powerful nodes can get more partitions to get more load. In this configuration, the number of partitions is usually fixed when the database is first set up and not changed afterward. It is hard to get the number of partitions right for very variable datasets.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dynamic-partitioning","title":"Dynamic partitioning","text":"<p>Fixed partition number is not the most indicate solution for key range partitioning, therefore they usually allocate partitions dynamically: When a partition grows to exceed a configured size, it is split into two partitions so that approximately half of the data ends up on each side of the split (similar to a B tree). An empty database starts with a single partition,so all writes have to be processed by a single node while the other nodes sit idle until the dataset is first split. Some databases allow an initial set of partitions to be configured on an empty database ( pre-splitting). But pre-splitting requires that you already know what the key distribution is going to look like.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning-proportionally-to-nodes","title":"Partitioning proportionally to nodes","text":"<p>Both in fixed and dynamic partitioning, the number of partitions is independent of the number of nodes in the cluster. Another approach is to make the number of partitions proportional to the number of nodes, this is to have a fixed number of partitions per node. When a new node joins the cluster, it randomly chooses a fixed number of existing partitions to split, and then takes ownership of one half of each of those split partitions while leaving the other half of each partition in place. Picking partition boundaries randomly requires that hash-based partitioning is used.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#operations-automatic-or-manual-rebalancing","title":"Operations: Automatic or Manual Rebalancing","text":"<p>Fully automated rebalancing require less operational work to do for normal maintenance but it can be unpredictable. Such automation can be dangerous in combination with automatic failure detection.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#request-routing","title":"Request Routing","text":"<p>As partitions are rebalanced, the assignment of partitions to nodes changes and request have to be issued to the new holders of partitions (This is an instance of a more general problem called service discovery). There are different approaches to the problem:</p> <pre><code>* Allow clients to contact any node, if the node has the partitions it responds to the query otherwise it \nprovides the address to the node\n* Send all requests from clients to a routing tier first (load balancer), which replies with the address of the node\n* Require that clients be aware of the partitioning and the assignment of partitions to nodes\n</code></pre> <p>Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata, which maintains the authoritative mapping of partitions to nodes. Other approaches use a gossip protocol among the nodes to disseminate any changes in cluster state.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#parallel-query-execution","title":"Parallel Query Execution","text":"<p>Massively parallel processing (MPP) relational database products,relies on query optimizer to break query complexty into a number of execution stages and partitions, many of which can be executed in parallel on different nodes of the database cluster.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-7-transactions","title":"Chapter 7: Transactions","text":"<p>A transaction is a way for an application to group several reads and writes together into a logical unit. Either the entire transaction succeeds ( commit) or it fails (abort, rollback).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-slippery-concept-of-a-transaction","title":"The Slippery Concept of a Transaction","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-meaning-of-acid","title":"The Meaning of ACID","text":"<p>ACID stands for Atomicity, Consistency, Isolation, and Durability. Systems that do not meet the ACID criteria are sometimes called BASE, which stands for Basically Available, Soft state, and Eventual consistency.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#atomicity","title":"Atomicity","text":"<p>Refers to something that cannot be broken down into smaller parts. If the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#consistency","title":"Consistency","text":"<p>Consistency ensures that you have certain statements about your data (invariants) that must always be true. Some specific invariants that can be checked by the database are using foreign key constraints or uniqueness constraints.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#isolation","title":"Isolation","text":"<p>Isolation means that concurrently executing transactions are isolated from each other. The database ensures that when the transactions have committed, the result is the same as if they had run serially.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#durability","title":"Durability","text":"<p>Durability is the promise that once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#single-object-and-multi-object-operations","title":"Single-Object and Multi-Object Operations","text":"<p>Multi-object transactions require a way of determining which read and write operations belong to the same transaction. Everything between a BEGIN TRANSACTION and a COMMIT statement is considered to be part of the same transaction.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#single-object-writes","title":"Single-object writes","text":"<p>Atomicity can be implemented using a log for crash recovery and isolation using a lock on each object.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-need-for-multi-object-transactions","title":"The need for multi-object transactions","text":"<p>Many distributed datastores have abandoned multi-object transactions because they are difficult to implement across partitions, and they can get in the way in some scenarios where very high availability or performance is required. But multi-objects makes total sense in updating rows it FK, several documents in a denormalized document database or databases with secondary indexes on value changes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#handling-errors-and-aborts","title":"Handling errors and aborts","text":"<p>ACID transactions can be retried if aborted, but in leaderless replication it follows the best effort which can be translated as \"I do as much as I can, but on error I won't undo something done\". Retrying a failed transaction is not perfect, it can lead to duplicates if the transaction succeeded and there were a network problem, can make things worst if it was due to an overloaded node, can be pointless if it was not a transient error like a deadlock, can have side effects even if it is aborted (like sending an email).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#weak-isolation-levels","title":"Weak Isolation Levels","text":"<p>If two transactions don't touch the same data, they can safely be run in parallel, because neither depends on the other. Serializable isolation means that the database guarantees that transactions have the same effect as if they ran serially but this has a performance cost, therefore some systems offers a weaker form of isolation.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#read-committed","title":"Read Committed","text":"<p>This level of isolation makes two guarantees:</p> <pre><code>1. When reading from the database, you will only see data that has been committed (no dirty reads)\n2. When writing to the database, you will only overwrite data that has been committed (no dirty writes)\n</code></pre> <p>Dirty read refers to the act of reading an uncommitted transaction, which should be prevented if the transaction happens at read committed. If two transactions concurrently try to update the same object in a database, with the earlier write being part of a transaction that has not yet committed, if the later write overwrites an uncommitted value we call this a dirty write. Read committed does not prevent the race condition between two counter increments. Databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object and hold it until the transaction is committed. To prevent dirty reds, instead of acquiring the lock on the object which would slow down the reads on long running writes, while the transaction is ongoing any other transactions that read the object are simply given the old value.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#snapshot-isolation-and-repeatable-read","title":"Snapshot Isolation and Repeatable Read","text":"<p>nonrepeatable read or read skew is a temporary inconsistency in a DB due to unfortunate timing during a read operation. It is acceptable on Read Committed, but might not be if:</p> <pre><code>* Backups: if a backup that takes some time is performed, and meanwhile writes happens, some part of the backup \nwould end up with outdated data\n* Analytic queries and integrity checks: On queries that scans large parts of the database, the query is likely \nto observe parts of the database at different points in time\n</code></pre> <p>The solution to this problem is Snapshot Isolation, which guarantees that the transaction sees all the data that was committed in the database at the start of the transaction.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementing-snapshot-isolation","title":"Implementing snapshot isolation","text":"<p>Implementations of snapshot isolation typically use write locks to prevent dirty writes, reads do not require any locks. From a performance point of view, a key principle of snapshot isolation is readers never block writers, and viceversa. The database must potentially keep several different committed versions of an object, this technique is known as multi-version concurrency control (MVCC). Each transaction has an always increasing transaction ID that is used for recovering, writing or deleting purposes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#visibility-rules-for-observing-a-consistent-snapshot","title":"Visibility rules for observing a consistent snapshot","text":"<p>Transaction IDs are used to decide which objects it can see and which are invisible.</p> <pre><code>1. At the start of each transaction, the database makes a list of all the other transactions in progress. Any \nwrites that those transactions have made are ignored, even if the transactions subsequently commit\n2. Any writes made by aborted transactions are ignored\n3. Any writes made by transactions with a later transaction ID are ignored, regardless of whether those \ntransactions have committed\n4. All other writes are visible to the application's queries\n</code></pre> <p>A transaction is visible if at the time when the reader's transaction started, the transaction that created the object had already committed or the object is not marked for deletion (the transaction that requested deletion had not yet committed at the time when the reader's transaction started).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#indexes-and-snapshot-isolation","title":"Indexes and snapshot isolation","text":"<p>Several implementations to solve this problem exists, from indexes simply point to all versions of an object to B-trees and use an append-only/copy-on-write variant that does not overwrite pages of the tree when they are updated.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#preventing-lost-updates","title":"Preventing Lost Updates","text":"<p>Aside of the dirty write case, other problems might arise on concurrent writes. The lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#atomic-write-operations","title":"Atomic write operations","text":"<p>Many databases provide atomic update operations, which remove the need to implement read-modify-write cycles in application code, in situations where atomic operations can be used, they are usually the best choice. Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied (called cursor stability), other option is to force all atomic operations to be executed on a single thread.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#explicit-locking","title":"Explicit locking","text":"<p>If the database's built-in atomic operations don't provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated. This is done by:</p> <pre><code>BEGIN TRANSACTION;\n--FOR UPDATE indicates that the database should take a lock on all rows returned by this query.\nSELECT * FROM figures WHERE name = 'robot' AND game_id = 222 FOR UPDATE; \nUPDATE figures SET position = 'c4' WHERE id = 1234;\nCOMMIT;\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#compare-and-set","title":"Compare-and-set","text":"<p>Some DBs offers compare-and-set operations to avoid lost updates by allowing an update to happen only if the value has not changed since you last read it.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#conflict-resolution-and-replication","title":"Conflict resolution and replication","text":"<p>For multi-leader or leaderless replication, the locks and compare-and-set techniques are not valid cause there is several copies of the data distributed in several machines. A common approach is to allow concurrent writes to create several conflicting versions of a value (siblings) and merge it with special data structures or application code. Last write wins (LWW) conflict resolution method is prone to lost updates</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#write-skew-and-phantoms","title":"Write Skew and Phantoms","text":"<p>There are more race conditions apart of the dirty writes and lost updates. If there are two concurrent writes that check on a condition (for example a minimum number doctors on a call shift) in a database that has snapshot isolation it can happen that the condition for an update is meet given the snapshot value, but then after both writes are committed the condition is not valid anymore. This anomaly is called write skew.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#characterizing-write-skew","title":"Characterizing write skew","text":"<p>In a write skew, the two transactions are updating two different objects and it is a generalization of the lost update problem: two transactions read the same objects, and then update some of those objects (if they update the same object, you get a dirty write or lost update anomaly). Write skews can't be prevented wit atomic writes, automatic detection of lost updates, using DB constrains (because several objects needs to be checked). They are usually prevented with serializable isolation level or by locking the rows the transaction depends on:</p> <pre><code>BEGIN TRANSACTION;\nSELECT * FROM doctors WHERE on_call = true AND shift_id = 1234 FOR UPDATE;\nUPDATE doctors SET on_call = false WHERE name = 'Alice' AND shift_id = 1234;\nCOMMIT;\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#phantoms-causing-write-skew","title":"Phantoms causing write skew","text":"<p>The usual pattern for write skew is as follows:</p> <pre><code>1. A SELECT query checks whether some requirement is satisfied by searching for rows that match a search condition \n2. Depending on the result of the first query, the application code decides how to continue\n3. If the condition is met, the application makes a write to the db and commits the transaction\n4. The effect of this write changes the precondition of the decision of step 2\n</code></pre> <p>The problem is that if the query in step 1 doesn't return any rows cause we are checking for the absence of them, SELECT FOR UPDATE can't attach locks to anything (calling phantoms).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#materializing-conflicts","title":"Materializing conflicts","text":"<p>A possible solution to avoid phantoms we can artificially introduce a lock object into the database. For example create a table with all possible combinations that can appear (for example rooms and slots) and make a select for update on those rows (this is called materializing conflicts).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#serializability","title":"Serializability","text":"<p>The different scenarios covered before shows how difficult is to account of all the possibilities that might produce a race condition, therefore the usual recommendation has been to use the serializable isolation, which is considered the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency. This is achieved through one of these techniques:</p> <pre><code>* Literally executing transactions in a serial order\n* Two-phase locking\n* Optimistic concurrency control techniques such as serializable snapshot isolation\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#actual-serial-execution","title":"Actual Serial Execution","text":"<p>The simplest solution to this problem is to execute only one transaction at a time, in serial order, on a single thread. This is now possible cause with increase and cheaper RAM memory you can have the whole dataset in memory and OLTP transactions are usually short and only make a small number of reads and writes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#encapsulating-transactions-in-stored-procedures","title":"Encapsulating transactions in stored procedures","text":"<p>If a transaction flow for an application depends on a input to progress depending on certain conditions, the data base needs to support a potentially huge number of concurrent transactions, most of them idle. An interactive style of application (sending request back and forth depending on the answer of the previous query), it not feasible for single threaded solutions unless the entire transaction is processed at once in a stored procedure.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#pros-and-cons-of-stored-procedures","title":"Pros and cons of stored procedures","text":"<p>Stored procedures are vendor dependant (PL/SQL, T-SQL, PL/pgSQL), difficult to manage (debugging, code checking, deploying...) and are performance sensitive (since is a sink for a lof of applications).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioning","title":"Partitioning","text":"<p>For applications with high write throughput, the single-threaded transaction processor can become a serious bottleneck. Partition the dataset can be an option to scale, but the database must coordinate the transactions, making the cross-partition speed slower due to the necessary overhead, and it is very dataset dependant.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#summary-of-serial-execution","title":"Summary of serial execution","text":"<p>Transactions must be small and fast, limited active dataset that can fit in memory, write throughput must be low, cross-partition transactions are possible but there is a hard limit to the extent to which they can be used.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#two-phase-locking-2pl","title":"Two-Phase Locking (2PL)","text":"<p>Two-phase allows several transactions to concurrently read the same object as long as nobody is writing to it, but as soon as anyone wants to write an object, exclusive access is required. Writers don't just block other writers they also block readers and vice versa.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementation-of-two-phase-locking","title":"Implementation of two-phase locking","text":"<p>The blocking of readers and writers is implemented by a having a lock on each object in the database. The lock can either be in shared mode or in exclusive mode:</p> <pre><code>* If a transaction wants to read an object, it must acquire the lock in shared mode. Several transactions can hold \nthe lock in shared mode simultaneously, but if another transaction has an exclusive lock on the object, these \ntransactions must wait\n* If a transaction wants to write to an object, it must acquire the lock in exclusive mode. No other \ntransaction may hold the lock at the same time\n* If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The \nupgrade works the same as getting an exclusive lock directly\n* After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction. \nThis is where the name \u201ctwo-phase\u201d comes from: the first phase (while the transaction is executing) is when the\nlocks are acquired, and the second phase (at the end of the transaction) is when all the locks are released\n</code></pre> <p>The database automatically detects deadlocks between transactions and aborts one of them so that the others can make progress. The aborted transaction needs to be retried by the application.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#performance-of-two-phase-locking","title":"Performance of two-phase locking","text":"<p>Performance is much worst in 2PL than in weak isolation, mostly due to reduce concurrency. If one transaction has to wait on another(s), there is no limit on how long it may have to wait. Deadlocks can be frequent also.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#predicate-locks","title":"Predicate locks","text":"<p>Predicate locks (like the ones described in the phantoms), are acquired as follows in 2PL:</p> <pre><code>* A transaction that wants to read objects matching a condition must acquire a shared mode on the conditions of the \nquery (not the resulting rows)\n* A transaction that wants to insert or modify an object must check if either the old or the new value matches \nany existing predicate lock before.\n</code></pre> <p>Predicate lock applies even to objects that do not yet exist in the database (hence the lock in the condition itself).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#index-range-locks","title":"Index-range locks","text":"<p>Due to performance reasons, many 2PL databases implements index-range locking. Which are like a broader case of predicate locking. Instead of looking object room 1 between 1 and 3pm, you lock on room 1 for all times. An approximation of the search condition would be attached to one of the search indexes</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#serializable-snapshot-isolation-ssi","title":"Serializable Snapshot Isolation (SSI)","text":"<p>2PL don't perform well and serial execution don't scale well, serializable snapshot isolation (SSI) provides full serializability, but has only a small performance penalty compared to snapshot isolation.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#pessimistic-versus-optimistic-concurrency-control","title":"Pessimistic versus optimistic concurrency control","text":"<p>SSI is an optimistic concurrency control technique, wich doesn't block any object and allows transactions to continue, only when a transaction wants to commit, the database checks whether anything bad happened, only transactions that executed serializably are allowed to commit. This performs badly if many transactions trying to access the same objects, but then to perform better in if there is enough spare capacity.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#decisions-based-on-an-outdated-premise","title":"Decisions based on an outdated premise","text":"<p>On decisions based on the output of a query on SSI, the transaction is taking an action based on a premise (a fact that was true at the beginning of the transaction. To be safe, the database needs to assume that any change in the query result (the premise) means that writes in that transaction may be invalid.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#detecting-stale-multi-version-concurrency-control-mvcc-reads","title":"Detecting stale multi-version concurrency control (MVCC) reads","text":"<p>When a transaction reads from a consistent snapshot in an MVCC database, it ignores writes that were made by any other transactions that hadn't yet committed at the time when the snapshot was taken. When the transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#detecting-writes-that-affect-prior-reads","title":"Detecting writes that affect prior reads","text":"<p>Another case to consider is when another transaction modifies data after it has been read. When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. This process is similar to acquiring a write lock on the affected key range, but rather than blocking until the readers have committed, the lock acts as a tripwire: it simply notifies the transactions that the data they read may no longer be up to date.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#performance-of-serializable-snapshot-isolation","title":"Performance of serializable snapshot isolation","text":"<p>SSI is very appealing for read-heavy workloads, SSI is less sensitive to slow transactions than 2PL or serial execution.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-8-the-trouble-with-distributed-systems","title":"Chapter 8: The Trouble with Distributed Systems","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#faults-and-partial-failures","title":"Faults and Partial Failures","text":"<p>A failure in a distributed system is usually non-deterministic, resulting in partial failures.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#cloud-computing-and-supercomputing","title":"Cloud Computing and Supercomputing","text":"<p>Supercomputers usually behaves like a big machine with specialized hardware, usually shares the same network and if a fail occurs they are usually shut down for mainteinance. Clod computing is different because the nodes are built from commodity machines, can't go offline because they usually provide high availability services. We need to assume that software is likely to fail and we should still provide a reliable service in that scenario (Building a Reliable System from Unreliable Components).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#unreliable-networks","title":"Unreliable Networks","text":"<p>Shared nothing architectures where machines are connected using internet and each machines has their own memory and disc are the most common way of building systems. In such systems if a sender sends a message the problems are indistinguishable from asynchronous network problems, and the usual approach is to raise a timeout (which doesn't mean that the operation didn't went through).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#network-faults-in-practice","title":"Network Faults in Practice","text":"<p>There is no way around a network fault, whichever the reason is, you do need to know how your software reacts to network problems and ensure that the system can recover from them.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#detecting-faults","title":"Detecting Faults","text":"<p>Even if your system can handle network failure like not routing to a dead node or electing a new leader in case of replicated systems, there is still uncertainty on how much data the failing node actually processed: if you want to be sure that a request was successful, you need a positive response from the application itself.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#timeouts-and-unbounded-delays","title":"Timeouts and Unbounded Delays","text":"<p>Setting the appropriate timeouts can be problematic, if a node has simply slow down, then declared it dead might result on performing the same operation twice. Asynchronous networks have unbounded delays. That is, they try to deliver packets as quickly as possible, but there is no upper limit on the time it may take for a packet to arrive.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#network-congestion-and-queueing","title":"Network congestion and queueing","text":"<p>Network congestion slows down the rate of package sending. This can happen due to several reasons like multiple nodes trying to use the same communication link at the same time, if destination machine has all CPU cores busy (the package will be queued), round robin of CPU cycles in virtualized environments, the TCP own flow control (congestion avoidance or backpressure)... In public clouds, resource utilization and network delays can be highly variable specially in noisy clusters. In that case the timeout is usually chosen experimentally. Even more, timeouts can be configured dinamically by measuring response times and adjusting it automatically (jitter).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#synchronous-versus-asynchronous-networks","title":"Synchronous Versus Asynchronous Networks","text":"<p>In Synchronous networks like the telephone, the bandwidth is reserved and a circuit is established, guaranteeing almost no delay. The packets of a TCP connection in the other hand, opportunistically use whatever network bandwidth is available, but no network is used if a TCP link is idle, this is because they are optimized for bursty traffic.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#unreliable-clocks","title":"Unreliable Clocks","text":"<p>In a distributed system, time is a tricky business, because communication is not instantaneous, each machine on the network has its own clock, which is an actual hardware device. The most commonly used mechanism to synchronize clocks is the Network Time Protocol (NTP).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#monotonic-versus-time-of-day-clocks","title":"Monotonic Versus Time-of-Day Clocks","text":"<p>Modern computers have at least two different kinds of clocks: a time-of-day clock and a monotonic clock.</p> <pre><code>* A time-of-day clock returns the current date and time according to some calendar, they are usually synchronized \nwith NTP which can result in odd time back travels if they are far away from the NTP server\n* A monotonic clock is suitable for measuring a duration, they are guaranteed to always move forward, but the \nabsolute value of the clock is meaningless. NTP may adjust the frequency at which the monotonic clock moves forward, \nbut cannot cause the monotonic clock to jump forward or backward\n</code></pre> <p>In a distributed system, using a monotonic clock for measuring elapsed time (e.g., timeouts) is usually fine, because it doesn't assume any synchronization between different nodes' clocks and is not sensitive to slight inaccuracies of measurement.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#clock-synchronization-and-accuracy","title":"Clock Synchronization and Accuracy","text":"<p>The Time of day clock needs to be synchronized with the NTP server, a computer might reject this synchronization if the clock is too far away from the NTP server. More problems can arise if the firewall does not allow this synchronization to happen or if there is network delays, leap seconds can mess up timing assumptions in systems. It is possible to achieve a more accurate synchronization using precision time protocol (PTP).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#relying-on-synchronized-clocks","title":"Relying on Synchronized Clocks","text":"<p>If you use software that requires synchronized clocks, it is essential that you also carefully monitor the clock offsets between all the machines.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#timestamps-for-ordering-events","title":"Timestamps for ordering events","text":"<p>If two nodes are not synchronized, the events they generate (for example a counter increment), can lead to incorrectly ordering them, which might cause a drop of a value if the strategy followed is a LRW. logical clocks, which are based on incrementing counters rather than an oscillating quartz crystal, are a safer for ordering events.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#clock-readings-have-a-confidence-interval","title":"Clock readings have a confidence interval","text":"<p>Even if a clock has a lot of resolution (even nanoseconds), its accuracy depends on variable drift, reading a clock is more like a range of times, within a confidence interval.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#synchronized-clocks-for-global-snapshots","title":"Synchronized clocks for global snapshots","text":"<p>Snapshot isolation allows read-only transactions to see the database in a consistent state at a particular point in time, without locking and interfering with read-write transactions. A monotonically increasing transaction ID is the usual approach to do this, but in a distributed database this requires coordination. If we have two confidence intervals, each consisting of an earliest and latest possible timestamp, and those two intervals do not overlap, then B definitely happened after A. Only if the intervals overlap are we unsure in which order A and B happened.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#process-pauses","title":"Process Pauses","text":"<p>In an scenario of a single leader partition where a node needs to know if it is still a leader to accept writes, a lease (a lock with a timeout) can be granted, the node accepts writes until the lease is expired and needs to be renewed. This scenario can be problematic if the expiry date on the lease is compared with the internal clock value. In these situations, a process pause can mess up the clock checking, for example a pause after checking if the lease is valid and before accepting the write. This pause can be due to garbage collection, process pause in virtualized environments, I/O pause, disks swaps... A node in a distributed system must assume that its execution can be paused for a significant length of time at any point, even in the middle of a function.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#response-time-guarantees","title":"Response time guarantees","text":"<p>The pauses described before can be eliminated on hard real-time systems like airplane control systems, those in were if no response is given in a certain amount of time, it can provoke a system failure. This is achieved through real-time operating system (RTOS) which allows processes to be scheduled with a guaranteed allocation of CPU time in specified intervals is needed. These systems are limited to critical software cases due to cost.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#limiting-the-impact-of-garbage-collection","title":"Limiting the impact of garbage collection","text":"<p>An emerging idea is to treat GC pauses like brief planned outages of a node, and to let other nodes handle requests from clients while one node is collecting its garbage. If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node, wait for it to finish processing outstanding requests, or planned restart of process (and thus cleaning of long live objects) can be scheduled.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#knowledge-truth-and-lies","title":"Knowledge, Truth, and Lies","text":"<p>A node in the network can only make guesses based on the messages it receives (or doesn't receive) via the network. A node can only find out what state another node is in by exchanging messages with it. If a remote node doesn't respond, there is no way of knowing what state it is in. In a distributed system, we can state the assumptions we are making about the behavior (the system model) and design the actual system in such a way that it meets those assumptions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-truth-is-defined-by-the-majority","title":"The Truth Is Defined by the Majority","text":"<p>If a node can receive messages but can't send them, or after a long GC pause, it can be declared dead, thus we shouldn't rely on single node, but in a quorum of nodes. A majority has to take the decision, to avoid conflicts.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-leader-and-the-lock","title":"The leader and the lock","text":"<p>Frequently, a system requires there to be only one of some thing (single node processing write requests, or hold a lock). Implementing this in a distributed system requires care an quorum.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#fencing-tokens","title":"Fencing tokens","text":"<p>When using a lock or lease to protect access to some resource, we need to ensure that a node that is under a false belief of being \u201cthe chosen one\u201d cannot disrupt the rest of the system. A simple technique that achieves this goal is called fencing. A fencing token is given every time a lock is granted (an increasing number), which is included on the write requests. If a write request is processed by the storage system, it will reject writes with a lower token number.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#byzantine-faults","title":"Byzantine Faults","text":"<p>If a node deliberately wanted to subvert the system's guarantees, it can do so by sending messages with a fake fencing token. A Byzantine fault occurs when a node claim to have received a particular message when in fact it didn't and systems can be Byzantine fault-tolerant. Most Byzantine fault-tolerant algorithms require a supermajority of more than two-thirds of the nodes to be functioning correctly.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#weak-forms-of-lying","title":"Weak forms of lying","text":"<p>It can be worth adding mechanisms to software that guard against weak forms of \u201clying\u201d like invalid messages due to hardware issues, software bugs, and misconfiguration. This usually includes checksums, input sanitization, or multiserver configuration to determine quorums.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#system-model-and-reality","title":"System Model and Reality","text":"<p>Algorithms need to be written in a way that does not depend too heavily on the details of the hardware and software configuration on which they are run. With regard to timing assumptions, three system models are in common use:</p> <pre><code>* Synchronous model: assumes bounded network delay, bounded process pau\u2010 ses, and bounded clock error\n* Partially synchronous model: assumes that systems behaves like a synchronous system most of the time\n* Asynchronous model: an algorithm is not allowed to make any timing assumptions (it does not even have a clock)\n</code></pre> <p>The three most common system models for nodes are:</p> <pre><code>* Crash-stop faults: an algorithm may assume that a node can fail in only one way, namely by crashing\n* Crash-recovery faults: nodes may crash at any moment, and perhaps start responding again after some unknown time. \nNodes are assumed to have stable storage that is preserved across crashes, in-memory state is assumed to be lost\n* Byzantine (arbitrary) faults: Nodes may do absolutely anything, including trying to trick and deceive other nodes\n</code></pre> <p>For modeling real systems, the partially synchronous model with crash-recovery is generally the most useful model.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#correctness-of-an-algorithm","title":"Correctness of an algorithm","text":"<p>To define what it means for an algorithm to be correct, we can describe its properties. An algorithm is correct in some system model if it always satisfies its properties in all situations that we assume may occur in that system model.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#safety-and-liveness","title":"Safety and liveness","text":"<p>The properties mentioned before can be of two types:</p> <pre><code>* safety: defined as nothing bad happens, if they are violated we can point at a particular point in time at \nwhich it was broken (i.e. uniqueness), the violation can't be undone as the damage is already done\n* liveness: defined as something good eventually happens, they may not hold at some point in time (i.e. server \navailability)\n</code></pre> <p>For distributed algorithms, it is common to require that safety properties always hold, in all possible situations of a system model while with liveness properties we are allowed to make caveats.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#mapping-system-models-to-the-real-world","title":"Mapping system models to the real world","text":"<p>The system model is a simplified abstraction of reality, a real implementation may still have to include code to handle the case where something happens that was assumed to be impossible. Theoretical analysis and empirical testing are equally important.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-9-consistency-and-consensus","title":"Chapter 9: Consistency and Consensus","text":"<p>The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, one of the most useful ones is consensus.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#consistency-guarantees","title":"Consistency Guarantees","text":"<p>Most replicated databases provide at least eventual consistency (also named convergence), which means that if you stop writing to the database and wait for some unspecified length of time, then eventually all read requests will return the same value. This is a weak for of guarantee, systems with stronger guarantees may have worse performance or be less fault-tolerant than systems with weaker guarantees.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#linearizability","title":"Linearizability","text":"<p>The idea behind linearizability ( or atomic consistency, strong consistency, immediate consistency, or external consistency) is that the database gives the illusion that there is only one replica so the client would have the same view of the data. This means guaranteeing that the value read is the most recent, up-to-date value, and doesn't come from a stale cache or replica.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#what-makes-a-system-linearizable","title":"What Makes a System Linearizable?","text":"<p>In a linearizable system we imagine that there must be some point in time (between the start and end of the write operation) at which the value written atomically flips from one value to another. Thus, if one client's read returns the new value, all subsequent reads must also return the new value, even if the write operation has not yet completed. Once a new value has been written or read, all subsequent reads see the value that was written, until it is overwritten again.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#relying-on-linearizability","title":"Relying on Linearizability","text":"<pre><code>* Locking and leader election: A system that uses single-leader replication needs to ensure that there is indeed \nonly one leader, not several (split brain). This can be implemented with locks, every node that starts up tries \nto acquire the lock, and the one that succeeds becomes the leader\n* Constraints and uniqueness guarantees: If you want to enforce the uniqueness constraint as the data is written, \nyou need linearizability\n* Cross-channel timing dependencies: If a system needs two different communication channels to do some task (like\n a background resizing process that needs to read from an external storage), you need linearizability\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementing-linearizable-systems","title":"Implementing Linearizable Systems","text":"<p>The most common approach to making a system fault-tolerant is to use replication. But if we compare whether they can be made linearizable:</p> <pre><code>* Single-leader replication (potentially linearizable): Using the leader for reads relies on the assumption that \nyou know for sure who the leader is (not guaranteed with asynchronous replication)\n* Consensus algorithms (linearizable): these protocols contain measures to prevent split brain and stale replicas\n* Multi-leader replication (not linearizable): because concurrently process writes on multiple nodes and \nasynchronously replicate them to other nodes\n* Leaderless replication (probably not linearizable): Depending on the exact configuration of the quorums, and \ndepending on how you define strong consistency, linearizability might be broken\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#linearizability-and-quorums","title":"Linearizability and quorums","text":"<p>It is possible to make Dynamo-style quorums linearizable at the cost of reduced performance: a reader must perform read repair synchronously, before returning results to the application, and a writer must read the latest state of a quorum of nodes before sending its writes. It is safest to assume that a leaderless system with Dynamo-style replication does not provide linearizability.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-cost-of-linearizability","title":"The Cost of Linearizability","text":"<p>Consider what happens if there is a network interruption between the two datacenters. With a multi-leader database, each datacenter can continue operating normally: since writes from one datacenter are asynchronously replicated to the other, the writes are simply queued up and exchanged when network connectivity is restored. If single-leader replication is used,any writes and any linearizable reads must be sent to the leader, for any clients connected to a follower datacenter, those read and write requests must be sent synchronously over the network to the leader datacenter.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-cap-theorem","title":"The CAP theorem","text":"<p>Important considerations:</p> <pre><code>* If your application requires linearizability, and some replicas are disconnected from the other replicas due \nto a network problem, then some replicas cannot process requests while they are disconnected\n* If your application does not require linearizability, then it can be written in a way that each replica can \nprocess requests independently, even if it is disconnected from other replicas\n</code></pre> <p>The CAP theorem only considers one consistency model (linearizability) and one kind of fault (network partitions or nodes that are alive but disconnected from each other), so it has little practical value for designing systems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#linearizability-and-network-delays","title":"Linearizability and network delays","text":"<p>Even in multi core CPUs, reads are not guaranteed to get the value written by another core thread, since there are intermediate caches and buffers. In this case, the reason for dropping linearizability is performance, not fault tolerance, which is applicable to other databases. Weaker consistency models can be much faster than linear ones.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#ordering-guarantees","title":"Ordering Guarantees","text":"<p>Linearizability requires some guarantee of ordering, there are deep connections between ordering, linearizability, and consensus.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#ordering-and-causality","title":"Ordering and Causality","text":"<p>Order preserves causality, some examples of casuality are:</p> <pre><code>* Causal dependency between the question and the answer (prefix reads)\n* A row should exists to be able to modify it (read overtake)\n* A happened before relationship is another expression of causality (concurrent write detection)\n* In snapshot isolation, transaction reads from a consistent snapshot. Reading stale data violates casuality\n* Write skew between transactions, serializable snapshot isolation detects write skew by track\u2010 ing the causal \ndependencies between transactions\n</code></pre> <p>The chains of causally dependent operations define the causal order in the system, what happened before what. If a system obeys the ordering imposed by causality, we say that it is causally consistent.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-causal-order-is-not-a-total-order","title":"The causal order is not a total order","text":"<p>A total order allows any two elements to be compared (i.e natural numbers). The difference between a total order and a partial order is reflected in different database consistency models:</p> <pre><code>* Linearizability: There is a total order of operations, we can always say which operation happened first\n* Causality: Two events are ordered if they are causally related, but they are incomparable if they are \nconcurrent. Casualty defines a partial order (some elements are incomparable)\n</code></pre> <p>According to this definition, there are no concurrent operations in a linearizable datastore.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#linearizability-is-stronger-than-causal-consistency","title":"Linearizability is stronger than causal consistency","text":"<p>Linearizability implies causality: any system that is linearizable will preserve causality correctly. Linearizability is not the only way of preserving causality, causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#capturing-causal-dependencies","title":"Capturing causal dependencies","text":"<p>In order to maintain causality, you need to know which operation happened before which other operation. Causal consistency techniques are similar to the ones for detecting concurrent writes, but it needs to track causal dependencies across the entire database, not just for a single key. Hence these databases needs to know which version of the data was read by the application.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#sequence-number-ordering","title":"Sequence Number Ordering","text":"<p>Sequence numbers helps to guarantee casuality and keep tracks of dependencies. Timestamps for these numbers can come from a logical clock, which is an algorithm to generate a sequence of numbers to identify operations, typically using counters that are incremented for every operation. In a database with single-leader replication, the replication log defines a total order of write operations that is consistent with causality.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#noncausal-sequence-number-generators","title":"Noncausal sequence number generators","text":"<p>For non single-leader databases is a bit less clear how to generate these sequence numbers. There are several methods:</p> <pre><code>* Each node can generate its own independent set of sequence numbers\n* You can attach a timestamp from a time-of-day clock to each operation (used in LWR)\n* You can preallocate blocks of sequence numbers\n</code></pre> <p>These operations are more performant but the sequence numbers they generate are not consistent with causality.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#lamport-timestamps","title":"Lamport timestamps","text":"<p>There is a simple method for generating sequence numbers that is consistent with causality, Called Lamport timestamp. The Lamport timestamp is simply a pair of operation counter and node identifier attached to the operation. This provides total ordering: if you have two timestamps, the one with a greater counter value is the greater timestamp and if the counter values are the same, the one with the greater node ID is the greater timestamp. This is consistent with casuality because every node and every client keeps track of the maximum counter value it has seen so far, and includes that maximum on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#timestamp-ordering-is-not-sufficient","title":"Timestamp ordering is not sufficient","text":"<p>In the case where a node needs to make a decission on the moment, without gathering the results from other nodes to determine if it is possible (for example creation of a unique username), timestamp ordering is not sufficient, you also need to know when that order is finalized. This idea of knowing when your total order is finalized is known as total order broadcast.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#total-order-broadcast","title":"Total Order Broadcast","text":"<p>Total order broadcast is a protocol for exchanging messages between nodes. It requires two safety properties:</p> <pre><code>* Reliable delivery: No messages are lost (if a message is delivered to one node, it is delivered to all nodes)\n* Totally ordered delivery: Messages are delivered to every node in the same order\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#using-total-order-broadcast","title":"Using total order broadcast","text":"<p>Consensus services (ZooKeeper and etcd) implements total order broadcast. Total order broadcast is needed for database replication: if every message represents a write to the database, and every replica processes the same writes in the same order, then the replicas will remain consistent with each other (known as state machine replication). In total order broadcast the order is fixed at the time the messages are delivered, also useful for implementing a lock service that provides fencing tokens.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementing-linearizable-storage-using-total-order-broadcast","title":"Implementing linearizable storage using total order broadcast","text":"<p>Total order broadcast is asynchronous: messages are guaranteed to be delivered reliably in a fixed order, but there is no guarantee about when a message will be delivered. Linearizability is a recency guarantee: a read is guaranteed to see the latest value written. You can build linearizable storage on top of total order broadcast, with the example of the unique username:</p> <pre><code>* Append a message to the log, tentatively indicating the username you want to claim\n* Read the log, and wait for the message you appended to be delivered back to you\n* Check for any messages claiming the username that you want. If the first message for your desired username is \nyour own message, then you are successful (perhaps commmit it by appending another message to the log) and \nacknowledge it to the client\n</code></pre> <p>The procedure described provides sequential consistency, also known as timeline consistency, a slightly weaker guarantee than linearizability).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementing-total-order-broadcast-using-linearizable-storage","title":"Implementing total order broadcast using linearizable storage","text":"<p>The easiest way to build total order broadcast using linearizable storage is to assume you have a linearizable register that stores an integer and that has an atomic increment-and-get operation. Alternatively, an atomic compare-and-set operation would also do the job.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#distributed-transactions-and-consensus","title":"Distributed Transactions and Consensus","text":"<p>The goal of consensus is simply to get several nodes to agree on something. Consensus is important in leader election or Atomic commit (in distributed transactions either all nodes commits or rolls back).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#atomic-commit-and-two-phase-commit-2pc","title":"Atomic Commit and Two-Phase Commit (2PC)","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#from-single-node-to-distributed-atomic-commit","title":"From single-node to distributed atomic commit","text":"<p>On a single node, transaction commitment crucially depends on the order in which data is durably written to disk: first the data, then the commit record to indicate a successful transaction. In distributed transactions it is not sufficient to simply send a commit request to all of the nodes and independently commit the transaction on each one, as problems with communications, uniqueness of keys or even crashes might happen in any node making the transaction inconsistent.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#introduction-to-two-phase-commit","title":"Introduction to two-phase commit","text":"<p>Two-phase commit is an algorithm for achieving atomic transaction commit across multiple nodes. 2PC uses a component that does not normally appear in single-node transactions named coordinator or transaction manager. A 2PC transaction begins with the application reading and writing data on multiple database nodes (called participants in the transaction). In phase 1, the coordinator sends a prepare request to each of the nodes, asking them whether they are able to commit: If all participants reply \u201cyes\u201d, the coordinator sends out a commit request in phase 2, and the commit actually takes place, if any of the participants replies \u201cno\u201d the coordinator sends an abort request to all nodes in phase 2.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#a-system-of-promises","title":"A system of promises","text":"<p>To guarantee that the previous 2PC actually works, the process requires:</p> <pre><code>* When the application wants to begin a distributed transaction, it requests a transaction ID from the coordinator \n(globally unique)\n* The application begins a single-node transaction on each of the participants with the transaction ID\n* When the application is ready to commit, the coordinator sends a prepare request to all participants, tagged with \nthe global transaction ID (or an abort request if any node fails)\n* When a participant receives the prepare request, it makes sure that it can definitely commit the transaction \nunder all circumstances. This includes writing all transaction data to disk, and checking for any conflicts or \nconstraint violations. The participant surrenders the right to abort the transaction, but without committing it\n* When the coordinator has received responses to all prepare requests, it makes a definitive decision. The \ncoordinator must write that decision to its transaction log on disk (called the commit point)\n* Once the coordinator's decision has been written to disk, the commit or abort request is sent to all \nparticipants. If this request fails or times out, the coordinator must retry forever until it succeed\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#coordinator-failure","title":"Coordinator failure","text":"<p>If a participant has received a prepare request and voted \u201cyes\u201d it can no longer abort unilaterally a transaction. If the coordinator crashes or the network fails at this point, the participant can do nothing but wait (this state is called in doubt or uncertain). The only way 2PC can complete is by waiting for the coordinator to recover (polling other participants is not part of the protocol).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#distributed-transactions-in-practice","title":"Distributed Transactions in Practice","text":"<p>Many cloud services choose not to implement distributed transactions due to the operational problems they engender. Two quite different types of distributed transactions are often conflated:</p> <pre><code>* Database-internal distributed transactions: all the nodes participating in the transaction are running the same\n database software\n* Heterogeneous distributed transactions: the participants are two or more different technologies\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#exactly-once-message-processing","title":"Exactly-once message processing","text":"<p>Heterogeneous distributed transactions allow diverse systems to be integrated, a distributed transaction is only possible if all systems affected by the transaction are able to use the same atomic commit protocol. If all side effects of processing a message are rolled back on transaction abort, then the processing step can safely be retried as if nothing had happened.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#xa-transactions","title":"XA transactions","text":"<p>X/Open XA (short for eXtended Architecture) is a standard for implementing two- phase commit across heterogeneous technologies (it is a C API for interfacing with a transaction coordinator). XA assumes that your application uses a network driver or client library to communicate with the participant databases or messaging services.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#holding-locks-while-in-doubt","title":"Holding locks while in doubt","text":"<p>Database transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty writes. The database cannot release those locks until the transaction commits or aborts.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#recovering-from-coordinator-failure","title":"Recovering from coordinator failure","text":"<p>In practice, orphaned in-doubt transactions (transactions for which the coordinator cannot decide the outcome for whatever reason) do occur, sitting forever in the database, holding locks and blocking other transactions. The only way out is for an administrator to manually decide whether to commit or roll back the transactions. Many XA implementations have an emergency escape hatch called heuristic decisions: allowing a participant to unilaterally decide to abort or commit an in-doubt transaction without a definitive decision from the coordinator.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#limitations-of-distributed-transactions","title":"Limitations of distributed transactions","text":"<p>XA transactions introduces major operational problems:</p> <pre><code>* If the coordinator is not replicated but runs only on a single machine, it is a single point of failure for the \nentire system\n* Many server-side applications are developed in a stateless model, with all persistent state stored in a database, \nsince the coordinator logs are required in order to recover in-doubt transactions after a crash, those application \nservers are no longer stateless.\n* Since XA needs to be compatible with a wide range of data systems, it is necessarily a lowest common denominator\n* Distributed transactions thus have a tendency of amplifying failures, if any part of the system is broken, the \ntransaction also fails\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#fault-tolerant-consensus","title":"Fault-Tolerant Consensus","text":"<p>A consensus algorithm must satisfy the following properties:</p> <pre><code>* Uniform agreement: No two nodes decide differently\n* Integrity: No node decides twice\n* Validity: If a node decides value v, then v was proposed by some node\n* Termination: Every node that does not crash eventually decides some value\n</code></pre> <p>Here, termination is a liveness property, whereas the other three are safety properties (2PC does not meet the requirements for termination), although any consensus algorithm requires at least a majority of nodes to be functioning correctly in order to assure termination.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#consensus-algorithms-and-total-order-broadcast","title":"Consensus algorithms and total order broadcast","text":"<p>Most of consensus algorithms don't directly use the formal model described before. Instead, they decide on a sequence of values, which makes them total order broadcast algorithms. Total order broadcast is equivalent to repeated rounds of consensus.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#epoch-numbering-and-quorums","title":"Epoch numbering and quorums","text":"<p>As stated before, single-leader replication needs consensus to avoid the split-brain problem, but it seems that in order to elect a leader, we first need a leader (so we need to solve consensus). All of the consensus protocols discussed don't guarantee that the leader is unique. Instead, they can make a weaker guarantee: the protocols define an epoch number and guarantee that within each epoch, the leader is unique. If there is a conflict between two different leaders in two different epochs, then the leader with the higher epoch number prevails. For every decision that a leader wants to make, it must send the proposed value to the other nodes and wait for a quorum of nodes to respond in favor of the proposal.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#limitations-of-consensus","title":"Limitations of consensus","text":"<p>Consensus algorithms are not used everywhere, because the benefits come at the cost of performance (they are a kind of synchronous replication), always require a strict majority to operate, assume a fixed set of nodes that participate in voting (you can't just add or remove nodes), they generally rely on timeouts to detect failed nodes (producing false positive detections and affecting performance) and some consensus algorithms are particularly affected by network problems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#membership-and-coordination-services","title":"Membership and Coordination Services","text":"<p>Projects like ZooKeeper or etcd are often described as \u201cdistributed key-value stores\u201d, holding small amounts of data that can fit entirely in memory, which is replicated across all the nodes using a fault-tolerant total order broadcast algorithm. Zookeeper provides features that are particularly useful when building distributed systems:</p> <pre><code>* Linearizable atomic operations: Using an atomic compare-and-set operation, you can implement a lock (usually \nimplemented as a lease)\n* Total ordering of operations: fencing tokens are needed to prevent clients from conflicting with each other in \nthe case of a process pause. ZooKeeper provides this by totally ordering all operations and giving each \noperation a monotonically increasing transaction ID (zxid) and version number (cversion)\n* Failure detection: Clients maintain a long-lived session on ZooKeeper servers, and the client and server \nperiodically exchange heartbeats to check that the other node is still alive\n* Change notifications: A client can read locks and values that were created by another client and watch them for\nchanges\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#service-discovery","title":"Service discovery","text":"<p>ZooKeeper, etcd, and Consul are also often used for service discovery\u2014that is, to find out which IP address you need to connect to in order to reach a particular service. Although service discovery does not require consensus, leader election does. For this purpose, some consensus systems support read-only caching replicas. These replicas asynchronously receive the log of all decisions of the consensus algorithm, but do not actively participate in voting.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#membership-services","title":"Membership services","text":"<p>Zookeeper and other similar systems can be seen as a membership service, which determines which nodes are currently active and live members of a cluster. If you couple failure detection with consensus, nodes can come to an agreement about which nodes should be considered alive or not.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-10-batch-processing","title":"Chapter 10: Batch Processing","text":"<p>We can distinguish three different types of systems:</p> <pre><code> * Services (online systems): A service waits for a request or instruction from a client and sends a response back\n * Batch processing systems (offline systems): system takes a large amount of input data, process it, and \n produces some output data\n * Stream processing systems (near-real-time systems): somewhere between online and offline/batch processing, \n consumes inputs (events) and produces outputs shortly after the input is received\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#batch-processing-with-unix-tools","title":"Batch Processing with Unix Tools","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#simple-log-analysis","title":"Simple Log Analysis","text":"<p>It is possible to build a custom log analysis program by using unix tools and chaining different operations together.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chain-of-commands-versus-custom-program","title":"Chain of commands versus custom program","text":"<p>Instead of the chain of Unix commands, you could write similar things with other program languages, but there are differences in readibility and execution times (sorting versus in-memory aggregation of results).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#sorting-versus-in-memory-aggregation","title":"Sorting versus in-memory aggregation","text":"<p>If a job's working set is larger than the available memory, the sorting approach has the advantage that it can make efficient use of disks, in a similar way to SSTables vs LSM-Trees.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-unix-philosophy","title":"The Unix Philosophy","text":"<p>The Unix philosophy is a set of design principles that became popular among the developers and users of Unix:</p> <pre><code>* Make each program do one thing well. To do a new job, build afresh instead of adding new \u201cfeatures\u201d\n* Expect the output of every program to become the input to another. Don't clutter output with extraneous \ninformation. Avoid stringently columnar or binary input formats. Don't insist on interactive input\n* Design and build software, even operating systems, to be tried early, ideally within weeks. Don't hesitate to \nthrow away the clumsy parts and rebuild them\n* Use tools in preference to unskilled help to lighten a programming task, even if you have to detour to build the\n tools and expect to throw some of them out after you've finished using them\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#a-uniform-interface","title":"A uniform interface","text":"<p>If you expect the output of one program to become the input to another program, a compatible interface must be set (a file in unix programs or more precisely a file descriptor). By convention, many (but not all) Unix programs treat this sequence of bytes as ASCII text.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#separation-of-logic-and-wiring","title":"Separation of logic and wiring","text":"<p>Unix tools is their use of standard input (stdin) and standard output (stdout) which defaults to keyboard and screen respectively. Separating the input/output wiring from the program logic makes it easier to compose small tools into bigger systems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#transparency-and-experimentation","title":"Transparency and experimentation","text":"<p>Key concepts of the unix programs:</p> <pre><code>* The input files to Unix commands are normally treated as immutable\n* You can end the pipeline at any point, pipe the output into less, validate the output for debugging purposes\n* You can write the output of one pipeline stage to a file and use that file as input to the next stage\n</code></pre> <p>The biggest limitation of Unix tools is that they run only on a single machine</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#mapreduce-and-distributed-filesystems","title":"MapReduce and Distributed Filesystems","text":"<p>MapReduce is a bit like unix tools, with some differences. In MR files are written once, in a sequential fashion. In MR read and write files happens on a distributed filesystem called HDFS (Hadoop Distributed File System), which is based on a share-nothing architecture. HDFS consists of a daemon process running on each machine, exposing a network service that allows other nodes to access files stored on that machine with a central server called the NameNode keeps track of which file blocks are stored on which machine.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#mapreduce-job-execution","title":"MapReduce Job Execution","text":"<p>To create a MapReduce job, you need to implement two callback functions, the map\u2010 per and reducer:</p> <pre><code>* Mapper: called once for every input record, and its job is to extract the key and value from the input record\n* Reducer:  takes the key-value pairs produced by the mappers, collects all the values belonging to the same \nkey, and calls the reducer with an iterator over that collection of values\n</code></pre> <p>If you need a second sorting stage, you can use the output of a job as the input for another.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#distributed-execution-of-mapreduce","title":"Distributed execution of MapReduce","text":"<p>Unlike Unix, MapReduce can parallelize a computation across many machines, without you having to write code to explicitly handle the parallelism. The MapReduce framework first copies the code to the appropriate machines if it is not already there, and then starts the map task passing one record at a time to the mapper callback. The output is a key-value pair and must be sorted in stages before the reduce phase. Each map task partitions its output by reducer, based on the hash of the key. Each of these partitions is written to a sorted file on the mapper's local disk. Whenever a mapper finishes reading its input file and writing its sorted output files, the MapReduce scheduler notifies the reducers that they can start fetching the output files from that mapper. The reducers connect to each of the mappers and download the files of sorted key-value pairs for their partition (this is known as the shuffle), merging the files from the mappers together preserving the order. The output records from the reducer are written to a file on the distributed filesystem.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#mapreduce-workflows","title":"MapReduce workflows","text":"<p>It is very common for MapReduce jobs to be chained together into workflows since the range of problems you can solve with a single MapReduce job is limited (this chaining is done implicitly by directory name). A batch job's output is only considered valid when the job has completed successfully.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reduce-side-joins-and-grouping","title":"Reduce-Side Joins and Grouping","text":"<p>When we talk about joins in the context of batch processing, we mean resolving all occurrences of some association within a dataset.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#example-analysis-of-user-activity-events","title":"Example: analysis of user activity events","text":"<p>In order to achieve good throughput in a batch process, the computation must be (as much as possible) local to one machine. Making random-access requests over the network for every record you want to process is too slow.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#sort-merge-joins","title":"Sort-merge joins","text":"<p>When the MapReduce framework partitions the mapper output by key and then sorts the key-value pairs, all record with the same ID become adjacent to each other in the reducer input. The reducer processes all of the records for a particular ID in one go, it only needs to keep one record in memory at any one time, and it never needs to make any requests over the network. This algorithm is known as a sort-merge join, since mapper output is sorted by key, and the reducers then merge together the sorted lists of records from both sides of the join.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#bringing-related-data-together-in-the-same-place","title":"Bringing related data together in the same place","text":"<p>One way of looking at this architecture is that mappers \u201csend messages\u201d to the reducers. MapReduce handles all network communication, it also shields the application code from having to worry about partial failures, it transparently retries failed tasks without affecting the application logic.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#handling-skew","title":"Handling skew","text":"<p>Keys with a disproportionately number of related records are known as linchpin objects or hot keys. A MapReduce job is only complete when all of its mappers and reducers have completed, jobs must wait for the slowest reducer to complete before they can start. Load (randomize the hot keys) should be distributed to avoid skewed jobs.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#map-side-joins","title":"Map-Side Joins","text":"<p>In map-side joins, there are no reducers and no sorting. Mappers reads one input file block from the distributed filesystem and writes one output file to the filesystem.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#broadcast-hash-joins","title":"Broadcast hash joins","text":"<p>When a large dataset is joined with a small dataset, the mapper loads the small dataset in memory and does the join by doing a lookup with the key on the hash table it uses to store the dataset. This is called a broadcast hash join. An alternative is to store the small join input in a read-only index on the local disk, without actually requiring the dataset to fit in memory.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioned-hash-joins","title":"Partitioned hash joins","text":"<p>If the inputs to the map-side join are partitioned in the same way, then the hash join approach can be applied to each partition independently. This approach only works if both of the join's inputs have the same number of partitions (known as bucketed map joins in Hive).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#map-side-merge-joins","title":"Map-side merge joins","text":"<p>If the input datasets are not only partitioned in the same way, but also sorted based on the same key, the mapper can perform the same merging operation that would normally be done by a reduce.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#mapreduce-workflows-with-map-side-joins","title":"MapReduce workflows with map-side joins","text":"<p>The output of a reduce-side join is partitioned and sorted by the join key, the output of a map-side join is partitioned and sorted in the same way as the large input (which is important when optimizing join strategies).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-output-of-batch-workflows","title":"The Output of Batch Workflows","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#building-search-indexes","title":"Building search indexes","text":"<p>A batch process is an effective way of building the indexes if you need to perform a full-text search over a fixed set of documents: the mappers partition the set of documents as needed, each reducer builds the index for its partition, and the index files are written to the distributed filesystem. Index are immutable, so a reprocess is needed if anyone changes (although incremental indexes can be build using segment files and compaction).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#key-value-stores-as-batch-process-output","title":"Key-value stores as batch process output","text":"<p>The output of a map reduce job is often some kind of database to be queried from a web application. Writing from the batch job directly to the database server, one record at a time is a bad idea because the time it takes to do the round network trip, the taks run in parallel can overwhelm the database and there might be partial results if a job fails. It is better to write the results to a file, and then load it using a bulk process.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#philosophy-of-batch-process-outputs","title":"Philosophy of batch process outputs","text":"<p>The handling of output from MapReduce jobs should not produce side effects. By treating inputs as immutable and avoiding side effects (such as writing to external data\u2010 bases), batch jobs achieve good performance and are easier to maintain.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#comparing-hadoop-to-distributed-databases","title":"Comparing Hadoop to Distributed Databases","text":"<p>The biggest difference is that MPP databases focus on parallel execution of analytic SQL queries on a cluster of machines, while the combination of MapReduce and a distributed filesystem provides something much more like a general-purpose operating system that can run arbitrary programs.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#diversity-of-storage","title":"Diversity of storage","text":"<p>Databases require you to structure data according to a particular model, whereas files in a distributed filesystem are just byte sequences, which can be written using any data model and encoding. Hadoop has often been used for implementing ETL processes: data from transaction processing systems is dumped into the distributed filesystem in some raw form, and then MapReduce jobs are written to clean up that data, transform it into a relational form, and import it into an MPP data warehouse for analytic purposes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#diversity-of-processing-models","title":"Diversity of processing models","text":"<p>While MPP databases are monolithic, tightly integrated pieces of software providing very good performance on the types of queries for which it is designed, not all kinds of processing can be sensibly expressed as SQL queries. The Hadoop ecosystem includes both random-access OLTP databases such as HBase and MPP-style analytic databases such as Impala (appart of the MapReduce model).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#designing-for-frequent-faults","title":"Designing for frequent faults","text":"<p>MapReduce and MPPs differs also on the handling of faults and the use of memory and disk. Most MPP databases abort the entire query if a node crashes, and either let the user resubmit the query or automatically run it again. MPP databases also prefer to keep as much data as possible in memory. MapReduce is more appropriate for larger jobs that process so much data and run for such a long time that they are likely to experience at least one task failure along the way (rerunning the entire job due to a single task failure would be wasteful), this is designed in this way because the freedom to arbitrarily terminate processes enables better resource utilization in a computing cluster.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#beyond-mapreduce","title":"Beyond MapReduce","text":"<p>Various higher-level programming models were created as abstractions on top of MapReduce.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#materialization-of-intermediate-state","title":"Materialization of Intermediate State","text":"<p>If the output of one job is only ever used as input to one other job, the files on the distributed filesystem are simply intermediate state. The process of writing out this intermediate state to files is called materialization. The downside of this approach is:</p> <pre><code>* A MapReduce job can only start when all tasks in the preceding jobs, whereas processes connected by a Unix pipe \nstarts at the same time, with output being consumed as soon as it is produced. Skew tasks slows the whole processing\n* Mappers are often redundant: they just read back the same file that was just written by a reducer, and \n prepare it for the next stage of partitioning and sorting\n* Storing intermediate state in a distributed filesystem means those files are replicated across several nodes\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dataflow-engines","title":"Dataflow engines","text":"<p>In order to fix these problems, several new execution engines for distributed batch computations were developed (Tez, Spark, Flink), and they handle an entire workflow as one job and are known as dataflow engines. These dataflow engines connects the different stages of a job through operators, they do so by:</p> <pre><code>* Repartition and sorting records by key, which enables sort-merge joins and grouping like in MapReduce\n* Taking several inputs and partitioning them in the same way, but skiping the sorting.\n* For broadcast hash joins, the same output from one operator can be sent to all partitions of the join operator\n</code></pre> <p>This model:</p> <pre><code>* Skips the sorting if it is not needed\n* Removes work done by a mapper it it can be incorporated into the preceding reduce operator\n* The scheduler has an overview of what data is required where, so it can make locality optimizations\n* Saves HDFS I/O time if the intermediate data to be kept in memory or written to local disk\n* Operators can start executing as soon as their input is ready\n* Existing Java Virtual Machine (JVM) processes can be reused to run new operators\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#fault-tolerance","title":"Fault tolerance","text":"<p>Materializing results makes fault tolerance fairly easy to implement. Spark, Flink, and Tez recomputes the intermediate state if the machine is lost ( the framework must keep track of how a given piece of data was computed). When recomputing data, it is important to know whether the computation is deterministic, which matters if some of the lost data has already been sent to downstream operators (in this case the downstream operator is killed also).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#graphs-and-iterative-processing","title":"Graphs and Iterative Processing","text":"<p>Machine learning applications such as recommendation engines often needs to look at graph models in a batch processing context. This needs an iterative style that can't be implemented with MapReduce. This is usually done with an external scheduler that runs a batch process to calculate one step of the algorithm, then when the batch process completes the scheduler checks whether it has finished and rerun the batch again if not.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-pregel-processing-model","title":"The Pregel processing model","text":"<p>The bulk synchronous parallel (BSP) model of computation: one vertex can \u201csend a message\u201d to another vertex, and typically those messages are sent along the edges in a graph (Pregel model). In the Pregel model, a vertex remembers its state in memory from one iteration to the next, so the function only needs to process new incoming messages.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#fault-tolerance_1","title":"Fault tolerance","text":"<p>Pregel allows messages to be batched with less waiting for communication. Pregel model guarantees that all messages sent in one iteration are delivered in the next iteration, the prior iteration must completely finish, and all of its messages must be copied over the network, before the next one can start.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#parallel-execution","title":"Parallel execution","text":"<p>A vertex does not need to know on which physical machine it is executing; when it sends messages to other vertices, it simply sends them to a vertex ID, so the framework may partition the graph in arbitrary ways. In practice, no attempt to group related vertices together is made, resulting in a lot of cross-machine communication overhead and big intermediate states.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#high-level-apis-and-languages","title":"High-Level APIs and Languages","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-move-toward-declarative-query-languages","title":"The move toward declarative query languages","text":"<p>Hive, Spark, and Flink have cost-based query optimizers so the framework can analyze the properties of the join inputs and automatically decide which of the aforementioned join algorithms would be most suitable for the task at hand. If joins are specified in a declarative way, the application simply states which joins are required.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-11-stream-processing","title":"Chapter 11: Stream Processing","text":"<p>A \u201cstream\u201d refers to data that is incrementally made available over time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#transmitting-event-streams","title":"Transmitting Event Streams","text":"<p>In a stream processing context, a record is more commonly known as an event: a small, self- contained, immutable object containing the details of something that happened at some point in time. An event is generated once by a producer (or _publisher or sender), and then potentially processed by multiple consumers (subscribers or recipients). In a streaming system, related events are usually grouped together into a topic or stream. When moving toward continual processing with low delays, polling becomes expensive if the datastore is not designed for this kind of usage. The more often you poll, the lower the percentage of requests that return new events, and thus the higher the overheads become. Instead, it is better for consumers to be notified when new events appear.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#messaging-systems","title":"Messaging Systems","text":"<p>A messaging system allows multiple producer nodes to send messages to the same topic and allows multiple consumer nodes to receive messages in a topic (publish/subscribe model). Important considerations are:</p> <pre><code>* If the producers send messages faster than the consumers can process them, the system can either drop or buffer\n messages or apply backpresure (blocking the producer from sending more messages)\n* If nodes crash or temporarily go offline, messages can be lost to achieve higher throughput and lower latency, \nor stored if there is some combination of writing to disk and/or replication\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#direct-messaging-from-producers-to-consumers","title":"Direct messaging from producers to consumers","text":"<p>Direct messaging generally require the application code to be aware of the possibility of message loss, examples of direct messaging are UDP multicast in finance services, brokerless messaging libraries or consumer exposing HTTP or RPC request endpoints.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#message-brokers_1","title":"Message brokers","text":"<p>An alternative is to send messages via a message broker (also known as a message queue), which is a kind of database that is optimized for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. These systems can more easily tolerate clients that come and go, and the question of durability is moved to the broker instead. They generally allow unbounded queueing and consumers are generally asynchronous.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#message-brokers-compared-to-databases","title":"Message brokers compared to databases","text":"<p>Some message brokers can even participate in two-phase commit protocols using XA or JTA, although as oppose to databases, these systems deletes successfully delivered messages, the working set is small, often support some way of subscribing to a subset of topics matching some pattern and do not support arbitrary queries but notify clients when data changes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multiple-consumers","title":"Multiple consumers","text":"<p>When multiple consumers read messages in the same topic, two main patterns of messaging are used:</p> <pre><code>* Load balancing: Each message is delivered to one of the consumers, so the consumers can share the work of \nprocessing the messages in the topic. Useful when the messages are expensive to process\n* Fan-out: Each message is delivered to all of the consumersEach message is delivered to all of the consumers.\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#acknowledgments-and-redelivery","title":"Acknowledgments and redelivery","text":"<p>In order to ensure that the message is not lost, message brokers use acknowledgments. The broker assumes that the message was not processed if they don't receive this confirmation, and redelivers the message. Handling this case requires an atomic commit protocol, and wrong ordering of events can potentially happen in case of failure.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#partitioned-logs","title":"Partitioned Logs","text":"<p>In AMQP/JMS-style messaging approach, receiving a message is destructive if the acknowledgment causes it to be deleted from the broker, so you cannot run the same consumer again and expect to get the same result.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#using-logs-for-message-storage","title":"Using logs for message storage","text":"<p>In log message brokers, a producer sends a message by appending it to the end of the log, and a consumer receives messages by reading the log sequentially. Different partitions can then be hosted on different machines, making each partition a separate log that can be read and written independently from other partitions. Within each partition, the broker assigns a monotonically increasing sequence number, or offset, to every message, so messages within a partition are totally ordered. There is no ordering guarantee across different partitions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#logs-compared-to-traditional-messaging","title":"Logs compared to traditional messaging","text":"<p>In these systems, the broker can assign entire partitions to nodes in the consumer group, each client then consumes all the messages in the partitions it has been assigned. This approach means that:</p> <pre><code>* The number of nodes sharing the work of consuming topics can be at most the number of log partitions in that topic\n* If a single message is slow to process, it holds up the processing of subsequent messages in that partition\n</code></pre> <p>In situations where messages may be expensive to process and you want to parallelize processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. In situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#consumer-offsets","title":"Consumer offsets","text":"<p>The broker does not need to track acknowledgments for every single message since all messages with an offset less than a consumer's current offset have already been processed (similar to the log sequence number that in single-leader database replication).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#disk-space-usage","title":"Disk space usage","text":"<p>To reclaim disk space, the log is actually divided into segments, which are deleted depending on the retention policies.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#replaying-old-messages","title":"Replaying old messages","text":"<p>In log-based message brokers, The only side effect of processing is that the consumer offset moves forward. But the offset is under the consumer's control, so it can easily be manipulated if necessary (useful for reprocessing purposes).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#databases-and-streams","title":"Databases and Streams","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#keeping-systems-in-sync","title":"Keeping Systems in Sync","text":"<p>There is no single system that can satisfy data storage, querying, and processing needs. For replication, different approaches can be taken such as taking a full copy of a database, transforming it, and bulk-loading it, or by the use of dual writes (application code that explicitly writes to each of the systems when data changes). Dual writes can suffer from racing conditions thus requires additional concurrency detection mechanisms ( although it works for one replicated database with a single leader).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#change-data-capture","title":"Change Data Capture","text":"<p>Many databases does not have a documented way of getting the log of changes written to them, which makes difficult to take all the changes made in a database and replicate them to a different storage technology such as a search index, cache, or data warehouse which is solved with change data capture (CDC) which is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other systems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#implementing-change-data-capture","title":"Implementing change data capture","text":"<p>CDC makes one database the leader (the one from which the changes are captured), and turns the others into followers. A log-based message broker is well suited for transporting the change events from the source database, since it preserves the ordering of messages. Database triggers can be used to implement CDC, parsing the replication log can be a more robust, but it also comes with challenges, such as handling schema changes. CDC is usually asynchronous: the system of record database does not wait for the change to be applied to consumers before committing it.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#initial-snapshot","title":"Initial snapshot","text":"<p>If you don't have the entire log history, you need to start with a consistent snapshot, which must correspond to a known position or offset in the change log, so that you know at which point to start applying changes after the snapshot has been processed.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#log-compaction","title":"Log compaction","text":"<p>Systems with log compaction functionality, periodically looks for log records with the same key, throws away any duplicates, and keeps only the most recent update for each key. An update with a special null value (a tombstone) indicates that a key was deleted. The same idea can be applied in CDC.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#event-sourcing","title":"Event Sourcing","text":"<p>Event sourcing involves storing all changes to the application state as a log of change events. In event sourcing, the application logic is explicitly built on the basis of immutable events that are written to an event log. Events are designed to reflect things that happened at the application level, rather than low-level state changes.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#deriving-current-state-from-the-event-log","title":"Deriving current state from the event log","text":"<p>An event log by itself is not very useful, because users generally expect to see the cur\u2010 rent state of a system, not the history of modifications. Like with change data capture, replaying the event log allows you to reconstruct the current state of the system. However, log compaction needs to be handled differently: in CDC an even is the entire new version of the record and log compaction can discard previous events for the same key. In event sourcing, an event typically expresses the intent of a user action, not the mechanics of the state update that occurred as a result of the action. In this case, later events typically do not override prior events, and so you need the full history of events to reconstruct the final state. Log compaction is not possible in the same way.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#commands-and-events","title":"Commands and events","text":"<p>The event sourcing philosophy distinguish between events and commands. When a request from a user first arrives, it is initially a command: at this point it may still fail, for example because some integrity condition is violated. Applications must validate that it can execute the command. If it can, then it becomes an event, durable and immutable. At the point when the event is generated, it becomes a fact. A consumer of the event stream is not allowed to reject an event, any validation of a command needs to happen synchronously.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#state-streams-and-immutability","title":"State, Streams, and Immutability","text":"<p>Mutable state and an append-only log of immutable events do not contradict each other. The log of all changes, the changelog, represents the evolution of state over time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#advantages-of-immutable-events","title":"Advantages of immutable events","text":"<p>If you accidentally deploy buggy code that writes bad data to a database, with an append-only log of immutable events it is much easier to diagnose what happened and recover from the problem.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#deriving-several-views-from-the-same-event-log","title":"Deriving several views from the same event log","text":"<p>By separating mutable state from the immutable event log, you can derive several different read-oriented representations from the same log of events. Many of the complexities of schema design, indexing, and storage engines are the result of wanting to support certain query and access patterns, thus you gain a lot of flexibility by separating the form in which data is written from the form it is read. This is known as command query responsibility segregation (CQRS).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#concurrency-control","title":"Concurrency control","text":"<p>Much of the need for multi-object transactions stems from a single user action requiring data to be changed in several different places. With event sourcing, you can design an event such that it is a self-contained description of a user action which requires only a single write in one place\u2014namely appending the events to the log (made atomic).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#limitations-of-immutability","title":"Limitations of immutability","text":"<p>In some circumstances (data privacy regulations, sensitive information), it's not sufficient to delete information by appending another event to the log. Due to the replication of the data, deletion is more a matter of \"making it harder to retrieve the data\" than actually \"making it impossible to retrieve the data.\"</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#processing-streams","title":"Processing Streams","text":"<p>There are broadly three options to process streams:</p> <pre><code>* You can write events into a database, cache, or similar storage system, so they can be queried by other clients\n* You can push the events to users in some way,  a human is the ultimate consumer of the stream\n* You can process one or more input streams to produce one or more output streams\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#uses-of-stream-processing","title":"Uses of Stream Processing","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#complex-event-processing","title":"Complex event processing","text":"<p>Complex event processing (CEP) is an approach for analyzing event streams, especially geared toward the kind of application that requires searching for certain event patterns. CEP systems often use a high-level declarative query language like SQL, which are submitted to a processing engine. When a match is found, the engine emits a complex event with the details of the event pattern that was detected.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stream-analytics","title":"Stream analytics","text":"<p>Analytics tends to be less interested in finding specific event sequences and is more oriented toward aggregations and statistical metrics over a large number of events which are usually computed over fixed time intervals (window).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#maintaining-materialized-views","title":"Maintaining materialized views","text":"<p>Deriving an alternative view onto some dataset so that you can query it efficiently, and updating that view whenever the underlying data changes is called materializing views. Building the materialized view potentially requires all events over an arbitrary time period.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#search-on-streams","title":"Search on streams","text":"<p>Sometimes there is a need to search for individual events based on complex criteria, such as full-text search queries. This is done by formulating a search query in advance, and then continually matching the stream of news items against this query: the queries are stored, and the documents run past the queries, like in CEP.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reasoning-about-time","title":"Reasoning About Time","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#event-time-versus-processing-time","title":"Event time versus processing time","text":"<p>Stream processing algorithms need to be specifically written to accommodate timing and ordering issues, such as events originated in different servers or spikes while processing steady data.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#knowing-when-youre-ready","title":"Knowing when you're ready","text":"<p>A tricky problem when defining windows in terms of event time is that you can never be sure when you have received all of the events for a particular window, or whether there are some events still to come. You need to be able to handle straggler events that arrive after the window has already been declared complete. There is two ways to do this:</p> <pre><code>* Ignore the straggler events, You can track the number of dropped events as a metric, and raise alerts accordingly\n* Publish a correction, an updated value for the window with stragglers included\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#whose-clock-are-you-using-anyway","title":"Whose clock are you using, anyway?","text":"<p>Assigning timestamps to events is even more difficult when events can be buffered at several points in the system (like an app recording metrics offline to push them once is online again). To adjust for incorrect device clocks, one approach is to log three timestamps:</p> <pre><code>* The time at which the event occurred, according to the device clock\n* The time at which the event was sent to the server, according to the device clock\n* The time at which the event was received by the server, according to the server clock\n</code></pre> <p>By subtracting the second timestamp from the third, you can estimate the offset between the device clock and the server clock and apply that offset to the event timestamp to estimate the true time at which the event occurred.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#types-of-windows","title":"Types of windows","text":"<p>Windows over time periods can be defined in several ways:</p> <pre><code>* Tumbling window: Windows has a fixed length, and every event belongs to exactly one window\n* Hopping window: Windows with fixed length, but allows windows to overlap in order to provide some smoothing\n* Sliding window: A sliding window contains all the events that occur within some interval of each other\n* Session window: No fixed duration. Defined by grouping all events for the same user that occur closely in time,\n ending when the user has been inactive for some time\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stream-joins","title":"Stream Joins","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stream-stream-join-window-join","title":"Stream-stream join (window join)","text":"<p>Both input streams consist of activity events, and the join operator searches for related events that occur within some window of time.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stream-table-join-stream-enrichment","title":"Stream-table join (stream enrichment)","text":"<p>One input stream consists of activity events, while the other is a database change\u2010log. To perform this join, the stream process needs to look at one activity event at a time, look up the event's user ID in the database (which might be very slow). Another approach is to load a copy of the database into the stream processor so that it can be queried locally without a network round-trip. The problem is that the database is likely to change over time, so the stream processor's local copy of the database needs to be kept up to date (i.e. with CDC).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#table-table-join-materialized-view-maintenance","title":"Table-table join (materialized view maintenance)","text":"<p>If you want to maintain a user's feed in a social media like in twitter, you need a timeline cache where events about other users are inserted, deleted and updated, for which you need streams of events for tweets (sending and deleting) and for follow relationships (follow/unfollow). Both input streams are database changelogs. In this case, every change on one side is joined with the latest state of the other side.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#time-dependence-of-joins","title":"Time-dependence of joins","text":"<p>All previously described joins require the stream processor to maintain some state based on one join input, and query that state on messages from the other join input, and order here matters. If the ordering of events across streams is undetermined, the join becomes nondeterministic. This is known as a slowly changing dimension (SCD) in data warehousing and it is often addressed by using a unique identifier for a particular version of the joined record.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#fault-tolerance_2","title":"Fault Tolerance","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#microbatching-and-checkpointing","title":"Microbatching and checkpointing","text":"<p>One solution is to break the stream into small blocks, and treat each block like a miniature batch process (microbatching). This implicitly provides a tumbling window equal to the batch size, any jobs that require larger windows need to explicitly carry over state from one microbatch to the next. A variant approach is to periodically generate rolling checkpoints of state and write them to durable storage.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#atomic-commit-revisited","title":"Atomic commit revisited","text":"<p>To give the appearance of exactly-once processing in the presence of faults, all outputs and side effects of processing an event take effect if and only if the processing is successful. These implementations do not attempt to provide transactions across heterogeneous technologies, but instead keep them internal by managing both state changes and messaging within the stream processing framework.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#idempotence","title":"Idempotence","text":"<p>If an operation is not naturally idempotent, it can often be made idempotent with a bit of extra metadata. Idempotent operations can be an effective way of achieving exactly-once semantics with only a small overhead.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#rebuilding-state-after-a-failure","title":"Rebuilding state after a failure","text":"<p>Any stream process that requires state must ensure that this state can be recovered after a failure. One option is to keep the state in a remote datastore and replicate it, or keep state local to the stream processor, and replicate it periodically for performance.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#chapter-12-the-future-of-data-systems","title":"Chapter 12: The Future of Data Systems","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#data-integration","title":"Data Integration","text":"<p>In complex applications, data is often used in several different ways, there is unlikely to be one piece of software that is suitable for all the different circumstances.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#combining-specialized-tools-by-deriving-data","title":"Combining Specialized Tools by Deriving Data","text":"<p>As the number of different representations of the data increases, the integration problem becomes harder.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reasoning-about-dataflows","title":"Reasoning about dataflows","text":"<p>If it is possible for you to funnel all user input through a single syste m that decides on an ordering for all writes, it becomes much easier to derive other representations of the data by processing the writes in the same order.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#derived-data-versus-distributed-transactions","title":"Derived data versus distributed transactions","text":"<p>In the absence of wides pread support for a good distributed transaction protocol, log-based derived data is the most promising approach for integrating different data systems.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-limits-of-total-ordering","title":"The limits of total ordering","text":"<p>Constructing a totally ordered event log is feasible if the system is small enough, however, as they are scaled toward bigger and more complex workloads, limitations begin to emerge:</p> <pre><code>* Constructing a totally ordered log requires all events to pass through a single leader node that decides on the\n ordering. If the throughput of events is greater than a single machine can handle, you need to partition it. The \n order of events in two different partitions is then ambiguous\n* If the servers are spread across multiple geographically distributed datacenters, you typically have a separate \nleader in each datacenter. This implies an undefined ordering of events that originate in two different datacenters\n* In microservices a common design choice is to deploy each service and its  durable state as an independent \nunit (non-shared). When two events originate in different services, there is no defined order for those events\n* Some applications maintain client-side state that is updated immediately on user input, and even continue to \nwork offline . With such applications, clients and servers are very likely to see events in different orders\n</code></pre> <p>Deciding on a total order of events is known as total order broadcast, which is equivalent to consensus which is usually designed for situations in which the throughput of one node is enough to process the entire stream of events.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#ordering-events-to-capture-causality","title":"Ordering events to capture causality","text":"<p>If there is no causal link between events, concurrent events can be ordered arbitrarily. Timing issues might arise if order is required:</p> <pre><code>* \u2022 Logical timestamps can provide total ordering without coordination, but require recipients to handle events \nthat are delivered out of order, and they require additional metadata to be passed around.\n* If you can log an event to record the state of the system that the user saw before making a decision, and give \nthat event a unique identifier, then any later events can reference that ID in order to record the causal dependency\n* Conflict resolution algorithms helps with processing events that are delivered in an unexpected order. They are\n useful for maintaining state, but they do not help if actions have external side effects\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#batch-and-stream-processing","title":"Batch and Stream Processing","text":"<p>The goal of data integration is to make sure that data ends up in the right form in all the right places. Batch and stream processors are the tools for achieving this goal.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#maintaining-derived-state","title":"Maintaining derived state","text":"<p>Batch processing encourages deterministic, pure functions whose output depends only on the input and which have no side effects other than the explicit outputs, treating inputs as immutable and outputs as append-only. Derived data systems could be maintained synchronously, but asynchrony makes systems based on event logs robust: it allows a fault in a part of the system to be contained locally, distributed transactions abort if any participant fails. A partitioned system with secondary indexes needs to send writes to multiple partitions or send reads to all partitions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reprocessing-data-for-application-evolution","title":"Reprocessing data for application evolution","text":"<p>Reprocessing existing data provides a good mechanism for maintaining a system, evolving it to support new features and changed requirements. Derived views allow gradual evolution, you can maintain the old schema and the new schema side by side as two independently derived views onto the same underlying data.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-lambda-architecture","title":"The lambda architecture","text":"<p>The core idea of the lambda architecture is that incoming data should be recorded by appending immutable events to an always-growing dataset. From these events, read-optimized views are derived. The lambda architecture proposes running two different systems in parallel: a batch processing system and a separate stream-processing system.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#unifying-batch-and-stream-processing","title":"Unifying batch and stream processing","text":"<p>Unifying batch and stream processing in one system requires:</p> <pre><code>* The ability to replay historical events through the same processing engine handling the stream of recent events\n* Exactly-once semantics for stream processors, discarding the partial output of any failed tasks\n* Tools for windowing by event time. Processing time is meaningless when reprocessing historical events\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#unbundling-databases","title":"Unbundling Databases","text":"<p>Unix and relational databases has different approaches for the information management problem. Unix presents programmers with a low-level hardware abstraction, relational databases uses a high-level abstraction that hides the complexities of data structures on disk. Unix developed pipes and files (sequences of bytes), DBs developed SQL and transactions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#composing-data-storage-technologies","title":"Composing Data Storage Technologies","text":"<p>There are parallels between the features that are built into databases and the derived data systems that people are building with batch and stream processors.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#creating-an-index","title":"Creating an index","text":"<p>Creating an index in a database is remarkably similar to setting up a new follower replica. When you run CREATE INDEX, the database reprocesses the existing dataset and derives the index as a new view onto the existing data.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-meta-database-of-everything","title":"The meta-database of everything","text":"<p>Batch and stream processors are like elaborate implementations of triggers, stored procedures, and materialized view maintenance routines. Different storage and processing tools can nevertheless be composed into a cohesive system:</p> <pre><code>* Federated databases (unifying reads): It is possible to provide a unified query interface to a wide variety of\nunderlying storage engines and processing methods\u2014an approach known as a federated database or polystore\nA federated query interface follows the relational tradition of a single integrated system with a high-level \nquery language and elegant semantics, but a complicated implementation\n* Unbundled databases (unifying writes): When we compose several storage systems, we need to ensure that all \ndata changes ends in the right places, even in the face of faults. The unbundled approach follows the Unix \ntradition of small tools that do one thing well [22], that communicate through a uniform low-level API (pipes), \nand that can be composed using a higher-level language (the shell)\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#making-unbundling-work","title":"Making unbundling work","text":"<p>The traditional approach to synchronizing writes requires distributed transactions across heterogeneous storage systems but asynchronous event log with idempotent writes is a much more robust and practical approach. The big advantage of log-based integration is loose coupling between the various components, which manifests itself in two ways:</p> <pre><code>* Asynchronous event streams make the system as a whole more robust to outages or performance degradation of \nindividual components\n* Unbundling data systems allows different software components and services to be developed, improved, and \nmaintained independently from each other by different teams\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#unbundled-versus-integrated-systems","title":"Unbundled versus integrated systems","text":"<p>Databases will still be important for particular workloads (query engines in MPP data warehouses), plus a single integrated software product may also be able to achieve better and more predictable performance on the kinds of workloads for which it is designed, compared to a system consisting of several tools that you have composed with application code. The goal of unbundling is not to compete with individual databases on performance for particular workloads; the goal is to allow you to combine several different data\u2010 bases in order to achieve good performance for a much wider range of workloads than is possible with a single piece of software.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#whats-missing","title":"What's missing?","text":"<p>We don't yet have the unbundled-database equivalent of the Unix shell: high-level language for composing storage and processing systems in a simple and declarative way.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#designing-applications-around-dataflow","title":"Designing Applications Around Dataflow","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#application-code-as-a-derivation-function","title":"Application code as a derivation function","text":"<p>When one dataset is derived from another, it goes through some kind of transformation function. The function that creates a derived dataset is not a standard cookie-cutter function like creating a secondary index, custom code is required to handle the application-specific aspects.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#separation-of-application-code-and-state","title":"Separation of application code and state","text":"<p>It makes sense to have some parts of a system that specialize in durable data storage, and other parts that specialize in running application code. The database acts as a kind of mutable shared variable that can be accessed synchronously over the network. The application can read and update the variable, and the database takes care of making it durable, providing some concurrency control and fault tolerance.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dataflow-interplay-between-state-changes-and-application-code","title":"Dataflow: Interplay between state changes and application code","text":"<p>Thinking about applications in terms of dataflow implies thinking much more about the interplay and collaboration between state, state changes, and code that processes them. Application code responds to state changes in one place by triggering state changes in another place, and we can use stream processing and messaging systems for this purpose:</p> <pre><code>* When maintaining derived data, the order of state changes is often important \n* Losing just a single message causes the derived dataset to go permanently out of sync with its data source\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stream-processors-and-services","title":"Stream processors and services","text":"<p>The currently trendy style of application development involves breaking down functionality into a set of services that communicate via synchronous network requests such as REST APIs. Composing stream operators into dataflow systems is similar to the microservices approach. However, the underlying communication mechanism is very different: one-directional, asynchronous message streams rather than synchronous request/response interactions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#observing-derived-state","title":"Observing Derived State","text":"<p>A dataflow systems has a process for creating derived datasets and keeping them up to date called write path (precomputed). At the same time the read path is the process of serving users request you read from the derived dataset (only happens when someone asks for it). The write path is similar to eager evaluation, and the read path is similar to lazy evaluation.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#materialized-views-and-caching","title":"Materialized views and caching","text":"<p>An option to have a balance between reducing indexes to speed up writes and maintain all possible search results to speed up reads is to precompute the search results for only a fixed set of the most common queries to serve them quickly without having to do a look up on the indexes. This would need to be updated when new documents appear that should be included in the results of one of the common queries. There is more work to do on the write path, by precomputing results, but it saves effort on the read path.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#stateful-offline-capable-clients","title":"Stateful, offline-capable clients","text":"<p>Offline-first applications do as much as possible using a local database on the same device, without requiring an internet connection, and sync with remote servers in the background when a network connection is available. Think of the on-device state as a cache of state on the server.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#pushing-state-changes-to-clients","title":"Pushing state changes to clients","text":"<p>Traditionally, a browser only reads the data at one point in time, assuming that it is static. It does not subscribe to updates from the server. Server-sent events (EventSource API) and WebSockets provide communication channels by which a web browser can keep an open TCP connection to a server, and the server can actively push messages to the browser as long as it remains connected. This means pushing state changes all the way to client devices means extending the write path all the way to the end user.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#end-to-end-event-streams","title":"End-to-end event streams","text":"<p>Some development tools alrea dy manage internal client-side state by subscribing to a stream of events representing user input or responses from a server, structured similarly to event sourcing. To extend the write path all the way to the end user, we will need to move away from request/response interaction and toward publish/subscribe dataflows.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#reads-are-events-too","title":"Reads are events too","text":"<p>Some stream processor frameworks allows to query its state by outside clients, turning the stream processor itself into a kind of simple database. Usually, the writes to the store go through an event log, while reads are transient network requests that go directly to the nodes that store the data being queried, but it is also possible to represent read requests as streams of events, and send both the read events and the write events through a stream processor. Writing read events to durable storage enables better tracking of causal dependencies.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-partition-data-processing","title":"Multi-partition data processing","text":"<p>The effort of sending queries through a stream and collecting a stream of responses opens the possibility of distributed execution of complex queries that need to combine data from several partitions.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#aiming-for-correctness","title":"Aiming for Correctness","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-end-to-end-argument-for-databases","title":"The End-to-End Argument for Databases","text":"<p>Just because an application uses a data system that provides compara tively strong safety properties, such as serializable transactions, that does not mean the application is guaranteed to be free from data loss or corruption.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#exactly-once-execution-of-an-operation","title":"Exactly-once execution of an operation","text":"<p>Processing twice a message is a form of data corruption, exactly-once me ans arranging the compu\u2010 tation such that the final effect is the same as if no faults had occurred, even if the operation actually was retried due to some fault.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#duplicate-suppression","title":"Duplicate suppression","text":"<p>Two-phase commit protocols break the 1:1 mapping between a TCP connection and a transaction, and does not ensure that the transaction will only be executed once (for example if the user doesn't receive the confirmation of the transaction and retries even if this has succeeded).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#operation-identifiers","title":"Operation identifiers","text":"<p>To make the previous operation idempotent, you need to consider the end-to-end flow of the request (for example generating a unique operation ID).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-end-to-end-argument","title":"The end-to-end argument","text":"<p>End-to-end argument states that a function can completely and correctly be implemented only with the knowledge and help of the application standing at the endpoints of the communication system, which includes checking the integrity of data.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#enforcing-constraints","title":"Enforcing Constraints","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#uniqueness-constraints-require-consensus","title":"Uniqueness constraints require consensus","text":"<p>In a distributed setting, enforcing a uniqueness constraint (or other constraints) requires consensus, which is usually achieved through making a single node the leader, although this can be scaled out by partitioning based on the value that needs to be unique. Asynchronous multi-master replication is ruled out, because it could hap\u2010 pen that different masters concurrently accept conflicting writes, to immediately reject any writes that would violate the constraint, synchronous coordination is unavoidable.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#uniqueness-in-log-based-messaging","title":"Uniqueness in log-based messaging","text":"<p>In the unbundled database approach with log-based messaging, we can enforce uniqueness constraints by having a stream processor consuming all the messages in a log partition sequentially on a single thread, which can unambiguously and deterministically decide which one of several conflicting operations came first. The fundamental principle is that any writes that may conflict are routed to the same partition and processed sequentially.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#multi-partition-request-processing","title":"Multi-partition request processing","text":"<p>To ensure that an operation is executed atomically, while satisfying constraints when several partitions are involved, we can either do an atomic commit over all the partitions (traditional approach) or by first appending to a log partition based on the request ID so a stream processor can emit messages (including the request ID) to the appropriate partitions, and then another processor would consume those messages and apply the changes. In this case, if the second request crashes, it will process the request on resuming.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#timeliness-and-integrity","title":"Timeliness and Integrity","text":"<p>consistency conflates two different requirements to be consider separately:</p> <pre><code>* Timeliness:  Ensuring that users observe the system in an up-to-date state. This inconsistency is temporary, \nand will eventually be resolved simply by waiting and trying again\n* Integrity: Absence of corruption; i.e.,  no data loss, and no contradictory or false data. This inconsistency \nis permanent, waiting and trying again is not going to fix database corruption in most cases\n</code></pre> <p>Violations of timeliness are \"eventual consistency,\" whereas violations of integrity are \"perpetual inconsistency.\"</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#correctness-of-dataflow-systems","title":"Correctness of dataflow systems","text":"<p>ACID transactions usually provide both timeliness and integrity guarantees. Event-based dataflow systems decouples timeliness and integrity. When processing event streams asynchronously, there is no guarantee of timeliness, but integrity is in fact central. In these systems integrity is achieved through:</p> <pre><code>* Representing the content of the write operation as a single message, which can easily be written atomically\n* Deriving all other state updates from that single message using deterministic der\u2010 ivation functions\n* Passing a client-generated request ID through all these levels of processing, enabling end-to-end duplicate \nsuppression and idempotence\n* Making messages immutable and allowing derived data to be reprocessed from time to time\n</code></pre>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#loosely-interpreted-constraints","title":"Loosely interpreted constraints","text":"<p>Many real applications can actually get away with much weaker notions of uniqueness if it is actually acceptable to temporarily violate a constraint and fix it up later by apologizing. These applications do require integrity, but they don't require timeliness on the enforcement of the constraint.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#coordination-avoiding-data-systems","title":"Coordination-avoiding data systems","text":"<p>Given the previous two points, namely:</p> <pre><code>* Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizability, or \nsynchronous cross-partition coordination\n* Strict uniqueness constraints require timeliness and coordination, many applications are actually fine with \nloose constraints that may be temporarily violated and fixed up later, as long as integrity is preserved throughout\n</code></pre> <p>A dataflow systems can provide the data management services for many applications without requiring coordination, while still giving strong integrity guarantees (coordination-avoiding).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#trust-but-verify","title":"Trust, but Verify","text":""},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#dont-just-blindly-trust-what-they-promise","title":"Don\u2019t just blindly trust what they promise","text":"<p>Data corruption is inevitable sooner or later, checking the integrity of data is known as auditing (read an check).</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#designing-for-auditability","title":"Designing for auditability","text":"<p>Event-based systems provides better auditability: user input to the system is represented as a single immutable event, and any resulting state updates are derived from that event (deterministic and repeatable). Being explicit about dataflow, makes integrity checking much more feasible. For the event log, we can use hashes to check that the event storage has not been corrupted. For any derived state, we can rerun the batch and stream processors that derived it from the event log in order to check whether we get the same result.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#the-end-to-end-argument-again","title":"The end-to-end argument again","text":"<p>We must check periodically the integrity of our data, which is best done in an end-to-end fashion.</p>"},{"location":"Designing%20Data%20Intensive%20Applications/Cliff%20Notes/Cliff%20Notes%20-%20moyano83/#tools-for-auditable-data-systems","title":"Tools for auditable data systems","text":"<p>Cryptographic auditing and integrity checking often relies on Merkle trees, which are trees of hashes that can be used to efficiently prove that a record appears in some dataset. Certificate transparency is a security technology that relies on Merkle trees to check the val\u2010 idity of TLS/SSL certificates.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/","title":"CAP Theorum","text":"<p>In a distributed computer system, you can only support two of the following guarantees: -   Consistency\u00a0- Data is always up-to-date on every read.  Every read receives the most recent write or an error. -   Availability\u00a0- Data is always accessible.  Every request receives a response, without guarantee that it contains the most recent version of the information -   Partition Tolerance\u00a0-  the distributed system continues to operate in the presence of a failure of the network where not all members can reach other members.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#important-note","title":"Important Note","text":"<p>Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#cp-consistency-and-partition-tolerance","title":"CP - consistency and partition tolerance","text":"<p>Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#ap-availability-and-partition-tolerance","title":"AP - availability and partition tolerance","text":"<p>Responses return the most readily available version of the data available on any node, which might not be the latest. Writes might take some time to propagate when the partition is resolved.</p> <p>AP is a good choice if the business needs allow for\u00a0eventual consistency\u00a0or when the system needs to continue working despite external errors.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#consistency-patterns","title":"Consistency patterns","text":"<p>With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data. Recall the definition of consistency from the\u00a0CAP theorem\u00a0- Every read receives the most recent write or an error.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#weak-consistency","title":"Weak consistency","text":"<p>After a write, reads may or may not see it. A best effort approach is taken.</p> <p>This approach is seen in systems such as memcached. Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games. For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#eventual-consistency","title":"Eventual consistency","text":"<p>After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously.</p> <p>This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#strong-consistency","title":"Strong consistency","text":"<p>After a write, reads will see it. Data is replicated synchronously.</p> <p>This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#availability-patterns","title":"Availability patterns","text":""},{"location":"Designing%20Systems/CAP%20Theorum/#fail-over","title":"Fail-over","text":""},{"location":"Designing%20Systems/CAP%20Theorum/#active-passive","title":"Active-passive","text":"<p>With active-passive fail-over, heartbeats are sent between the active and the passive server on standby. If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.</p> <p>The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby. Only the active server handles traffic.</p> <p>Active-passive failover can also be referred to as master-slave failover.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#active-active","title":"Active-active","text":"<p>In active-active, both servers are managing traffic, spreading the load between them.</p> <p>If the servers are public-facing, the DNS would need to know about the public IPs of both servers. If the servers are internal-facing, application logic would need to know about both servers.</p> <p>Active-active failover can also be referred to as master-master failover.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#disadvantages-failover","title":"Disadvantage(s): failover","text":"<ul> <li>Fail-over adds more hardware and additional complexity.</li> <li>There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.</li> </ul>"},{"location":"Designing%20Systems/CAP%20Theorum/#databases","title":"Databases","text":""},{"location":"Designing%20Systems/CAP%20Theorum/#ca-databases","title":"CA databases","text":"<p>CA databases enable consistency and availability across all nodes. Unfortunately, CA databases can\u2019t deliver fault tolerance. In any distributed system, partitions are bound to happen, which means this type of database isn\u2019t a very practical choice. That being said, you still can find a CA database if you need one. Some relational databases, such as PostgreSQL, allow for consistency and availability. You can deploy them to nodes using replication.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#cp-databases","title":"CP databases","text":"<p>CP databases enable consistency and partition tolerance, but not availability. When a partition occurs, the system has to turn off inconsistent nodes until the partition can be fixed. MongoDB is an example of a CP database. It\u2019s a NoSQL database management system (DBMS) that uses documents for data storage. It\u2019s considered schema-less, which means that it doesn\u2019t require a defined database schema. It\u2019s commonly used in big data and applications running in different locations. The CP system is structured so that there\u2019s only one primary node that receives all of the write requests in a given replica set. Secondary nodes replicate the data in the primary nodes, so if the primary node fails, a secondary node can stand-in.</p>"},{"location":"Designing%20Systems/CAP%20Theorum/#ap-databases","title":"AP databases","text":"<p>AP databases enable availability and partition tolerance, but not consistency. In the event of a partition, all nodes are available, but they\u2019re not all updated. For example, if a user tries to access data from a bad node, they won\u2019t receive the most up-to-date version of the data. When the partition is eventually resolved, most AP databases will sync the nodes to ensure consistency across them. Apache Cassandra is an example of an AP database. It\u2019s a NoSQL database with no primary node, meaning that all of the nodes remain available. Cassandra allows for eventual consistency because users can resync their data right after a partition is resolved.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/","title":"Distributed System Fundamentals","text":""},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#acid","title":"ACID","text":"<p>In computer science, ACID (atomicity, consistency, isolation, durability) is a set of properties of database transactions intended to guarantee data validity despite errors, power failures, and other mishaps.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#data-durability-consistency","title":"Data Durability &amp; Consistency","text":"<p>The differences and impacts of failure rates of storage solutions and corruption rates in read-write processes.  </p> <p>Durability guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure (e.g., power outage or crash). This usually means that completed transactions (or their effects) are recorded in non-volatile memory.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#replication","title":"Replication","text":"<p>Reasons why you might want to replicate data: * To keep data geographically close to your users * Increase availability * Increase read throughput</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#leaders-and-followers","title":"Leaders and followers","text":"<p>Every write to the database needs to be processed by every replica. The most common solution for this is called leader-based replication (active/passive or master-slave replication). 1. One of the replicas is designated the leader (master or primary). Writes to the database must send requests to the leader. 2. Other replicas are known as followers (read replicas, slaves, secondaries or hot standbys). The leader sends the data change to all of its followers as part of a replication log or change stream. 3. Reads can then query the leader or any of the followers, while writes are only accepted on the leader.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#synchronous-vs-asynchronous","title":"Synchronous vs asynchronous","text":"<p>The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data. The disadvantage is that it the synchronous follower doesn't respond, the write cannot be processed.</p> <p>It's impractical for all followers to be synchronous. If you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#setting-up-new-followers","title":"Setting up new followers","text":"<p>Setting up a follower can usually be done without downtime. The process looks like: 1. Take a snapshot of the leader's database 2. Copy the snapshot to the follower node 3. Follower requests data changes that have happened since the snapshot was taken 4. Once follower processed the backlog of data changes since snapshot, it has caught up.</p> <p>Process is similar for followers that fell off-line.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#leader-failure-failover","title":"Leader failure: failover","text":"<p>Automatic failover consists: 1. Determining that the leader has failed. If a node does not respond in a period of time it's considered dead. 2. Choosing a new leader. The best candidate for leadership is usually the replica with the most up-to-date changes from the old leader. 3. Reconfiguring the system to use the new leader. The system needs to ensure that the old leader becomes a follower and recognises the new leader.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#statement-based-replication","title":"Statement-based replication","text":"<p>The leader logs every statement and sends it to its followers (every <code>INSERT</code>, <code>UPDATE</code> or <code>DELETE</code>).</p> <p>This type of replication has some problems: * Non-deterministic functions such as <code>NOW()</code> or <code>RAND()</code> will generate different values on replicas. * Statements that depend on existing data, like auto-increments, must be executed in the same order in each replica. * Statements with side effects may result on different results on each replica.</p> <p>A solution to this is to replace any nondeterministic function with a fixed return value in the leader.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#write-ahead-log-wal-shipping","title":"Write-ahead log (WAL) shipping","text":"<p>The log is an append-only sequence of bytes containing all writes to the database.  This way of replication is used in PostgresSQL and Oracle. The main disadvantage is that the log describes the data at a very low level (like which bytes were changed in which disk blocks), coupling it to the storage engine.</p> <p>Usually is not possible to run different versions of the database in leaders and followers.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#logical-row-based-log-replication","title":"Logical (row-based) log replication","text":"<ul> <li>For an inserted row, the new values of all columns.</li> <li>For a deleted row, the information that uniquely identifies that column.</li> <li>For an updated row, the information to uniquely identify that row and all the new values of the columns.</li> </ul> <p>Since logical log is decoupled from the storage engine internals, it's easier to make it backwards compatible.</p> <p>Logical logs are also easier for external applications to parse, useful for data warehouses, custom indexes and caches (change data capture).</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#problems-with-replication-lag","title":"Problems with replication lag","text":"<p>In a read-scaling architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this only realistically works on asynchronous replication. The more nodes you have, the likelier is that one will be down, so a fully synchronous configuration would be unreliable.</p> <p>With an asynchronous approach, a follower may fall behind, leading to inconsistencies in the database (eventual consistency).</p> <p>The replication lag could be a fraction of a second or several seconds or even minutes.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#reading-your-own-writes","title":"Reading your own writes","text":"<p>Read-after-write consistency, also known as read-your-writes consistency is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves.</p> <p>How to implement it: * After writing, ensure the user reads from the leader (or synchronous copies) * The client can remember the timestamp of the most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp. * Ensure usage of same datacenter * You may want to provide cross-device read-after-write consistency for the same user</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#monotonic-reads","title":"Monotonic reads","text":"<p>Ensure the user reads from the same replica, so they do not experience time discrepencies (such as time moving backward due to lag)</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#multi-leader-replication","title":"Multi-leader replication","text":"<p>Leader-based replication has one major downside: there is only one leader, and all writes must go through it.</p> <p>A natural extension is to allow more than one node to accept writes (multi-leader, master-master or active/active replication) where each leader simultaneously acts as a follower to the other leaders.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#multi-datacenter-operation","title":"Multi-datacenter operation","text":"<p>You can have a leader in each datacenter. Within each datacenter, regular leader-follower replication is used. Between datacenters, each datacenter leader replicates its changes to the leaders in other datacenters.</p> <p>Compared to a single-leader replication model deployed in multi-datacenters * Performance. With single-leader, every write must go across the internet to wherever the leader is, adding significant latency. In multi-leader every write is processed in the local datacenter and replicated asynchronously to other datacenters. The network delay is hidden from users and perceived performance may be better. * Tolerance of datacenter outages. In single-leader if the datacenter with the leader fails, failover can promote a follower in another datacenter. In multi-leader, each datacenter can continue operating independently from others. * Tolerance of network problems. Single-leader is very sensitive to problems in this inter-datacenter link as writes are made synchronously over this link. Multi-leader with asynchronous replication can tolerate network problems better.</p> <p>It's common to fall on subtle configuration pitfalls. Autoincrementing keys, triggers and integrity constraints can be problematic. Multi-leader replication is often considered dangerous territory and avoided if possible.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#consensus","title":"Consensus","text":"<p>Ensuring all nodes are in agreement, which prevents fault processes from running and ensures consistency and replication of data and processes</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#consistency-guarantees","title":"Consistency guarantees","text":"<ul> <li>Do we need data guarantees or is eventual consistency (convergence) OK?</li> <li>Stronger guarantees have worse performance</li> </ul>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#linearizability","title":"Linearizability","text":"<ul> <li>Make system appear as if it has only one copy of the data and looks atomic<ul> <li>Once new values are read, never return old ones</li> <li>Make data appear serialized -as if they executed in order</li> <li>recency guaranteed on reads and writes</li> </ul> </li> <li>How different systems are<ul> <li>Single-leader repolication is potentially linearizable.</li> <li>Consensus algorithms is linearizable.</li> <li>Multi-leader replication is not linearizable.</li> <li>Leaderless replication is probably not linearizable.</li> </ul> </li> <li>Multi-leader replication is often a good choice for multi-datacenter replication. On a network interruption between data-centers will force a choice between linearizability and availability.<ul> <li>With multi-leader configuraiton, each data center can operate normally with interruptions.</li> <li>If an application does not require linearizability it can be more tolerant of network problems.</li> </ul> </li> </ul>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#leader-election","title":"Leader election","text":"<ul> <li>All (or voting) nodes must agree on the leader via a lock</li> <li>Apache Zookeeper and etcd are often used for distributed locks</li> <li>Voting<ul> <li>Mostly a synchronous operation</li> <li>Votes are held if the slave nodes believe the master is dead</li> <li>If a predefined fixed # of nodes are selected to vote, then you can't just remove these systems from the cluster</li> <li>Network issues, especially across geographic regions, can cause mass voting</li> <li>Too much voting prevents work from getting done.</li> </ul> </li> </ul>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#ordering-and-causality","title":"Ordering and Causality","text":"<ul> <li>Order preserves causality</li> <li>Rows must exist before modification</li> <li>Detect concurrent writes</li> <li>In order to determine casual ordering<ul> <li>Database needs to know which copy of the data was read before write<ul> <li>use versions</li> </ul> </li> <li>with single leader replication, master can just increment a number</li> <li>With multi-leader, you can use something like Lamport timestamps<ul> <li>combo of timestamp and node ID where the largest numbers wins.</li> <li>Each node sends out the lamport timetamp on each request</li> <li>Client sends back these timestamps. If another replica gets a future request with a larger timestamp, the server updates.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#key-points","title":"Key points","text":"<ul> <li>Linearizability is stronger than causal consistency<ul> <li>Linearizability implies causality</li> </ul> </li> <li>Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.<ul> <li>The causal order is not a total order</li> <li>Some elements will be incomparable WEAKNESS</li> <li>Timestamp ordering is not sufficient WEAKNESS</li> </ul> </li> </ul>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#total-order-broadcast","title":"Total Order Broadcast","text":"<p>Total order broadcast is a protocol for exchanging messages between nodes. It requires two safety properties: * Reliable delivery: No messages are lost a (if a message is delivered to one node, it is delivered to all nodes) * Totally ordered delivery: Messages are delivered to every node in the same order * Makes sure every message and write are delivered in the same order * No guaranteed when message is delivered</p> <p>Example: * Append a message to the log, tentatively indicating the username you want to claim  * Read the log, and wait for the message you appended to be delivered back to you  * Check for any messages claiming the username that you want. If the first message for your desired username is your own message, then you are successful</p> <p>Consensus services  * ZooKeeper * etcd</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#partitioning","title":"Partitioning","text":"<p>Replication, for very large datasets or very high query throughput is not sufficient, we need to break the data up into partitions (sharding).</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#partitioning-types","title":"Partitioning Types","text":"<ul> <li>key-value<ul> <li>Good<ul> <li>simple</li> </ul> </li> <li>Bad<ul> <li>suspectible to hot spots</li> <li>no way of knowing what shard the data is on</li> </ul> </li> </ul> </li> <li>key range<ul> <li>Good<ul> <li>Can be sorted and scanned easily</li> <li>Know where shard is located physically</li> </ul> </li> <li>Bad<ul> <li>suspectible to hot spots</li> </ul> </li> </ul> </li> <li>hash of key<ul> <li>Good<ul> <li>avoids hotspots</li> </ul> </li> <li>Bad<ul> <li>Can't do sorted range queries anymore</li> <li>Must send range queries to all shards</li> </ul> </li> </ul> </li> </ul>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#secondary-indexes","title":"Secondary Indexes","text":"<p>Each partition maintains its secondary indexes, covering only the documents in that partition (local index).</p> <p>You need to send the query to all partitions, and combine all the results you get back (scatter/gather). This is prone to tail latency amplification and is widely used in MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud and VoltDB.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#global-index","title":"Global Index","text":"<p>We construct a global index that covers data in all partitions. The global index must also be partitioned so it doesn't become the bottleneck.</p> <p>The advantage is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. The downside of a global index is that writes are slower and complicated.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#distributed-transactions","title":"Distributed Transactions","text":"<p>Once consensus is reached, transactions from applications need to be committed across databases with fault checks by each resource involved.</p> <p>There are situations in which it is important for nodes to agree: * Leader election: All nodes need to agree on which node is the leader. * Atomic commit: Get all nodes to agree on the outcome of the transacction, either they all abort or roll back.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#atomic-commit-and-two-phase-commit-2pc","title":"Atomic commit and two-phase commit (2PC)","text":"<p>2PC is also called a blocking atomic commit protocol, as 2Pc can become stuck waiting for the coordinator to recover. Heavy performance penalty. * If all participants reply \"yes\", the coordinator sends out a commit request in phase 2, and the commit takes place. * If any of the participants replies \"no\", the coordinator sends an abort request to all nodes in phase 2. * The problem with locking is that database transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty writes. * If Coordinator fails, human interaction is required, otherwise orphaned updates and locks may linger in database forever.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#fault-tolerant-consensus","title":"Fault-tolerant consensus","text":"<p>One or more nodes may propose values, and the consensus algorithm decides on those values.</p> <p>Consensus algorithm must satisfy the following properties: * Uniform agreement: No two nodes decide differently. * Integrity: No node decides twice. * Validity: If a node decides the value v, then v was proposed by some node. * Termination: Every node that does not crash eventually decides some value.</p> <p>So total order broadcast is equivalent to repeated rounds of consensus: * Due to agreement property, all nodes decide to deliver the same messages in the same order. * Due to integrity, messages are not duplicated. * Due to validity, messages are not corrupted. * Due to termination, messages are not lost.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#storage-and-retrieval","title":"Storage and Retrieval","text":""},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#write-log","title":"Write Log","text":"<p>Most databases use an append-only log.  To avoid the log from growing too big, you can close a log and start a new one.  Then, compact the older logs.</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#types-of-indexes","title":"Types of Indexes","text":"<p>Indexes speed up retrieval. * Hash Indexes * SSTables * LSM-Trees * B-Trees</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#data-warehousing","title":"Data Warehousing","text":"<p>A data warehouse is a separate database that analysts can query to their heart's content without affecting OLTP operations. It contains read-only copy of the dat in all various OLTP systems in the company. Data is extracted out of OLTP databases (through periodic data dump or a continuous stream of update), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse (process Extract-Transform-Load or ETL).</p>"},{"location":"Designing%20Systems/Distibuted%20System%20Fundamentals/#column-oriented-storage","title":"Column-oriented storage","text":"<p>In a row-oriented storage engine, when you do a query that filters on a specific field, the engine will load all those rows with all their fields into memory, wasting lots of resoruces</p> <p>Column-oriented storage is simple: don't store all the values from one row together, but store all values from each column together instead. Only load what is needed.  </p> <p>Column-oriented storage is more CPU effecient and can be compressed for faster loading.</p>"},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/","title":"How to Design Large Scale Systems","text":""},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/#step-1-clarify-the-goals","title":"Step 1: Clarify the goals","text":"<p>Make sure you understand the basic requirements and any clarilying questions.</p>"},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/#step-2-determine-the-scope","title":"Step 2: Determine the scope","text":"<p>Describe the feature set you'll be discussing in the given solution, and define all of the features and their importance to the end goal.</p>"},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/#step-3-design-for-the-right-scale","title":"Step 3: Design for the right scale","text":"<p>Determine the scale so you know whether the data can be supported by a single machine or if you need to scale.</p>"},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/#step-4-start-simple-then-iterate","title":"Step 4: Start simple, then iterate","text":"<p>Describe the high-level process end-to-end based on your feature set and overall goals. This is a good time to discuss potential bottlenecks.</p>"},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/#step-5-consider-relevant-dsa","title":"Step 5: Consider relevant DSA","text":"<p>Determine which fundamental data structures and algorithms will help your system perform efficiently and appropriately.</p>"},{"location":"Designing%20Systems/How%20to%20Design%20Large%20Scale%20Systems/#step-6-describe-trade-offs","title":"Step 6: Describe trade-offs","text":"<p>Describe trade-offs while explaining your solution to show you understand large-scale systems and their complexities.</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/","title":"System Design Interview Steps","text":"<p>The system design interview is an\u00a0open-ended conversation. You are expected to lead it.</p> <p>You can use the following steps to guide the discussion. To help solidify this process, work through the\u00a0System design interview questions with solutions\u00a0section using the following steps.</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#step-1-outline-use-cases-constraints-and-assumptions","title":"Step 1: Outline use cases, constraints, and assumptions","text":"<ul> <li>Who is going to use it?</li> <li>How are they going to use it?</li> <li>How many users are there?</li> <li>What does the system do?</li> <li>What are the inputs and outputs of the system?</li> <li>How much data do we expect to handle?</li> <li>How many requests per second do we expect?</li> <li>What is the expected read to write ratio?</li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#step-2-create-a-high-level-design","title":"Step 2: Create a high level design","text":"<p>Outline a high level design with all important components.</p> <ul> <li>Sketch the main components and connections</li> <li>Justify your ideas</li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#step-3-design-core-components","title":"Step 3: Design core components","text":"<p>Dive into details for each core component. For example, if you were asked to\u00a0design a url shortening service, discuss:</p> <ul> <li>Generating and storing a hash of the full url<ul> <li>MD5/Base64</li> <li>Hash collisions</li> <li>SQL or NoSQL</li> <li>Database schema</li> </ul> </li> <li>Translating a hashed url to the full url<ul> <li>Database lookup</li> </ul> </li> <li>API and object-oriented design</li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#step-4-scale-the-design","title":"Step 4: Scale the design","text":"<p>Identify and address bottlenecks, given the constraints. For example, do you need the following to address scalability issues?</p> <ul> <li>Load balancer</li> <li>Horizontal scaling</li> <li>Caching</li> <li>Database sharding</li> </ul> <p>Discuss potential solutions and trade-offs. Everything is a trade-off. Address bottlenecks using\u00a0principles of scalable system design.</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#sources-and-further-reading","title":"Source(s) and further reading","text":"<p>Check out the following links to get a better idea of what to expect:</p> <ul> <li>How to ace a systems design interview</li> <li>The system design interview</li> <li>Intro to Architecture and Systems Design Interviews</li> <li>System design template</li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#back-of-the-envelope-calculations","title":"Back of the Envelope Calculations","text":""},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#numbers-everyone-should-know","title":"Numbers Everyone Should Know","text":"<p>To evaluate design alternatives you first need a good sense of how long typical operations will take. Dr. Dean gives this list:</p> <pre><code>Latency Comparison Numbers\n--------------------------\nL1 cache reference                           0.5 ns\nBranch mispredict                            5   ns\nL2 cache reference                           7   ns                      14x L1 cache\nMutex lock/unlock                           25   ns\nMain memory reference                      100   ns                      20x L2 cache, 200x L1 cache\nCompress 1K bytes with Zippy            10,000   ns       10 us\nSend 1 KB bytes over 1 Gbps network     10,000   ns       10 us\nRead 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD\nRead 1 MB sequentially from memory     250,000   ns      250 us\nRound trip within same datacenter      500,000   ns      500 us\nRead 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory\nHDD seek                            10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip\nRead 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD\nRead 1 MB sequentially from HDD     30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD\nSend packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms\n\nNotes\n-----\n1 ns = 10^-9 seconds\n1 us = 10^-6 seconds = 1,000 ns\n1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns\n</code></pre>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#design-1-serial","title":"Design 1 - Serial","text":"<ul> <li>Read images serially. Do a disk seek. Read a\u00a0256K\u00a0image and then go on to the next image.</li> <li>Performance: 30 seeks * 10 ms/seek + 30 * 256K / 30 MB /s = 560ms</li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#design-2-parallel","title":"Design 2 - Parallel","text":"<ul> <li>Issue reads in\u00a0parallel.</li> <li>Performance:\u00a010 ms/seek + 256K read / 30 MB/s = 18ms</li> <li>There will be variance from the disk reads, so the more likely time is 30-60ms</li> </ul> <p>Which design is best? It depends on you requirements, but given the back-of-the-envelope calculations you have a quick way to compare them without building them.</p> <p>Now you have a framework for asking yourself other design questions and comparing\u00a0different design variations: -   Does it make sense to cache single thumbnail images? -   Should you cache a whole set of images in one entry? -   Does it make sense to precompute the thumbnails?</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#system-design-topics","title":"System Design Topics","text":""},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#step-1-review-the-scalability-video-lecture","title":"Step 1: Review the scalability video lecture","text":"<p>Scalability Lecture at Harvard</p> <ul> <li>Topics covered:<ul> <li>Vertical scaling</li> <li>Horizontal scaling</li> <li>Caching</li> <li>Load balancing</li> <li>Database replication</li> <li>Database partitioning</li> </ul> </li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#step-2-review-the-scalability-article","title":"Step 2: Review the scalability article","text":"<p>Scalability</p> <ul> <li>Topics covered:<ul> <li>Clones</li> <li>Databases</li> <li>Caches</li> <li>Asynchronism</li> </ul> </li> </ul>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#performance-vs-scalability","title":"Performance vs Scalability","text":"<p>A service is\u00a0scalable\u00a0if it results in increased\u00a0performance\u00a0in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.1</p> <p>Another way to look at performance vs scalability: -   If you have a\u00a0performance\u00a0problem, your system is slow for a single user. -   If you have a\u00a0scalability\u00a0problem, your system is fast for a single user but slow under heavy load.</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#latency-vs-throughput","title":"Latency vs Throughput","text":"<p>Latency\u00a0is the time to perform some action or to produce some result. - e.g. requests take 100ms</p> <p>Throughput\u00a0is the number of such actions or results per unit of time. * e.g. The system can handle 10,000 requests per hour</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#availability-vs-consistency","title":"Availability vs Consistency","text":"<p>More: CAP Theorum</p>"},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#replication","title":"Replication","text":""},{"location":"Designing%20Systems/System%20Design%20Interview%20Steps/#master-slave-and-master-master","title":"Master-slave and master-master","text":"<p>Databases</p>"},{"location":"Specifics/Asynchronism/","title":"Asynchronism","text":""},{"location":"Specifics/Asynchronism/#message-queues","title":"Message queues","text":"<p>Message queues receive, hold, and deliver messages. If an operation is too slow to perform inline, you can use a message queue with the following workflow: -   An application publishes a job to the queue, then notifies the user of job status -   A worker picks up the job from the queue, processes it, then signals the job is complete</p> <p>The user is not blocked and the job is processed in the background. During this time, the client might optionally do a small amount of processing to make it seem like the task has completed. For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.</p> <p>Redis\u00a0is useful as a simple message broker but messages can be lost.</p> <p>RabbitMQ\u00a0is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.</p> <p>Amazon SQS\u00a0is hosted but can have high latency and has the possibility of messages being delivered twice.</p>"},{"location":"Specifics/Asynchronism/#task-queues","title":"Task queues","text":"<p>Tasks queues receive tasks and their related data, runs them, then delivers their results. They can support scheduling and can be used to run computationally-intensive jobs in the background.</p> <p>Celery\u00a0has support for scheduling and primarily has python support.</p>"},{"location":"Specifics/Asynchronism/#back-pressure","title":"Back pressure","text":"<p>If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance.\u00a0Back pressure\u00a0can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue. Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later. Clients can retry the request at a later time, perhaps with\u00a0exponential backoff.</p>"},{"location":"Specifics/Caching/","title":"Cache","text":"<p>Caching improves page load times and can reduce the load on your servers and databases. In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.</p> <p>Databases often benefit from a uniform distribution of reads and writes across its partitions. Popular items can skew the distribution, causing bottlenecks. Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.</p>"},{"location":"Specifics/Caching/#client-caching","title":"Client caching","text":"<p>Caches can be located on the client side (OS or browser),\u00a0server side, or in a distinct cache layer.</p>"},{"location":"Specifics/Caching/#cdn-caching","title":"CDN caching","text":"<p>CDNs\u00a0are considered a type of cache.</p>"},{"location":"Specifics/Caching/#web-server-caching","title":"Web server caching","text":"<p>Reverse proxies\u00a0and caches such as\u00a0Varnish\u00a0can serve static and dynamic content directly. Web servers can also cache requests, returning responses without having to contact application servers.</p>"},{"location":"Specifics/Caching/#database-caching","title":"Database caching","text":"<p>Your database usually includes some level of caching in a default configuration, optimized for a generic use case. Tweaking these settings for specific usage patterns can further boost performance.</p>"},{"location":"Specifics/Caching/#application-caching","title":"Application caching","text":"<p>In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage. Since the data is held in RAM, it is much faster than typical databases where data is stored on disk. RAM is more limited than disk, so\u00a0cache invalidation\u00a0algorithms such as\u00a0least recently used (LRU)\u00a0can help invalidate 'cold' entries and keep 'hot' data in RAM.</p>"},{"location":"Specifics/Caching/#caching-at-the-database-query-level","title":"Caching at the database query level","text":"<p>Whenever you query the database, hash the query as a key and store the result to the cache. This approach suffers from expiration issues: -   Hard to delete a cached result with complex queries -   If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell</p>"},{"location":"Specifics/Caching/#caching-at-the-object-level","title":"Caching at the object level","text":"<p>See your data as an object, similar to what you do with your application code. Have your application assemble the dataset from the database into a class instance or a data structure(s): -   Remove the object from cache if its underlying data has changed -   Allows for asynchronous processing: workers assemble objects by consuming the latest cached object</p> <p>Suggestions of what to cache: -   User sessions -   Fully rendered web pages -   Activity streams -   User graph data</p>"},{"location":"Specifics/Caching/#cache-updates","title":"Cache Updates","text":"<p>Eviction policies:     -  Cache aside         - client makes a write, then invalidates the related cache         - not guaranteed to be consistent             - DB could be updated by outside process             - query to invalidate could fail     -  Write through         - In write-through, data is\u00a0simultaneously updated to cache and memory. This process is simpler and more reliable. This is used when there are no frequent writes to the cache(The number of write operations is less).     - Write Behind/Back/Deferred         - The data is updated only in the cache and updated into the memory at a later time.         - Implies database cannot fail         - Have to watch out for out-of-order updates     - Refresh ahead         - Automaticaly update caches before they expire         - Helps prevent slow reads due to fetch from DB         - Read through occurs if cache does expire     - Read through         - if the cache is not availabe, get from data source and cache for later use.  Else, just return the existing cache.</p>"},{"location":"Specifics/Databases/","title":"Databases","text":""},{"location":"Specifics/Databases/#what-is-the-difference-between-myisam-and-innodb","title":"What is the difference between MyISAM and InnoDB?","text":""},{"location":"Specifics/Databases/#myisam","title":"MyISAM","text":"<ul> <li>Supports table level locking</li> <li>Is Faster than InnoDB</li> <li>Does not support foreign keys</li> <li>Stores table, data and index in different files</li> <li>Does not support transactions (no commit with rollback)</li> <li>Useful for more selects with fewer updates  </li> </ul>"},{"location":"Specifics/Databases/#innodb","title":"InnoDB","text":"<ul> <li>Support row level locking</li> <li>Is slower than MyISAM</li> <li>Supports foreign keys</li> <li>Stores table and index in table space</li> <li>Supports transactions<ul> <li>A transaction is a unit of work that you want to treat as \"a whole.\" It has to either happen in full or not at all.</li> <li>e.g move money from one account to another.  Can't let this be half finished.</li> </ul> </li> </ul>"},{"location":"Specifics/Databases/#synchronous-vs-asynchronous-commits","title":"Synchronous vs Asynchronous Commits","text":""},{"location":"Specifics/Databases/#asynchronous-commit","title":"Asynchronous Commit","text":"<p>In an asynchronous commit, the transaction is considered complete as soon as the command is issued, without waiting for the data to be physically written to the disk. This approach can improve performance, especially in situations involving a large number of write operations.</p> <p>However, there's a risk involved: if a system failure occurs before the data is physically written to the disk, the changes may be lost.</p> <p>Use Cases for Asynchronous Commit - When high throughput and lower latency are more important than the risk of potential data loss. - When the data is not mission-critical and can be recovered or recreated if lost. - When the database is used for caching data or for temporary storage.</p>"},{"location":"Specifics/Databases/#synchronous-commit","title":"Synchronous Commit","text":"<p>In a synchronous commit, the transaction is not considered complete until the data has been physically written to the disk. This ensures a higher level of data integrity as it reduces the risk of data loss in case of a system failure.</p> <p>However, this approach may have a performance impact as it requires the system to wait until the write operation is confirmed.</p> <p>Use Cases for Synchronous Commit - When data integrity is paramount and any data loss is unacceptable. - When the database stores critical data, such as financial transactions, where consistency and durability are required. - When the system can tolerate the potential performance impact of waiting for commit confirmation. - In summary, the decision to use asynchronous or synchronous commit depends largely on the trade-off between performance and data integrity that is acceptable for your specific use case.</p>"},{"location":"Specifics/Databases/#what-are-some-things-to-check-on-a-slow-database","title":"What are some things to check on a slow database?","text":"<p>MySQL is fairly popular, so let\u2019s look at some basic MySQL debugging. First off, check the OS to make sure the system is running fine, specially check CPU, memory, SWAP space and disk I/O. </p> <p>Assuming those are all ok, then log into MySQL and check the running queries, you can do so by running the command <code>show full processlist</code>. This will give you a list of queries running on the server. If you see a query that has been running for an excessively long time, you should investigate that query.</p> <p>Check - <code>Explain</code> query - check slow query log file - Check query cache</p>"},{"location":"Specifics/Databases/#acid","title":"ACID","text":"<p>ACID\u00a0is a set of properties of relational database\u00a0transactions. - Atomicity\u00a0- Each transaction is all or nothing - Consistency\u00a0- Any transaction will bring the database from one valid state to another - Isolation\u00a0- Executing transactions concurrently has the same results as if the transactions were executed serially - Durability\u00a0- Once a transaction has been committed, it will remain so</p>"},{"location":"Specifics/Databases/#replication","title":"Replication","text":""},{"location":"Specifics/Databases/#tuning","title":"Tuning","text":"<p>There are many techniques to scale a relational database:\u00a0master-slave replication,\u00a0master-master replication,\u00a0federation,\u00a0sharding,\u00a0denormalization, and\u00a0SQL tuning.</p>"},{"location":"Specifics/Databases/#master-slave-replication","title":"Master-slave replication","text":"<p>The master serves reads and writes, replicating writes to one or more slaves, which serve only reads. Slaves can also replicate to additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned. </p>"},{"location":"Specifics/Databases/#master-master-replication","title":"Master-master replication","text":"<p>Both masters serve reads and writes and coordinate with each other on writes. If either master goes down, the system can continue to operate with both reads and writes.</p>"},{"location":"Specifics/Databases/#disadvantages-master-master-replication","title":"Disadvantage(s): master-master replication","text":"<ul> <li>You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.</li> <li>Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.</li> <li>Conflict resolution comes more into play as more write nodes are added and as latency increases. </li> </ul>"},{"location":"Specifics/Databases/#disadvantage-for-all-replication","title":"Disadvantage for all replication","text":"<ul> <li>There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.</li> <li>Writes are replayed to the read replicas. If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.</li> <li>The more read slaves, the more you have to replicate, which leads to greater replication lag.</li> <li>On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.</li> </ul>"},{"location":"Specifics/Databases/#federation","title":"Federation","text":"<p>Federation (or functional partitioning) splits up databases by function. For example, instead of a single, monolithic database, you could have three databases:\u00a0forums,\u00a0users, and\u00a0products, resulting in less read and write traffic to each database and therefore less replication lag. Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality. With no single central master serializing writes you can write in parallel, increasing throughput.</p>"},{"location":"Specifics/Databases/#disadvantages-federation","title":"Disadvantage(s): federation","text":"<ul> <li>Federation is not effective if your schema requires huge functions or tables.</li> <li>You'll need to update your application logic to determine which database to read and write.</li> <li>Joining data from two databases is more complex with a\u00a0server link.</li> </ul>"},{"location":"Specifics/Databases/#sharding","title":"Sharding","text":"<p>Sharding distributes data across different databases such that each database can only manage a subset of the data.  If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss.</p>"},{"location":"Specifics/Databases/#disadvantages-sharding","title":"Disadvantage(s): sharding","text":"<ul> <li>You'll need to update your application logic to work with shards, which could result in complex SQL queries.</li> <li>Data distribution can become lopsided in a shard. For example, a set of power users on a shard could result in increased load to that shard compared to others.<ul> <li>Rebalancing adds additional complexity. A sharding function based on\u00a0consistent hashing\u00a0can reduce the amount of transferred data.</li> </ul> </li> <li>Joining data from multiple shards is more complex. </li> </ul>"},{"location":"Specifics/Databases/#denormalization","title":"Denormalization","text":"<p>Denormalization attempts to improve read performance at the expense of some write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins. Some RDBMS such as\u00a0PostgreSQL\u00a0and Oracle support\u00a0materialized views\u00a0which handle the work of storing redundant information and keeping redundant copies consistent.</p> <p>Once data becomes distributed with techniques such as\u00a0federation\u00a0and\u00a0sharding, managing joins across data centers further increases complexity. Denormalization might circumvent the need for such complex joins.</p> <p>In most systems, reads can heavily outnumber writes 100:1 or even 1000:1. A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.</p>"},{"location":"Specifics/Databases/#disadvantages-denormalization","title":"Disadvantage(s): denormalization","text":"<ul> <li>Data is duplicated.</li> <li>Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.</li> <li>A denormalized database under heavy write load might perform worse than its normalized counterpart.</li> </ul>"},{"location":"Specifics/Databases/#change-control","title":"Change Control","text":""},{"location":"Specifics/Databases/#database-content-control","title":"Database Content Control","text":"<p>Change control for iterative structure releases in a database can be achieved through a combination of version control systems, migration scripts, and database schema management tools.</p>"},{"location":"Specifics/Databases/#version-control-systems","title":"Version Control Systems","text":"<p>Version Control Systems (like Git) can be used to keep track of changes made to the database structure. Each change to the database structure is committed to the version control system with a description of what was changed, why it was changed, and by whom. This allows for easy tracking and rollback of changes if necessary.</p>"},{"location":"Specifics/Databases/#migration-scripts","title":"Migration Scripts","text":"<p>Migration scripts are scripts that make changes to the database structure. These scripts are written in such a way that they can be run multiple times without causing issues. Each script corresponds to a specific change to the database structure.</p>"},{"location":"Specifics/Databases/#database-schema-management-tools","title":"Database Schema Management Tools","text":"<p>Database Schema Management Tools can be used to manage and automate the process of applying migration scripts. They keep track of which scripts have been run, ensuring that each script is only run once.</p>"},{"location":"Specifics/Databases/#database-configuration-control","title":"Database Configuration Control","text":"<p>This section is weak.  I'm guessing it is a combination of documentation, config management repo, a tool to deploy the changes, and a tool to test/benchmark the changes before a full roll out versus the existing configuration.</p>"},{"location":"Specifics/Databases/#nosql","title":"NoSQL","text":"<p>NoSQL is a collection of data items represented in a\u00a0key-value store,\u00a0document store,\u00a0wide column store, or a\u00a0graph database. Data is denormalized, and joins are generally done in the application code. Most NoSQL stores lack true ACID transactions and favor\u00a0eventual consistency.</p>"},{"location":"Specifics/Databases/#key-value-store","title":"Key-value store","text":"<p>Abstraction: hash table</p> <p>A key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD. Data stores can maintain keys in\u00a0lexicographic order (aka alphabetical, totally ordered), allowing efficient retrieval of key ranges. Key-value stores can allow for storing of metadata with a value.</p> <p>Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer. Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.</p>"},{"location":"Specifics/Databases/#document-store","title":"Document store","text":"<p>Abstraction: key-value store with documents stored as values</p> <p>A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object. Document stores provide APIs or a query language to query based on the internal structure of the document itself.\u00a0Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.</p>"},{"location":"Specifics/Databases/#wide-column-store","title":"Wide column store","text":"<p>A wide column store's basic unit of data is a column (name/value pair). A column can be grouped in column families (analogous to a SQL table). Super column families further group column families</p>"},{"location":"Specifics/Databases/#graph-database","title":"Graph database","text":"<p> In a graph database, each node is a record and each arc is a relationship between two nodes. Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.</p> <p>Graphs databases offer high performance for data models with complex relationships, such as a social network. They are relatively new and are not yet widely-used.</p>"},{"location":"Specifics/Databases/#popular-nosql-options","title":"Popular NoSQL Options","text":"<p>There are several major non-relational database options, each with its own unique strengths. Here are some of the most popular ones:</p>"},{"location":"Specifics/Databases/#mongodb","title":"MongoDB","text":"<p>Strengths: Flexible schema, scalability, and speed. MongoDB uses a document-oriented model which allows for records that do not have to have a uniform structure, unlike relational databases. This flexibility can make it easier to scale and develop with.</p>"},{"location":"Specifics/Databases/#cassandra","title":"Cassandra","text":"<p>Strengths: High availability, fault tolerance, and performance. Cassandra was designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.</p>"},{"location":"Specifics/Databases/#redis","title":"Redis","text":"<p>Strengths: Speed, simplicity, and versatility. Redis is an in-memory data structure store used as a database, cache, and message broker. Its in-memory nature gives it excellent performance.</p>"},{"location":"Specifics/Databases/#hbase","title":"HBase","text":"<p>Strengths: Scalability, strong consistency, and integration with Hadoop. HBase is a column-oriented database that's designed to store large amounts of data in a distributed environment. It's built on top of Hadoop and is often used for real-time read/write access to big data.</p>"},{"location":"Specifics/Databases/#couchdb","title":"CouchDB","text":"<p>Strengths: Replication &amp; Sync, ease of use, and a HTTP/JSON API.</p> <p>CouchDB is a document-oriented database and uses JSON to store data, JavaScript as its query language, and HTTP for an API. It also offers incremental replication with bi-directional conflict detection and resolution.</p>"},{"location":"Specifics/Databases/#sql-or-nosql","title":"SQL or NoSQL","text":"<p>Cliff Notes - moyano83#Relational Model Versus Document Model Cliff Notes - Someguy#Relational model vs document model</p>"},{"location":"Specifics/Databases/#reasons-for-sql","title":"Reasons for\u00a0SQL:","text":"<ul> <li>Structured data</li> <li>Strict schema</li> <li>Relational data</li> <li>Need for complex joins</li> <li>Transactions</li> <li>Clear patterns for scaling</li> <li>More established: developers, community, code, tools, etc</li> <li>Lookups by index are very fast</li> </ul>"},{"location":"Specifics/Databases/#reasons-for-nosql","title":"Reasons for\u00a0NoSQL:","text":"<ul> <li>Semi-structured data</li> <li>Dynamic or flexible schema</li> <li>Non-relational data</li> <li>No need for complex joins</li> <li>Store many TB (or PB) of data</li> <li>Very data intensive workload</li> <li>Very high throughput for IOPS</li> </ul> <p>Sample data well-suited for NoSQL: -   Rapid ingest of clickstream and log data -   Leaderboard or scoring data -   Temporary data, such as a shopping cart -   Frequently accessed ('hot') tables -   Metadata/lookup tables</p>"},{"location":"Specifics/Databases/#testing","title":"Testing","text":"<p>Strategies would aim to maintain data integrity, database performance, and continuity of operations even under unexpected conditions.</p> <p>Implement Data Validation Rules: Use database constraints (such as UNIQUE, NOT NULL, CHECK) to ensure data consistency and prevent invalid data from being inserted into the database.</p> <pre><code>CREATE TABLE Employees (\n ID int NOT NULL,\n Name varchar(255) NOT NULL,\n Age int CHECK (Age&gt;=18),\n PRIMARY KEY (ID)\n);\n</code></pre> <p>Use Exception Handling: Implement error handling mechanisms in your SQL procedures to catch exceptions and manage them appropriately.</p> <pre><code>BEGIN TRY\n -- Code here\nEND TRY\nBEGIN CATCH\n -- Handle error\nEND CATCH\n</code></pre> <p>Write Unit Tests: Create tests for your database procedures and queries to check their correctness under a variety of scenarios, including corner cases.</p> <pre><code>-- A simple unit test might look like this (pseudo-code)\nTEST PROCEDURE test_employee_age() AS\nBEGIN\n -- Insert an employee with an invalid age\n INSERT INTO Employees (ID, Name, Age) VALUES (1, 'Test', 17);\n -- Check that the insert failed\n ASSERT SELECT COUNT(*) FROM Employees WHERE ID = 1 IS NULL;\nEND\n</code></pre> <p>Perform Stress Testing: Test how your database performs under heavy load or large amounts of data to identify and address performance issues.</p> <p>Design for Scalability: Use strategies like partitioning, sharding, or using distributed databases to ensure your database can handle high volumes of data.</p> <p>Regularly Monitor and Profile Your Database: Use database profiling tools to regularly monitor the performance of your database and identify any potential corner cases causing issues.</p> <p>Use Transaction Management: Use transactions to ensure that your database operations are atomic, consistent, isolated, and durable (ACID). This is especially important for handling corner cases where operations might fail partway.</p>"},{"location":"Specifics/Databases/#security","title":"Security","text":"<p>Database security is a critical aspect of information system design and administration. There are two primary types of privileges in a database environment: administrative privileges and system privileges. Here's a detailed breakdown of the two and their associated security concerns:</p>"},{"location":"Specifics/Databases/#administrative-privileges","title":"Administrative Privileges","text":"<p>Administrative privileges allow a user to perform high-level operations in a database. These operations include:</p> <ul> <li>Creating, altering, and dropping databases and tables</li> <li>Granting, revoking, and altering user privileges</li> <li>Running backup and restore operations</li> </ul>"},{"location":"Specifics/Databases/#security-concerns","title":"Security Concerns","text":"<p>Access Control: The primary security concern is unauthorized access. If a malicious user gains administrative access, they could potentially alter the database, damage the data, or grant themselves further permissions.</p> <p>Data Integrity: Administrators have the ability to alter the database schema. If not properly controlled, this could lead to issues with data integrity.</p> <p>Data Loss: Administrators typically have the ability to drop databases and tables. If a malicious user or even a careless administrator were to drop critical databases or tables, it could result in significant data loss.</p>"},{"location":"Specifics/Databases/#system-privileges","title":"System Privileges","text":"<p>System privileges are more general than administrative privileges and allow a user to perform actions within the operating system that the database resides on. These privileges include:</p> <ul> <li>Reading and writing files</li> <li>Starting and stopping services</li> <li>Installing and uninstalling software</li> </ul>"},{"location":"Specifics/Databases/#security-concerns_1","title":"Security Concerns","text":"<p>System Integrity: If a user has system privileges, they can potentially alter critical system settings, which could destabilize the system or make it vulnerable to further attacks.</p> <p>Data Privacy: System privileges often give the user the ability to read and write files. This could potentially be used to access sensitive data or introduce malicious software.</p> <p>Denial of Service: A user with system privileges could potentially stop critical services, resulting in a denial of service.</p> <p>To mitigate these security concerns, it's important to follow the principle of least privilege, which means only granting users the minimum privileges they need to perform their tasks. Additionally, regular audits and monitoring can help detect any unusual activity or privilege escalation.</p>"},{"location":"Specifics/File%20Systems/","title":"File Systems","text":""},{"location":"Specifics/File%20Systems/#list-open-file-handles","title":"List open file handles","text":"<ul> <li>lsof -p process-id</li> <li>Or ls /proc/process-id/fd</li> </ul>"},{"location":"Specifics/File%20Systems/#what-is-an-inode","title":"What is an inode?","text":"<p>An inode is a data structure in Unix that contains metadata about a file. Some of the items contained in an inode are: 1) mode 2) owner (UID, GID) 3) size 4) atime, ctime, mtime 5) acl\u2019s 6) blocks list of where the data is</p> <p>The filename is present in the parent directory\u2019s inode structure.</p>"},{"location":"Specifics/File%20Systems/#what-is-the-difference-between-atime-mtime-and-ctime","title":"What is the difference between atime, mtime and ctime?","text":"<p>atime is the last time a file was accessed. For instance if you opened the file to view it.</p> <p>mtime is the last time a file was modified. For instance you changed some text in the file, if it\u2019s a text file.</p> <p>ctime is the last time the inode contents were changed of a file, for instance the mode, or owner.</p>"},{"location":"Specifics/File%20Systems/#what-is-the-difference-between-a-soft-link-and-a-hard-link","title":"What is the difference between a soft link and a hard link?","text":"<p>1) Hardlink shares the same inode number as the source link. Softlink has a different inode number. Example:</p> <p>$ touch a</p> <pre><code>$ ln a b  \n$ ls -i a b  \n24 a 24 b  \n$ ln -s a c  \n$ ls -i a c  \n24 a 25 c\n</code></pre> <p>2) In the data portion of the softlink is the name of the source file 3) Hardlinks are only valid in the same filesystem, softlinks can be across filesystems</p>"},{"location":"Specifics/File%20Systems/#when-would-you-use-a-hardlink-over-a-softlink","title":"When would you use a hardlink over a softlink?","text":"<p>A hardlink is useful when the source file is getting moved around, because renaming the source does not remove the hardlink connection. On the other hand, if you rename the source of a softlink, the softlink is broken. This is because hardlink\u2019s share the same inode, and softlink uses the source filename in it\u2019s data portion.</p>"},{"location":"Specifics/File%20Systems/#describe-lvm-and-how-it-can-be-helpful","title":"Describe LVM and how it can be helpful","text":"<p>LVM stands for logical volume manager and it is a way of grouping disks into logical units. </p> <p>The basic unit of LVM is a PE or a physical extent. One disk may be divided into one or more PE\u2019s. One or more PE\u2019s are contained in a VG or a volume group. Or or more LV or logical volumes are created out of a VG. </p> <p>Example - if we have a server with 2x1TB disk drives, we can create 4xPE\u2019s on it, each one being 500GB.  - On disk 1 let\u2019s say we name the PE\u2019s PE1 and PE3 and on disk 2 we name the PE\u2019s PE2 and PE4.  - We can then create VG0 out of PE1 and PE2, and VG1 out of PE3 and PE4.  - After that we can create a LV called /root and another one called swap on VG0.</p> <p>An advantage of using LVM is that we can create \u2018software\u2019 RAID, i.e., we can join multiple disks into one bigger disk. We cannot select the RAID level with LVM, for instance we cannot say that a VG is of RAID 5 type, however we are able to pick and chose the different PE\u2019s we want in a VG. Also LVM allows for dynamically growing a disk.</p>"},{"location":"Specifics/File%20Systems/#what-is-md-and-how-do-you-use-it","title":"What is \u2018md\u2019 and how do you use it?","text":"<p>MD is Linux software RAID. RAID can be done either in hardware wherein there is a RAID controller that does RAID and presents a logical volume to the OS, or RAID can be done in software wherein the kernel has a RAID driver which takes one or more disks can does RAID across them. \u2018MD\u2019 refers to the software RAID component of Linux.</p>"},{"location":"Specifics/File%20Systems/#if-a-filesystem-is-full-and-you-see-a-large-file-that-is-taking-up-a-lot-of-space-how-do-make-space-on-the-filesystem","title":"If a filesystem is full, and you see a large file that is taking up a lot of space, how do make space on the filesystem?","text":"<p>1) If no process has the filehandle open, you can delete the file 2) If a process has the filehandle open, it is better if you do not delete the file, instead you can \u2018cp /dev/null\u2019 on the file, which will reduce it\u2019s size to 0. 3) A filesystem has a reserve, you can reduce the size of this reserve to create more space using tunefs.</p>"},{"location":"Specifics/File%20Systems/#what-is-the-difference-between-character-device-and-block-device","title":"What is the difference between character device and block device?","text":"<p>Block devices are generally buffered and are read/written to in fixed sizes, for instance hard drives, cd-roms. </p> <p>Characters devices read/writes are one character at a time, such as from a keyboard or a tty, and are not buffered.</p>"},{"location":"Specifics/Google%20Systems%20Design%20-%20Code%20Deployment/","title":"Google Systems Design - Code Deployment","text":"<p>Must be global and fast</p>"},{"location":"Specifics/Google%20Systems%20Design%20-%20Code%20Deployment/#deployment-from-master-to-global","title":"Deployment from master to global","text":"<pre><code>Questions asked:\n* Assume it has already been tested and reviewed. \n* No. of machines: 100,000's of machines all over the world in n number of regions.\n* Is it internal or external service?\n* Since it is internal, is it OK with a lower levels of availability?\n* Deployment system must be reliable enough to rollback and roll forward.\n* 30 minutes for entire deployment?\n* All deployments must end up in terminal state, failed or success.\n* Availability target is 99 or 99.9%\n* How big are the binaries?  Say 10GB.\n\nRequirements\n* Trigger build\n* Builds the binary\n* Deploys the binary effeciently\n* \n\nActions\n* Trigger build using SHA from commits\n    * Commits are FIFO to create binaries\n    * Want to persist the status of builds in case of system disaster\n        * For a very long time, do not leave in memory\n        * Maybe use SQL queue\n            * Jobs Table Description\n                * Autogenerated ID\n                * Name\n                * Creation time\n                * Commit SHA\n                * Status\n                    * Queued\n                    * Processing\n                    * Fail\n                    * Success\n                    * Cancelled\n                * Building Node Name\n                * Last HeartBeat\n                * Binary Hash\n        * SQL gives **ACID** transactions to make sure the builds are concurrency safe\n        * You can write the SQL query to make sure no jobs are currently processing\n            * `select * from jobs where status=\"queued\" order by created asc limit 1`\n            * `BEGIN TRANSACTION; update jobs set status=\"processing\" where id=_id; COMMIT`\n        * Use way to make health check on the bulding node\n            * Nodes update heartbeat in the table\n            * Set a max timeout for non-response of the heartbeat\n            * If node is timed out but still building, it can check at end of build that is markd as failed and maybe send an alert to refine the system\n    * Estimate size of build system in nodes\n        *  Requirements * 5000 builds per day, 15m per build\n        * Ask what are worst case scenarios for builds happening at once. **5000 builds per day don't come evenly**\n        * Estimation _if even_ 24hrs*60m/15m = 96\n            * 5000/96 = 52 nodes\n            * Padding: 20% = 63 nodes\n    * Each node can download the entire source code and apply incremental patches if needed, when needed\n* Builds the binary\n    * Before marking build as succesful\n        * Make sure the binary is in local blob storage\n        * Update tables/deployment managers in other regions to know that there is a binary that is built and ready to be copied, maybe give binary blob location\n* Store binary in blob storage\n    * Can't have 100k machines downloading binary from blobs\n    * Best to use peer-to-peer networking, like bittorrent\n* Deploys the binary effeciently\n    * How do the peers know to download a new file?\n        * Peers should know what their \"goal\" state is * meaning this is the version they should be running\n            * Use Key/Value pair to know * App -&gt; Version\n            * Maybe store this value in something like Zookeeper\n            * Every peer has a key/value system that is watching for changes\n            * If value is found, it starts downloading via bittorrent\n    * How do peers know to not build over each other?\n</code></pre>"},{"location":"Specifics/HTTP/","title":"HTTP","text":""},{"location":"Specifics/HTTP/#common-http-response-codes","title":"Common Http response codes","text":"<p>200 OK The request has succeeded 500 Internal Server Error (Server Error) 403 Forbidden 301 Permanent Redirect 302 Temporary Redirect</p> <p>Reference: http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html</p>"},{"location":"Specifics/HTTP/#what-is-an-http-cookie","title":"What is an HTTP Cookie","text":"<p>Http cookie is a small piece of data that a server sends to a browser, which a browser usually stores in it\u2019s cookie cache. Cookie can be used to maintain session information since HTTP is stateless, and also for user preferences at a given site. Cookies can also be used to store encrypted password. Browsers send cookies back to the server when they make a connection\u2019 Reference: http://en.wikipedia.org/wiki/HTTP_cookie</p>"},{"location":"Specifics/HTTP/#http-methods","title":"HTTP Methods","text":"<p>Http methods are ways of communicating between server and client.  GET, POST, PUT, PATCH, DELETE</p> <p>HTTP is an application layer protocol relying on lower-level protocols such as\u00a0TCP\u00a0and\u00a0UDP.</p>"},{"location":"Specifics/HTTP/#http-headers","title":"HTTP Headers","text":"<p>Http header fields are common components of HTTP requests and responses. Headers are colon separated name-value pairs in clear text. Some common headers are: Cache-control which specifies where to cache or not the contents of a page, Accept, which can be text/plain, Content-length which specifies the size of the content, Host, which is the domain name of the server.  </p>"},{"location":"Specifics/HTTP/#remote-procedure-call-rpc","title":"Remote Procedure Call (RPC)","text":"<p> In an RPC, a client causes a procedure to execute on a different address space, usually a remote server. The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program. Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls. Popular RPC frameworks include\u00a0Protobuf,\u00a0Thrift, and\u00a0Avro.</p>"},{"location":"Specifics/HTTP/#disadvantages-rpc","title":"Disadvantage(s): RPC","text":"<ul> <li>RPC clients become tightly coupled to the service implementation.</li> <li>A new API must be defined for every new operation or use case.</li> <li>It can be difficult to debug RPC.</li> <li>You might not be able to leverage existing technologies out of the box. For example, it might require additional effort to ensure\u00a0RPC calls are properly cached\u00a0on caching servers such as\u00a0Squid.</li> </ul>"},{"location":"Specifics/HTTP/#representational-state-transfer-rest","title":"Representational State Transfer (REST)","text":"<p>REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server. The server provides a representation of resources and actions that can either manipulate or get a new representation of resources. All communication must be stateless and cacheable.</p> <p>There are four qualities of a RESTful interface: -   Identify resources (URI in HTTP)\u00a0- use the same URI regardless of any operation. -   Change with representations (Verbs in HTTP)\u00a0- use verbs, headers, and body. -   Self-descriptive error message (status response in HTTP)\u00a0- Use status codes, don't reinvent the wheel. -   HATEOAS\u00a0(HTML interface for HTTP)\u00a0- your web service should be fully accessible in a browser.</p> <p>REST is focused on exposing data. It minimizes the coupling between client/server and is often used for public HTTP APIs. REST uses a more generic and uniform method of exposing resources through URIs,\u00a0and actions through verbs such as GET, POST, PUT, DELETE, and PATCH. Being stateless, REST is great for horizontal scaling and partitioning.</p>"},{"location":"Specifics/HTTP/#disadvantages-rest","title":"Disadvantage(s): REST","text":"<ul> <li>With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy. For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path. With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.</li> <li>REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case. For example, moving expired documents to the archive folder might not cleanly fit within these verbs.</li> <li>Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.</li> <li>Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.</li> </ul>"},{"location":"Specifics/Linux%20Commands/","title":"Linux Commands","text":""},{"location":"Specifics/Linux%20Commands/#clis","title":"CLIs","text":""},{"location":"Specifics/Linux%20Commands/#netstat","title":"netstat","text":"<p>Networking tool being used for troubleshooting and configuration and used to display all network connections on a system.\u202fIt simply\u202fprovides a way to check whether various aspects of TCP/IP are working and what connections are present.</p> <p>Check all listening ports <code>netstat -ntlp</code> </p>"},{"location":"Specifics/Linux%20Commands/#ping","title":"ping","text":"<p>Check connection status between source and destination.  Useful for name resolution, too.</p>"},{"location":"Specifics/Linux%20Commands/#du","title":"du","text":"<p>Check size of a directory <code>du -sh /var/log/*</code></p>"},{"location":"Specifics/Linux%20Commands/#fdisk","title":"fdisk","text":"<p>Create partitions</p>"},{"location":"Specifics/Linux%20Commands/#wc","title":"wc","text":"<p>Count number of characters or lines in a file</p>"},{"location":"Specifics/Linux%20Commands/#grep","title":"grep","text":"<p>Search for a string in file(s)</p>"},{"location":"Specifics/Linux%20Commands/#env","title":"env","text":"<p>Print list of environment variables.  Can also be used to launch programs in different enviroments without modifying the current one.</p>"},{"location":"Specifics/Linux%20Commands/#pwd","title":"pwd","text":"<p>Show path</p>"},{"location":"Specifics/Linux%20Commands/#add-new-user-and-set-password","title":"Add new user and set password","text":"<p><code>useradd smith; passwd smith</code></p>"},{"location":"Specifics/Linux%20Commands/#check-memory","title":"Check memory","text":"<p>The command used mostly to check memory status in Linux is \u201cfree\u201d. Other commands that can be used are given below:\u00a0 -   \u201ccat\u201d command:\u00a0It can be used to show or display Linux memory information.      - <code>cat /proc/meminfo</code> -   \u201cvmstat\u201d command:\u00a0It can be used to report statistics of virtual memory.\u00a0 -   \u201ctop\u201d command:\u00a0It can be used to check the usage of memory.\u00a0 -   \u201chtop\u201d command:\u00a0It can be used to find the memory load of each process.</p>"},{"location":"Specifics/Linux%20Commands/#vmstat","title":"vmstat","text":"<pre><code>Procs\n    r: The number of processes waiting for run time.\n    b: The number of processes in uninterruptible sleep.\nMemory\n    swpd: the amount of virtual memory used.\n    free: the amount of idle memory.\n    buff: the amount of memory used as buffers.\n    cache: the amount of memory used as cache.\n    inact: the amount of inactive memory. (-a option)\n    active: the amount of active memory. (-a option)\nSwap\n    si: Amount of memory swapped in from disk (/s).\n    so: Amount of memory swapped to disk (/s).\nIO\n    bi: Blocks received from a block device (blocks/s).\n    bo: Blocks sent to a block device (blocks/s).\nSystem\n    in: The number of interrupts per second, including the clock.\n    cs: The number of context switches per second.\nCPU\n    These are percentages of total CPU time.\n    us: Time spent running non-kernel code. (user time, including nice time)\n    sy: Time spent running kernel code. (system time)\n    id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time.\n    wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle.\n    st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown.\n</code></pre>"},{"location":"Specifics/Linux%20Commands/#buffers-vs-cache","title":"Buffers vs Cache","text":"<p>Buffers are associated with a specific block device, and cover caching of filesystem metadata as well as tracking in-flight pages. The cache only contains parked file data. That is, the buffers remember what's in directories, what file permissions are, and keep track of what memory is being written from or read to for a particular block device. The cache only contains the contents of the files themselves.</p>"},{"location":"Specifics/Linux%20Commands/#pipe","title":"pipe","text":"<p>Basically a form of redirection that is used to send the output of one command to another command for further processing</p>"},{"location":"Specifics/Linux%20Commands/#unmask","title":"unmask","text":"<p>Unmask, also known as user file-creation mask, is a Linux command that allows you to set up default permissions for new files and folders that you create. In Linux OS, unmask command is used to set default file and folder permission. It is also used by other commands in Linux like mkdir, tee, touch, etc. that create files and directories.</p>"},{"location":"Specifics/Linux%20Commands/#dmesg","title":"dmesg","text":"<p>View boot logs</p>"},{"location":"Specifics/Linux%20Commands/#updatedb","title":"updatedb","text":"<p>The\u00a0updatedb\u00a0utility is part of\u00a0mlocate. It examines the entire file system and accordingly creates the database for the locate command</p>"},{"location":"Specifics/Linux%20Commands/#first-line-of-bash-script","title":"First Line of Bash Script","text":"<p><code>!#/bin/bash</code></p>"},{"location":"Specifics/Linux%20Commands/#lspci","title":"lspci","text":"<p>List devices and their drivers, physical locations</p>"},{"location":"Specifics/Linux%20Commands/#lshw","title":"lshw","text":"<p>List detailed information about hardware configurations as root user.  Some flags can be used to see details information about the networking hardware.</p>"},{"location":"Specifics/Linux%20Commands/#lsmod","title":"lsmod","text":"<p>List kernel modules</p>"},{"location":"Specifics/Linux%20Commands/#insmod-or-modprobe","title":"insmod (or modprobe)","text":"<p>Load one kernel module.  Can also use modprobe instead</p>"},{"location":"Specifics/Linux%20Commands/#rsync","title":"rsync","text":"<p>Transfer files either to or from a remote host</p>"},{"location":"Specifics/Linux%20Commands/#export","title":"export","text":"<p>Marks enviroment variables to be shared in forked processes.  Otherwise, variables only apply to current environment.</p>"},{"location":"Specifics/Linux%20Commands/#set","title":"set","text":"<p>Display, set or unset values of shell attributes and positional parameters. Debug script: <code>set -x</code> Set positional arguments</p> <pre><code>&gt;set red blue green\n&gt;echo $1 $2 $3\nred blue green\n\n#unset \n&gt;set\u00a0--\n</code></pre> <p>Don't ignore when pipes fail <code>set\u00a0-eo\u00a0pipefail</code></p>"},{"location":"Specifics/Linux%20Commands/#umask","title":"umask","text":"<p>unmask stands for user file creation mode. When the user creates any file, it has default file permissions. So unmask will specify few restrictions to the newly created file (it controls the file permissions).</p>"},{"location":"Specifics/Linux%20Commands/#reduce-or-shrink-the-size-of-the-lvm-partition","title":"Reduce or shrink the size of the LVM partition?","text":"<p>Below are the logical steps to reduce the size of the LVM partition: -   Unmount the file system using the\u00a0unmount\u00a0command -   Use the\u00a0resize2fs\u00a0command as follows:</p> <p><code>resize2fs /dev/mapper/myvg-mylv 10G</code></p> <p>Then, use the\u00a0lvreduce\u00a0command as follows: <code>lvreduce -L 10G /dev/mapper/myvg-mylv</code></p>"},{"location":"Specifics/Linux%20Commands/#repquota","title":"repquota","text":"<p>uncommon question unless you expect many user work spaces check the status of a user\u2019s defined quota, along with the disk space and the number of files used.</p>"},{"location":"Specifics/Linux%20Commands/#different-types-of-modes-used-in-vi","title":"Different types of modes used in VI","text":"<p> * Command Mode - from Escape * Insert/Edit Mode - from 'i' * Execution/Replacement - from ':'</p>"},{"location":"Specifics/Linux%20Commands/#networking-specific","title":"Networking Specific","text":""},{"location":"Specifics/Linux%20Commands/#ssh-copy-id","title":"ssh-copy-id","text":"<p>Install your public key in a remote machine's authorized_keys.</p>"},{"location":"Specifics/Linux%20Commands/#ssh-keygen","title":"ssh-keygen","text":"<p>Generate ssh keys used for authentication, password-less logins, and other things.</p>"},{"location":"Specifics/Linux%20Commands/#ip","title":"ip","text":"<p>Identify network interfaces  <code>ip a</code></p> <p>Add temporary address to device  <code>sudo ip addr add 10.102.66.200/24 dev enp0s25</code></p> <p>Set link up or down</p> <pre><code>ip link set dev enp0s25 up\nip link set dev enp0s25 down\n</code></pre> <p>Add route to interface</p> <pre><code>ip route add default via 10.102.66.1\n</code></pre> <p>Show route</p> <pre><code>ip route show\n</code></pre>"},{"location":"Specifics/Linux%20Commands/#ethtool","title":"ethtool","text":"<p>Displays and changes Ethernet card settings such as auto-negotiation, port speed, duplex mode, and Wake-on-LAN.</p>"},{"location":"Specifics/Linux%20Commands/#restart-networking","title":"Restart Networking","text":"<pre><code>sudo /etc/init.d/networking restart\n# or\nsudo systemctl restart networking\n</code></pre>"},{"location":"Specifics/Linux%20Commands/#network-status","title":"Network Status","text":"<pre><code># sudo /etc/init.d/networking status\nor\n# sudo systemctl status networking\n</code></pre>"},{"location":"Specifics/Linux%20Commands/#netplan","title":"Netplan","text":"<p>The way Ubuntu and some others do things</p> <p>Apple netplan <code>netplan apply</code></p>"},{"location":"Specifics/Linux%20Interview%20Questions/","title":"Linux Interview Questions","text":""},{"location":"Specifics/Linux%20Interview%20Questions/#are-linux-commands-case-sensitive-easy-red-flag-question","title":"Are Linux commands case sensitive (easy red flag question)?","text":"<p>Yes</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#location-of-system-logs","title":"Location of system logs","text":"<p>/var/log/messages</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#command-to-view-boot-logs","title":"Command to view boot logs?","text":"<p>dmesg</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#can-you-legally-edit-the-linux-kernel-code","title":"Can you legally edit the Linux kernel code?","text":"<p>It is open source software under GPL2.  Anyone can edit and make changes.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#where-is-swap-space-located-and-what-is-it-used-for","title":"Where is swap space located and what is it used for?","text":"<p>It is located in a dedicated swap partition.  It is used to extend the available memory to start new processes, by storing infrequently used pages there.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-is-the-difference-between-memory-buffers-and-cache","title":"What is the difference between memory buffers and cache?","text":"<p>Buffers remember what's in directories, what file permissions are, and keep track of what memory is being written from or read to for a particular block device. The cache only contains the contents of the files themselves.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-is-the-difference-between-hard-and-symbolic-links","title":"What is the difference between hard and symbolic links?","text":"<p>Hard Links * Points to another file's inode directly (the physical location) * Acts like an additional name to the same file * Cannot be used across file systems.</p> <p>Soft Links - Points to another filename - if original filename is deleted, softlink becomes broken - Can be used across file names</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#you-see-the-load-average-of-a-system-and-the-numbers-are-05-1-2-what-does-this-mean","title":"You see the load average of a system and the numbers are 0.5 1 2.  What does this mean?","text":"<p>For the past minute, 0.5 processes were waiting for the cpu.  In the past minutes, an average of one process was waiting.  For the past 15, an average of 2 processes were waiting.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-is-the-first-process-that-is-started-by-the-kernel-in-linux-and-what-is-its-process-id","title":"What is the first process that is started by the kernel in Linux and what is its process id?","text":"<p>The first process started by the kernel in Linux is \u201cinit\u201d and its process id is 1.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#a-large-file-is-taking-up-all-the-space-on-a-partition-you-delete-it-yet-the-partition-is-still-full-why","title":"A large file is taking up all the space on a partition.  You delete it, yet the partition is still full.  Why?","text":"<p>There is a process holding the file handle open.  It needs to be stopped before the space can be freed.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-command-can-you-use-to-show-which-process-is-holding-the-file-open","title":"What command can you use to show which process is holding the file open?","text":"<p>lsof</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-types-of-filesdirectories-start-with","title":"What types of files/directories start with '.'?","text":"<p>Hidden</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-is-the-pwd-command","title":"What is the pwd command?","text":"<p>Shows current path</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-are-the-three-permissions-under-linux","title":"What are the three permissions under Linux?","text":"<p>Read, Write, Execute</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#how-are-the-three-permissions-applied-on-the-system","title":"How are the three permissions applied on the system?","text":"<p>User, Group, Everyone/Other/All</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-command-is-used-to-set-permissions","title":"What command is used to set permissions?","text":"<p>chmod</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-is-the-purpose-of-a-sticky-bit","title":"What is the purpose of a sticky bit?","text":"<p>Prevents anyone who is not the owner, owning group or root from removing or renaming a file</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-command-do-you-use-to-change-filedirectory-ownership","title":"What command do you use to change file/directory ownership?","text":"<p>chown</p> <p>Should you use telnet to administrate a Linux server? No, it is not encrypted.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#differences-between-cron-and-anacron","title":"Differences between Cron and Anacron?","text":"<ul> <li>A Cron job can be scheduled by any normal user, while Anacron can be scheduled only by a superuser</li> <li>Cron expects the system to be up and running, while Anacron doesn\u2019t expect this all the time. In the case of Anacron, if a job is scheduled and the system is down at this time, it will execute the job as soon as the system is up and running.</li> <li>Cron is ideal for servers, Anacron is ideal for both desktops and laptops.</li> <li>Cron should be used when we want a job to be executed at a particular hour and minute, while Anacron should be used when the job can be executed at any time.</li> </ul>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-is-a-zombie-process","title":"What is a Zombie process?","text":"<p>It's a dead or finished process that still has an entry in the process table.  It's parent hasn't read the exit status.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-commands-do-you-run-to-kill-a-process","title":"What commands do you run to kill a process?","text":"<p>ps and kill.  Maybe some sudo if you don't have the rights.</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#tell-me-about-the-proc-file-system","title":"Tell me about the /proc file system","text":"<p>The /proc file system is a RAM based file system which maintains information about the current state of the running kernel including details on CPU, memory, partitioning, interrupts, I/O addresses, DMA channels, and running processes. This file system is represented by various files which do not actually store the information, they point to the information in the memory. The /proc file system is maintained automatically by the system</p> <p>Explain key based authentication (more details the better) - The generation of private/public air <code>ssh-keygen</code> - Which key is transferred to the remote host (public) via  <code>ssh-copy-id</code> (or manually) - The file where public keys are stored <code>authorized_keys</code> - Proper permissions for authorized_keys <code>chmod 600 ~/.ssh/authorized_keys</code></p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-does-lspci-do","title":"What does lspci do?","text":"<p>The lspci command displays information about PCI buses and the devices attached to your system</p>"},{"location":"Specifics/Linux%20Interview%20Questions/#what-are-the-different-types-of-modes-in-vi-editor","title":"What are the different types of modes in VI editor?","text":"<ul> <li>Command Mode - from Escape</li> <li>Insert/Edit Mode - from 'i'</li> <li>Execution/Replacement - from ':'</li> </ul>"},{"location":"Specifics/Linux%20Interview%20Questions/#name-the-four-configuration-management-tools-used-in-unix-like-operating-systems","title":"Name the four Configuration Management Tools used in UNIX-like operating systems.","text":"<ul> <li>Ansible</li> <li>Chef</li> <li>Puppet</li> <li>CFEngine</li> </ul>"},{"location":"Specifics/Linux%20Networking/","title":"IP","text":""},{"location":"Specifics/Linux%20Networking/#what-is-ipv4","title":"What is IPv4?","text":"<p>IPv4\u00a0is an IP version widely used to identify devices on a network using an addressing system. It was the first version of IP deployed for production in the ARPANET in 1983. It uses a 32-bit address scheme to store 2^32 addresses which is more than 4 billion addresses. It is considered the primary Internet Protocol and carries 94% of Internet traffic.</p>"},{"location":"Specifics/Linux%20Networking/#what-is-ipv6","title":"What is IPv6?","text":"<p>IPv6\u00a0is the most recent version of the Internet Protocol. This new IP address version is being deployed to fulfill the need for more Internet addresses. It was aimed to resolve issues that are associated with IPv4. With 128-bit address space, it allows 340 undecillion unique address space. IPv6 is also called IPng (Internet Protocol next generation).</p>"},{"location":"Specifics/Linux%20Networking/#key-difference","title":"KEY DIFFERENCE","text":"<ul> <li>IPv4 is 32-Bit IP address whereas IPv6 is a 128-Bit IP address.</li> <li>IPv4 is a numeric addressing method whereas IPv6 is an alphanumeric addressing method.</li> <li>IPv4 binary bits are separated by a dot(.) whereas IPv6 binary bits are separated by a colon(:).</li> <li>IPv4 offers 12 header fields whereas IPv6 offers 8 header fields.</li> <li>IPv4 supports broadcast whereas IPv6 doesn\u2019t support broadcast.</li> <li>IPv4 has checksum fields while IPv6 doesn\u2019t have checksum fields</li> <li>When we compare IPv4 and IPv6, IPv4 supports VLSM (Variable Length Subnet Mask) whereas IPv6 doesn\u2019t support VLSM.</li> <li>IPv4 uses ARP (Address Resolution Protocol) to map to MAC address whereas IPv6 uses NDP (Neighbour Discovery Protocol) to map to MAC address.</li> <li>Because an hexadecimal number uses 4 bits this means that an IPv6 address consists of\u00a032 hexadecimal numbers.</li> </ul> <p> Note:\u00a0Because of the length of IPv6 addresses various\u00a0shortening techniques\u00a0are employed. The main technique being to omit\u00a0repetitive 0\u2019s\u00a0as shown in the example above.</p> <p></p> <p>The upper 64 bits are used for\u00a0routing. - upper 64 bits in more detail we can see that it is split into 2 blocks of\u00a048\u00a0and\u00a016 bits\u00a0respectively the lower 16 bits are used for\u00a0subnets\u00a0on an internal networks, and are controlled by a network administrator.</p> <p>The lower 64 bits identify the address of the interface or node, and is derived from the actual physical or\u00a0MAC address</p>"},{"location":"Specifics/Linux%20Networking/#address-types-and-scope","title":"Address Types and Scope","text":"<p>IPv6 addresses have three types: -   Global Unicast Address\u00a0\u2013Scope Internet- routed on Internet -   Unique Local\u00a0\u2014 Scope Internal Network or VPN internally routable, but\u00a0Not routed\u00a0on Internet -   Link Local\u00a0\u2013 Scope network link-\u00a0Not Routed\u00a0internally or externally.</p> <p></p>"},{"location":"Specifics/Linux%20Networking/#ipv6-loop-back","title":"IPv6 Loop Back","text":"<p>The IPv6 loopback address is ::1.</p>"},{"location":"Specifics/Linux%20Networking/#ipv4-wo-dhcp","title":"IPV4 w/o DHCP","text":"<p>The 169.254.0.0/16 network is used for\u00a0Automatic Private IP Addressing, or\u00a0APIPA. If a DHCP client attempts to get an address, but fails to find a DHCP server after the timeout and retries period it will randomly assume an address from this network.</p>"},{"location":"Specifics/Linux%20Networking/#networking","title":"Networking","text":""},{"location":"Specifics/Linux%20Networking/#why-etcresolvconf-and-etchosts-files-are-used","title":"Why /etc/resolv.conf and /etc/hosts files are used?","text":"<p>/etc/resolv.conf:\u00a0It is used to configure DNS name servers as it contains the details of the nameserver i.e., details of your DNS server. The DNS server is then used to resolve the hostname of the IP address.\u00a0  </p> <p>/etc/hosts:\u00a0It is used to map or translate any hostname or domain name to its relevant IP address.</p>"},{"location":"Specifics/Linux%20Networking/#advantages-of-using-nic-teaming","title":"Advantages of using NIC teaming?","text":"<p>NIC (Network Interface Card) teaming has several advantages as given below:\u00a0 -   Load Balancing \u00a0 -   Failover \u00a0 -   Increases uptime</p>"},{"location":"Specifics/Linux%20Networking/#network-bondingteaming","title":"Network Bonding/Teaming","text":"<p>Network Bonding, also known as NIC Teaming, is a type of bonding that is used to connect multiple network interfaces into a single interface. It usually improves performance and redundancy simply by increasing network throughput and bandwidth.</p>"},{"location":"Specifics/Linux%20Networking/#network-bonding-modes","title":"Network bonding modes","text":"<p>Doubt anyone will ask this question - just know there are different modes Different network bonding modes used in Linux are listed below:\u00a0 -   Mode-0 (balance-rr):\u00a0It is the default mode and is based on round-robin policy. It offers features like fault tolerance and load balancing.\u00a0 -   Mode-1 (active-backup):\u00a0It is based on an active-backup policy. In this, only one node responds or works at the time of failure of other nodes.\u00a0 -   Mode-2 (balance-xor):\u00a0It sets an XOR (exclusive-or) mode for providing load balancing and fault tolerance. \u00a0 -   Mode-3 (broadcast):\u00a0It is based on broadcast policy. It sets a broadcast mode for providing fault tolerance and can be used only for specific purposes. \u00a0 -   Mode-4 (802.3ad):\u00a0It is based on IEEE 802.3ad standard also known as Dynamic Link Aggregation mode. It sets an IEEE 802.3ad dynamic link aggregation mode and creates aggregation groups that share the same speed and duplex settings.\u00a0 -   Mode-5 (balance-tlb):\u00a0It is also known as Adaptive TLB (Transmit Load Balancing). It sets TLB mode for fault tolerance and load balancing. In this mode, traffic will be loaded based on each slave of the network. \u00a0 -   Mode-6 (balance-alb):\u00a0It is also known as Adaptive Load Balancing. It sets ALB mode for fault tolerance and load balancing. It doesn\u2019t need any special switch support.</p>"},{"location":"Specifics/Linux%20Networking/#default-ports-used-for-dns-smtp-ftp-ssh-dhcp-and-squid","title":"Default ports used for DNS, SMTP, FTP, SSH, DHCP and squid","text":""},{"location":"Specifics/Linux%20Networking/#location-of-network-interface-config-file","title":"Location of Network Interface Config File","text":"<p>Old way: <code>/etc/network/interfaces</code></p> <p>Redhat/CentOS <code>/etc/sysconfig/network</code></p> <p>New way: <code>/etc/sysconfig/network-scripts</code> Use:  <code>netplan</code></p> <p></p>"},{"location":"Specifics/Linux%20Networking/#hostname-lookup-order","title":"Hostname lookup order","text":"<p>List order of host name search. Typically look at local files, then\u00a0NIS\u00a0server, then\u00a0DNS\u00a0server. <code>/etc/nsswitch.conf</code></p>"},{"location":"Specifics/Linux%20OS%20Internals/","title":"Linux OS Internals","text":"<p>mount fstab lvm lspci lspof top how to find file system type uefi</p> <p></p> <ul> <li>Kernel:\u00a0It is considered a core or main part of Linux and is generally responsible for all major activities of OS such as process management, device management, etc. \u00a0</li> <li>System Library:\u00a0These are special functions or programs with the help of which application programs or system utilities can access features of the kernel without any requirement of code. It is simply used to implement the functionality of the OS.\u00a0</li> <li>System Utility:\u00a0These are utility programs that are responsible to perform specialized and individual-level tasks. They are considered more liable and allow users to manage the computer. \u00a0</li> <li>Hardware:\u00a0It is physical hardware that includes items such as a mouse, keyboard, display, CPU, etc.\u00a0</li> <li>Shell:\u00a0It is an environment in which we can run our commands, shell scripts, and programs. It is an interface between user and kernel that hides all complexities of functions of the kernel from the user. It is used to execute commands.</li> </ul>"},{"location":"Specifics/Linux%20OS%20Internals/#what-is-kernel-explain-its-functions","title":"What is Kernel? Explain its functions.","text":"<p>A kernel is considered the main component of Linux OS. It is simply a resource manager that acts as a bridge between hardware and software. Its main role is to manage hardware resources for users and is generally used to provide an interface for user-level interaction. A kernel is the first program that is loaded whenever a computer system starts. It is also referred to as low-level system software.</p> <p>Its other main functions include:\u00a0</p> <ul> <li>Memory Management</li> <li>Process Management</li> <li>Device Management</li> <li>Storage Management</li> <li>Manage access, and use of various peripherals that are connected to the computer.</li> </ul>"},{"location":"Specifics/Linux%20OS%20Internals/#what-are-two-types-of-linux-user-mode","title":"What are two types of Linux User Mode?","text":"<ul> <li>Command Line\u00a0</li> <li>GUI</li> </ul>"},{"location":"Specifics/Linux%20OS%20Internals/#what-is-lilo-and-grub","title":"What is LILO and Grub","text":"<p>LILO (Linux Loader) is basically a bootloader for Linux that is used to load Linux into memory and start the OS.  Only for Linux</p> <p>GRUB  - Supports all operating systems - Allows selecting of specific Linux Kernel</p>"},{"location":"Specifics/Linux%20OS%20Internals/#what-is-swap-space","title":"What is swap space?","text":"<p>Swap space, as the name suggests, is basically a space on a hard disk that is used when the amount of physical memory or RAM is full. It is considered a substitute for physical memory.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#process-states-in-linux","title":"Process States in Linux","text":"<ul> <li>New/Ready:\u00a0In this state, a new process is created and is ready to run.</li> <li>Running:\u00a0In this state, the process is being executed.</li> <li>Blocked/Wait:\u00a0In this state, the process is waiting for input from the user and if doesn't have resources to run such as memory, file locks, input, then it can remain in a waiting or blocked state.</li> <li>Terminated/Completed:\u00a0In this state, the process has completed the execution or terminated by the OS.</li> <li>Zombie:\u00a0In this state, the process is terminated but information regarding the process still exists and is available in the process table.</li> </ul>"},{"location":"Specifics/Linux%20OS%20Internals/#linux-shell","title":"Linux Shell","text":"<p>Linux shell is a user interface present between user and kernel. It is used for executing commands and communication with Linux OS. Linux shell is basically a program used by users for executing commands. It accepts human-readable commands as input and converts them into kernel understandable language</p> <p></p>"},{"location":"Specifics/Linux%20OS%20Internals/#maximum-length-for-a-filename","title":"Maximum length for a filename","text":"<p>255 bytes</p>"},{"location":"Specifics/Linux%20OS%20Internals/#maximum-length-for-a-filename_1","title":"Maximum length for a filename","text":"<p>Double memory size</p>"},{"location":"Specifics/Linux%20OS%20Internals/#file-permissions-in-linux","title":"File Permissions in Linux","text":"<p>Read (r):\u00a0It allows the user to open and read the file or list the directory.\u00a0 Write (w):\u00a0It allows the user to open and modify the file. One can also add new files to the directory.\u00a0 Execute (x):\u00a0It allows the user to execute or run the file. \u00a0One can also lookup a specific file within a directory.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#automatically-mount-file-systems","title":"Automatically mount file systems","text":"<p>/etc/fstab</p>"},{"location":"Specifics/Linux%20OS%20Internals/#lvm","title":"LVM","text":"<p>LVM (Logical Volume Management) is basically a tool that provides logical volume management for the Linux kernel.   It is an abstraction layer and can do - Allocate disks - strip/mirror - resizing (grow and reduce)</p> <p></p>"},{"location":"Specifics/Linux%20OS%20Internals/#proc-file-system","title":"\u201c/proc\u201d file system","text":"<p>Proc file system is a pseudo or virtual file system that provides an interface to the kernel data structure. It generally includes useful information about processes that are running currently. It can also be used to change some kernel parameters at runtime or during execution. It is also regarded as a control and information center for the kernel. All files under this directory are named virtual files.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#daemons","title":"Daemons","text":"<p>Background processes with no controlling terminal.  Generally started at boot and die at shutdown.  Main purpose is to handle requests and forward them to some program for execution.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#exec","title":"Exec","text":"<p>The <code>exec</code> call is a way to basically replace the entire current process with a new program. It loads the program into the current process space and runs it from the entry point.</p> <p>Shells typically do this whenever you try to run a program like <code>find</code> - the shell forks, then the child loads the <code>find</code> program into memory, setting up all command line arguments, standard I/O and so forth.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#zombie-process","title":"Zombie Process","text":"<p>Zombie Process, also referred to as a defunct or dead process in Linux, is a process that has finished the execution, but its entry remains in the process table.  Happens when the parent hasn't read the child's status and reaped it.  Zombie Processes don't use any system resources are just dead entries in the process table.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#difference-between-cron-and-anacron","title":"Difference between cron and anacron?","text":"<p>Cron:\u00a0It is a program in Linux that is used to execute tasks at a scheduled time. It works effectively on machines that run continuously.\u00a0  </p> <p>Anacron:\u00a0It is a program in Linux that is used to execute tasks at certain intervals. It works effectively on machines that are powered off in a day or week. </p>"},{"location":"Specifics/Linux%20OS%20Internals/#load-average","title":"Load average","text":"<p>From uptime and top: <code>load average: 1.05, 0.70, 5.09</code></p> <p>This means: - load average over the last 1 minute: 1.05     - The computer was overloaded by 5% on average. On average, .05 processes were waiting for the CPU. (1.05) - load average over the last 5 minutes: 0.70     - The CPU idled for 30% of the time. (0.70) - load average over the last 15 minutes: 5.09     - The computer was overloaded by 409% on average. On average, 4.09 processes were waiting for the CPU. (5.09)</p>"},{"location":"Specifics/Linux%20OS%20Internals/#node-and-process-id","title":"NODE and Process Id","text":"<p>INODE:\u00a0It is a unique name given to each file by OS. Each inode has a unique inode number within a file system. It stores various information about files in Linux such as ownership, file size, file type, access mode, number of links, etc. \u00a0  </p> <p>Process Id (Identifier):\u00a0It is a unique Id given to each process. It is simply used to uniquely identify an active process throughout the system until the process terminates.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#first-process-that-is-started-by-the-kernel-in-linux-and-what-is-its-process-id","title":"First process that is started by the kernel in Linux and what is its process id","text":"<p>The first process started by the kernel in Linux is \u201cinit\u201d and its process id is 1.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#is-there-any-relation-between-the-modprobeconf-file-and-network-devices","title":"Is there any relation between the modprobe.conf file and network devices?","text":"<p>Yes, this file assigns a kernel module to each network device.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#difference-between-ext2-and-ext3-file-systems","title":"Difference between ext2 and ext3 file systems?","text":"<ul> <li>The ext3 file system is an enhanced version of the ext2 file system.</li> <li>The most important difference between ext2 and ext3 is that ext3 supports journaling.<ul> <li>Reduces need for file check in all but the most dire circumstances.</li> </ul> </li> </ul>"},{"location":"Specifics/Linux%20OS%20Internals/#explain-linux-boot-sequence","title":"Explain Linux Boot Sequence.","text":"<p>Answer:\u00a0There are six levels of a Linux Boot Sequence. These are as follows:</p> <p>BIOS:\u00a0Full form of BIOS is\u00a0Basic Input or Output System that performs integrity checks and it will search and load and then it will execute the bootloader.</p> <p>MBR:\u00a0MBR means Master Boot Record. MBR contains the information regarding GRUB and executes and loads this bootloader.</p> <p>GRUB:\u00a0GRUB means Grand Unified Bootloader. In case, many kernel images are installed on your system then you can select which one you want to execute.</p> <p>Kernel:\u00a0Root file system is mounted by Kernel and executes the /sbin/init program.</p> <p>Init:\u00a0Init checks the file\u00a0/etc/inittab\u00a0and decides the run level. There are seven-run levels available from 0-6. It will identify the default init level and will load the program.</p> <p>Runlevel programs:\u00a0As per your default settings for the run level, the system will execute the programs.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#explain-interrupts","title":"Explain Interrupts","text":"<p>Interrupts means the processor is transferred temporarily to another program or function. When that program is completed, the processor will be given back to that program to complete the task.  Interrupts can come from other programs and devices.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#what-is-page-frame","title":"What is page frame?","text":"<p>A page frame is a block of RAM that is used for virtual memory. It has its page frame number. The size of a page frame may vary from system to system, and it is in the power of 2 in bytes. Also, it is the smallest length block of memory in which an operating system maps memory pages.</p>"},{"location":"Specifics/Linux%20OS%20Internals/#user-virtual-address-vs-kernel-virtual-address","title":"\u201cuser virtual address\u201d vs \u201ckernel virtual address\"","text":"<p>User programs are run in \"user virtual address\".  If a program is running in kernel mode it'll use \"kernel virtual address\".  If the kernel program needs user interaction, it must be translated to user space.</p>"},{"location":"Specifics/Load%20Balancers/","title":"Load Balancers","text":""},{"location":"Specifics/Load%20Balancers/#reverse-proxy-servers","title":"Reverse Proxy Servers","text":"<p>Why would you use a reverse proxy server in front of a frontend server that already contains a 'webserver' (eg. tomcat)? - security (rejecting bad requests, throttling etc.) - network performance (often better at handling connections \u2013 not always) static media offload - offload common tasks eg. manage login cookies, modify headers caching - caching (e.g. nginx reverse proxy)</p>"},{"location":"Specifics/Load%20Balancers/#l4-vs-l7","title":"L4 vs L7","text":"<p>TODO - L1 - Channel multiplexing (e.g. WDM, GPRS) - L2 - Channel bonding (e.g. LACP) - L3 - IP level (e.g. BGP, OSPF) - L4 - TCP level (e.g: NAT) - L7 - HTTP level (e.g. webserver remaps/rewrites) - Application level (e.g. providing different URIs to different clients) Client side LB (e.g. multiple A(AAA) records)</p> <ul> <li>What advantages does using HTTP LB provide us, when compared with TCP LB for the same connection (ie. an HTTP connection)?  <ul> <li>HTTP<ul> <li>Can do addtional decoration or header manipulation</li> <li>Load balance based on endpoint<ul> <li>send different endpoints to different clusters/servers</li> </ul> </li> <li>block bad requests</li> <li>cache data</li> <li>can terminate SSL certs</li> <li>set affinity for users to servers - keep experience for one user consistent</li> <li>logging capabilities</li> </ul> </li> <li>TCP<ul> <li>more performant due to doing less</li> <li>can due TCP packet passthrough or can terminate the packet and establish new connection to the backend</li> <li>stateless</li> <li>don't need to be updated often</li> </ul> </li> <li>Combo<ul> <li>to grow load balancers you can put TCP LBs in front of HTTP LBs</li> <li>allow graceful upgrades/restarts of TCP LBs</li> </ul> </li> </ul> </li> </ul>"},{"location":"Specifics/Load%20Balancers/#different-techniques","title":"Different techniques","text":"<p>eg. least connections first, round robin, sticky, lowest latency first</p>"},{"location":"Specifics/Networking/","title":"Networking","text":""},{"location":"Specifics/Networking/#osi-model","title":"OSI Model","text":""},{"location":"Specifics/Networking/#define-tcp-slow-start","title":"Define TCP slow start","text":"<p>Tcp slow start is a congestion control algorithm that starts by increasing the TCP congestion window each time an ACK is received, until an ACK is not received.    Exponential * cwnd = 2 MSS</p> <p>The reason you want to set it is to help find the upper limit of the maximum number of MSS's (cwnd) that can be sent at one time during each RTT. * MSS (maximum segment size) limits the size of packets, or small chunks of data, that travel across a network, such as the Internet</p> <p>According to RFC 5681: A TCP state variable that limits the amount of data a TCP can send. At any given time, a TCP MUST NOT send data with a sequence number higher than the sum of the highest acknowledged sequence number and the minimum of cwnd and rwnd.</p> <p>Also, according to the same RFC:</p> <p>The congestion window (cwnd) is a sender-side limit on the amount of data the sender can transmit into the network before receiving an acknowledgment (ACK), while the receiver\u2019s advertised window (rwnd) is a receiver-side limit on the amount of outstanding data. The minimum of cwnd and rwnd governs data transmission.</p> <p>After reaching ssthresh (slow start threshold), algorithm changes to Congestion Avoidance.     * Linear     * cwnd += 1 MSS</p>"},{"location":"Specifics/Networking/#name-a-few-tcp-connections-states","title":"Name a few TCP connections states","text":"<ul> <li>LISTEN - represents waiting for a connection request from any remote TCP and port.</li> <li>SYN-SENT - represents waiting for a matching connection request after having sent a connection request.</li> <li>SYN-RECEIVED - represents waiting for a confirming connection request acknowledgment after having both received and sent a connection request.</li> <li>ESTABLISHED - represents an open connection, data received can be delivered to the user. The normal state for the data transfer phase of the connection.</li> <li>FIN-WAIT-1 - represents waiting for a connection termination request from the remote TCP, or an acknowledgment of the connection termination request previously sent.</li> <li>FIN-WAIT-2 - represents waiting for a connection termination request from the remote TCP.</li> <li>CLOSE-WAIT - represents waiting for a connection termination request from the local user.</li> <li>CLOSING - represents waiting for a connection termination request acknowledgment from the remote</li> <li>TCP.</li> <li>LAST-ACK - represents waiting for an acknowledgment of the connection termination request previously</li> <li>sent to the remote TCP (which includes an acknowledgment of its connection termination</li> <li>request).</li> <li>TIME-WAIT -represents waiting for enough time to pass to be sure the remote TCP received the</li> <li>acknowledgment of its connection termination request.</li> <li>CLOSED - represents no connection state at all.</li> </ul>"},{"location":"Specifics/Networking/#define-the-various-protocol-states-of-dhcp","title":"Define the various protocol states of DHCP","text":"<ul> <li>DHCPDISCOVER client-&gt;server : broadcast to locate server</li> <li>DHCPOFFER server-&gt;client : offer to client with offer of configuration parameters</li> <li>DHCPREQUEST client-&gt;server : requesting a dhcp config from server</li> <li>DHCPACK server-&gt;client : actual configuration paramters</li> <li>DHCPNAK server-&gt;client : indicating client\u2019s notion of network address is incorrect</li> <li>DHCPDECLINE client-&gt;server : address is already in use</li> <li>DHCPRELEASE client-&gt;server : giving up of ip address</li> <li>DHCPINFORM client-&gt;server : asking for local config parameters</li> </ul>"},{"location":"Specifics/Networking/#describe-tcp-header-format","title":"Describe TCP header format","text":"<ol> <li>Source port</li> <li>Destination port</li> <li>Sequence number</li> <li>Acknowledgement number</li> <li>Data offset</li> <li>Reserved</li> <li>Control bits</li> <li>Window</li> <li>Checksum</li> <li>Urgent Pointer</li> <li>Options</li> <li>Padding</li> <li>Data</li> </ol>"},{"location":"Specifics/Networking/#difference-between-tcpudp","title":"Difference between TCP/UDP","text":"<ul> <li>Reliable/Unreliable</li> <li>Ordered/Unordered</li> <li>Heavyweight/Lightweight</li> <li>Streaming</li> <li>Header size</li> </ul>"},{"location":"Specifics/Networking/#what-are-the-different-kind-of-nat-available","title":"What are the different kind of NAT available?","text":"<p>There is SNAT and DNAT. </p>"},{"location":"Specifics/Networking/#snat","title":"SNAT","text":"<p>Source network address translation.  </p> <p>For instance if you are at home using your cable modem and want to connect to an external site such as www.cnn.com, then your router will change the source address of the TCP packet to be it\u2019s external public IP.</p>"},{"location":"Specifics/Networking/#dnat","title":"DNAT","text":"<p>Destination network address translation. </p> <p>For instance when your packet reaches the http://www.cnn.com router, and the web server behind the router has internal IP addresses so the router changes the destination IP (public) to the LAN IP (private).</p>"},{"location":"Specifics/Networking/#dns","title":"DNS","text":""},{"location":"Specifics/Networking/#how-does-dns-work","title":"How does DNS work?","text":"<p> Root nameserver\u00a0- The\u00a0root server\u00a0is the first step in translating (resolving) human readable host names into IP addresses. It can be thought of like an index in a library that points to different racks of books - typically it serves as a reference to other more specific locations.</p> <p>TLD nameserver\u00a0- The top level domain server (TLD) can be thought of as a specific rack of books in a library. This nameserver is the next step in the search for a specific IP address, and it hosts the last portion of a hostname (In example.com, the TLD server is \u201ccom\u201d).</p> <p>Authoritative nameserver\u00a0- This final nameserver can be thought of as a dictionary on a rack of books, in which a specific name can be translated into its definition. The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor (the librarian) that made the initial request.</p>"},{"location":"Specifics/Networking/#whats-the-difference-between-an-authoritative-dns-server-and-a-recursive-dns-resolver","title":"What's the difference between an authoritative DNS server and a recursive DNS resolver?","text":"<p>Both concepts refer to servers (groups of servers) that are integral to the DNS infrastructure, but each performs a different role and lives in different locations inside the pipeline of a DNS query. One way to think about the difference is the\u00a0recursive\u00a0resolver is at the beginning of the DNS query and the authoritative nameserver is at the end.</p> <p></p>"},{"location":"Specifics/Networking/#the-8-steps-in-a-dns-lookup","title":"The 8 steps in a DNS lookup:","text":"<ol> <li>A user types \u2018example.com\u2019 into a web browser and the query travels into the Internet and is received by a DNS recursive resolver.</li> <li>The resolver then queries a DNS root nameserver (.).</li> <li>The root server then responds to the resolver with the address of a Top Level Domain (TLD) DNS server (such as .com or .net), which stores the information for its domains. When searching for example.com, our request is pointed toward the .com TLD.</li> <li>The resolver then makes a request to the .com TLD.</li> <li>The TLD server then responds with the IP address of the domain\u2019s nameserver, example.com.</li> <li>Lastly, the recursive resolver sends a query to the domain\u2019s nameserver.</li> <li>The IP address for example.com is then returned to the resolver from the nameserver.</li> <li>The DNS resolver then responds to the web browser with the IP address of the domain requested initially.</li> </ol>"},{"location":"Specifics/Networking/#how-to-do-the-lookup-manually","title":"How to do the lookup manually","text":"<p>Start with the root, then do @ again with the IPs given.</p> <pre><code>dig pollstar.com @a.root-servers.net\ndig pollstar.com @192.12.94.30\ndig pollstar.com @108.162.194.74\n</code></pre>"},{"location":"Specifics/Networking/#iterative-vs-recurisve-query","title":"Iterative vs Recurisve Query","text":"<p>Recursive - DNS host communicates with other DNS servers to get the answer on behalf of the client</p> <p>Iterative - Client communicates with each DNS server in the chain itself.</p>"},{"location":"Specifics/Networking/#explain-the-soa-record-in-dns","title":"Explain the SOA record in DNS","text":"<p>SOA stands for Start of Authority and it contains the following entries:</p> <pre><code>@ IN SOA nameserver.mycomaind.com. postmaster.mydomain.com. (  \n    1 ; serial number  \n    3600 ; refresh [1h]  \n    600 ; retry [10m]  \n    86400 ; expire [1d]  \n    3600 \n) ; \nmin TTL [1h]fire\n</code></pre> <ul> <li>Serial number should be refreshed each time a change is made to the zone file. <ul> <li>This is how slave DNS servers know to pull a change from the master.  </li> </ul> </li> <li>Refresh is the amount of time a slave DNS server should wait before pulling from the master.  </li> <li>Retry is how long a slave should wait before retrying to get a zone file if the initial retry fails.  </li> <li>Expire is how long a secondary server will keep trying to get a zone from the master. If this time expires before a successful zone transfer, the secondary will stop answering queries.  </li> <li>TTL is how long to keep the data in a zone file.</li> </ul>"},{"location":"Specifics/Networking/#security","title":"Security","text":""},{"location":"Specifics/Networking/#how-does-ssl-work","title":"How does SSL work?","text":"<p>SSL stands for secure socket layer. It has been renamed to TLS starting from SSL v 4.0. </p> <p>TLS is a secure way of communicating through a network. A majority of secure HTTP communication on the web takes place using TLS. TLS works at session layer and presentation layer of the OSI model. </p> <p>Initially at the session layer asymmetric encryption takes place, after that at the presentation later symmetric cipher and session key are used. The basic principle behind TLS is to encrypt data going across the network using public key encryption first, followed by using a shared key. </p> <p>Also the other component of TLS is server certificate authentication which is done through a certificate authority. Clients contain a list of certificate authorities, and it uses the public key of the CA in the certificate to verify the certificate being authentic. A good reference for TLS is here https://en.wikipedia.org/wiki/Secure_Socket_Layer.</p>"},{"location":"Specifics/Networking/#tcp-three-way-handshake-process","title":"TCP Three-Way Handshake Process","text":"<p>TCP traffic begins with a three-way handshake. In this TCP handshake process, a client needs to initiate the conversation by requesting a communication session with the Server:</p> <p></p> <p>3 way Handshake Diagram -   Step 1:\u00a0In the first step,\u00a0the client establishes a connection with a server. It sends a segment with SYN and informs the server about the client should start communication, and with what should be its sequence number. -   Step 2:\u00a0In this step\u00a0server responds to the client request with SYN-ACK signal set. ACK helps you to signify the response of segment that is received and SYN signifies what sequence number it should able to start with the segments. -   Step 3:\u00a0In this final step, the client acknowledges the response of the Server, and they both create a stable connection will begin the actual data transfer process.</p>"},{"location":"Specifics/Networking/#tls-handshake-tldr","title":"TLS Handshake TLDR","text":"<p>SSL/TLS uses both asymmetric and symmetric encryption to protect the confidentiality and integrity of data-in-transit. Asymmetric encryption is used to establish a secure session between a client and a server, and symmetric encryption is used to exchange data within the secured session.\u00a0</p> <p>A website must have an SSL/TLS certificate for their web server/domain name to use SSL/TLS encryption. Once installed, the certificate enables the client and server to securely negotiate the level of encryption in the following steps:</p> <ul> <li>The client contacts the server using a secure URL (HTTPS\u2026).</li> <li>The server sends the client its certificate and public key.</li> <li>The client verifies this with a Trusted Root Certification Authority to ensure the certificate is legitimate.</li> <li>The client and server negotiate the strongest type of encryption that each can support.</li> <li>The client encrypts a session (secret) key with the server\u2019s public key, and sends it back to the server.</li> <li>The server decrypts the client communication with its private key, and the session is established.</li> <li>The session key (symmetric encryption) is now used to encrypt and decrypt data transmitted between the client and server.</li> </ul>"},{"location":"Specifics/Networking/#tls-handshake","title":"TLS handshake","text":"<ul> <li>The client computer sends a\u00a0<code>ClientHello</code>\u00a0message to the server with its Transport Layer Security (TLS) version, list of cipher algorithms and compression methods available.</li> <li>The server replies with a\u00a0<code>ServerHello</code>\u00a0message to the client with the TLS version, selected cipher, selected compression methods and the server's public certificate signed by a CA (Certificate Authority). The certificate contains a public key that will be used by the client to encrypt the rest of the handshake until a symmetric key can be agreed upon.</li> <li>The client verifies the server digital certificate against its list of trusted CAs. If trust can be established based on the CA, the client generates a string of pseudo-random bytes and encrypts this with the server's public key. These random bytes can be used to determine the symmetric key.</li> <li>The server decrypts the random bytes using its private key and uses these bytes to generate its own copy of the symmetric master key.</li> <li>The client sends a\u00a0<code>Finished</code>\u00a0message to the server, encrypting a hash of the transmission up to this point with the symmetric key.</li> <li>The server generates its own hash, and then decrypts the client-sent hash to verify that it matches. If it does, it sends its own\u00a0<code>Finished</code>\u00a0message to the client, also encrypted with the symmetric key.</li> <li>From now on the TLS session transmits the application (HTTP) data encrypted with the agreed symmetric key.</li> </ul>"},{"location":"Specifics/SRE%20Interview%20Questions/","title":"SRE Interview Questions","text":""},{"location":"Specifics/SRE%20Interview%20Questions/#q1-depth-of-knowledge","title":"Q1: Depth of knowledge","text":"<p>Goals: With this question, we want to check the candidate\u2019s ability to demonstrate some depth of knowledge with regard to operating systems and networking. Particularly, a successful candidate will be able to methodically explore the answer space and give a coherent description of what is going on at each step. Rather than concept memorization, we\u2019re looking for the candidate to connect the ideas and reason about the answer \u2013 so it may be fine for reasonable hypotheses to be accepted as long as the candidate calls them out as such.</p> <p>Topics we want covered by this question: -   Linux system administration background knowledge, particularly a good understanding of:     -   CLI Tools     -   Fork/Exec, argv     -   Networking     -   Routing     -   DNS -   Specific networking topics:     -   TCP handshake     -   ARP -   Security topics:     -   Basic understanding of PKI and why it\u2019s important</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#questions-one-of-the-following-10-15-minutes","title":"Questions (One of the following \u2013 10-15 minutes):","text":"<ul> <li>Explain in as much detail as possible what happens on the client, network, and server when you are at a bash shell prompt on a Linux host and type \u201cssh admin@shell.linkedin.com\u201d</li> <li>Explain in as much detail as possible what happens on the client, network, and server when you are at a bash shell prompt on a Linux host and type \u201ccurl https://www.linkedin.com/api/\u201d</li> <li>Explain in as much detail as possible what happens on the client, network, and server when you are at a bash shell prompt on a Linux host and type \u201cscp -r my_directory admin@shell.linkedin.com:~\u201d</li> </ul>"},{"location":"Specifics/SRE%20Interview%20Questions/#answers","title":"Answers","text":"<p>Interviewers should probe for the candidate to describe the flow systematically and feel free to ask specifically about the top level bullet points. -   Client host     -   Parsing shell arguments     -   Finding the binary in the path/checking aliases     -   Executing the binary using fork/exec     -   Some sort of reasoning about the meaning of the arguments (common things vs trivia)         -   Ssh: They should be able to note that admin is the user and shell.linkedin.com is the host         -   Curl: They should note https. -L means to follow redirects         -   SCP: In this case, a candidate should know that my_directory refers to a local directory and the \u201c:~\u201d refers to the remote destination     -   Checking local configuration         -   SSH/SCP: Candidates should have some understanding of ssh_config files and known_hosts (or at least know that these exist)         -   CURL: That curl checks for a curlrc is likely less common knowledge but it\u2019s nice if they mention it</p> <ul> <li>Network communication<ul> <li>DNS<ul> <li>At least candidates should know what DNS is and its purpose</li> </ul> </li> <li>How DNS works<ul> <li>Client Cache + Servers defined in resolv.conf</li> <li>Recursive DNS resolution</li> <li>Authoritative DNS Server</li> </ul> </li> </ul> </li> <li>OSI Model \u2013 (if they have some familiarity with networking)<ul> <li>Networking#OSI Model</li> </ul> </li> <li>Network<ul> <li>This probably depends a lot on candidate experience with networking. There\u2019s a lot of pieces they could talk about \u2013 give them credit for this but don\u2019t let it take the whole time for the question!</li> <li>They should know about ip addresses and ports</li> <li>Ideally they would also know that routers and switches exist and what each is for</li> <li>Great depth for this time period would be to give a simple description of AS + BGP Routing on the internet and some description for why this is needed</li> </ul> </li> <li>Transport<ul> <li>In this case, all 3 commands use TCP</li> <li>Candidates should be able to describe the TCP Handshake (SYN SYN/ACK ACK)</li> <li></li> <li>They should also understand the guarantees of TCP (retransmission + order)<ul> <li>Checksum</li> <li>serial number</li> <li>Confirm response</li> <li>Retransmit after timeout</li> <li>Connection management</li> <li>flow control</li> <li>Congestion control</li> </ul> </li> <li>Great depth would be the ability to give some description for how these are accomplished</li> </ul> </li> <li>Application<ul> <li>SSH</li> </ul> </li> <li>Basic knowledge about the protocol in each command is nice (for example how HTTP works)</li> </ul>"},{"location":"Specifics/SRE%20Interview%20Questions/#how-to-ask-this-question","title":"How to ask this question","text":"<ul> <li>[First 5 minutes] In the next 5 or so minutes, describe some high level problems that could happen to make  ssh/curl not work like you would expect.<ul> <li>Guide the candidate to give very general answers here and make notes about what they say. Also don\u2019t allow candidates to give too similar answers twice (say two answers around security \u2013 we want to get a broad scope of answers and dive on each of them).</li> <li>For example, say they identify \u201cdns could be broken\u201d and \u201crouting could be broken \u2013 no route to host\u201d</li> <li>These are two different networking topics</li> <li>An interviewer should combine this into one 5 minute description below (?)</li> </ul> <li>In 5 minute chunks of time, ask about each of the candidates questions \u2013 if they were not able to give 3, propose an example problem.</li>"},{"location":"Specifics/SRE%20Interview%20Questions/#rubric","title":"Rubric","text":"<p>This rubric provides a point-based system for assessing each \u201cpillar of knowledge\u201d regarding . This allows us to assess both breadth and depth. The below tables provide possible candidate explanations that pertain to a certain level of knowledge, 0 being least knowledgeable and 3 being most."},{"location":"Specifics/SRE%20Interview%20Questions/#client-side-problems","title":"Client Side Problems","text":"<p>1 anything pertaining to path or file permissions, possible aliases 2 issues with binary loading and execution 3 kernel issues, issues with virtual file system of physical disk  </p>"},{"location":"Specifics/SRE%20Interview%20Questions/#network-problems","title":"Network Problems","text":"<p>1 \u201cIs it connected to the internet?\" 2 ICMP vs TCP, DNS resolution 3 flapping NIC</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#security-problems","title":"Security Problems","text":"<p>1 Firewall, port blocked 2 untrusted certificate, known host mismatch 3 cipher negotiation failure, potential spoof, failure to generate session key</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#remote-problems","title":"Remote Problems","text":"<p>1 Remote doesn\u2019t exist or isn\u2019t able to receive ssh 2 Port blocked locally 3 Kernel issues, issues with virtual file system or physical disk</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#freshness-justification","title":"Freshness Justification","text":"<p>These questions are open-ended enough so as to allow the candidate to explore the topics deeply and demonstrate understanding. Just knowing the question does not help the candidate to give a coherent answer, and we feel discussion will quickly reveal candidates with prepared answers.</p> <p>We feel that even knowing the question or having a prepared answer, this question still gives a strong signal for the question goals here.</p> <p>Specifically these questions (curl + ssh) are well used enough that the majority of candidates should be familiar with them. In addition, we give candidates the opportunity to talk about many concepts at both the operating system and network layers, which another question might not fully capture.</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#q2-scalability-and-monitoring","title":"Q2: Scalability and Monitoring","text":"<p>Goals: With this question, we want to give the candidate the opportunity to apply their knowledge of systems to solve a scalability problem in production.\u00a0 Rather than a knowledge based answer (listing buzzwords), a successful candidate will demonstrate critical thinking and give reasonable ideas for debugging, monitoring, and scaling.\u00a0 We expect this interview to be more discussion oriented than the first question, and candidates should ask questions to understand the scenario well before trying to come up with answers.</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#topics-covered","title":"Topics Covered","text":"<ul> <li>Monitoring<ul> <li>Metrics/Logging/Synthetic Testing/RUM (real user monitoring)</li> </ul> </li> <li>Software profiling</li> <li>Service Tracing</li> <li>Scaling<ul> <li>Horizontal Scaling</li> <li>Vertical Scaling</li> <li>Autoscaling</li> <li>Sharding or partitioning data</li> </ul> </li> </ul>"},{"location":"Specifics/SRE%20Interview%20Questions/#question-15-20-minutes","title":"Question (15-20 minutes?):","text":"<p>You start a new job and receive user complaints that the page load time for shipping/payment screen has been getting slower and slower the past few months.\u00a0 The management team projects this slowness is causing revenue to drop, asking you to make fixing the issue your top priority.</p> <p>Additionally, they want you to come up with a solution to detect issues like this in the future.</p> <p>The system architecture for the service in question currently contains a frontend presentation webserver, a number of backend api services, and a mysql database.</p> <p>What steps and in what order would you take to try to resolve this issue this week?</p> <p>What methods would you recommend to detect and prevent such issues in the longer term?  </p> <p>[First 5 minutes] For the next 5 minutes, let\u2019s talk about some high level monitoring and metrics you want to examine. -   Candidates should give a few high level categories (client side issues, server side issues, remote issues) [Next 10 minutes] Talk about each (or a selection) of the categories they identified [Final 5 minutes] Talk about how to evaluate/improve the scalability of the application.</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#rubric_1","title":"Rubric","text":"<p>1 least points, 3 most points</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#external-monitoring","title":"External Monitoring","text":"<p>1 Identifies need incase local monitoring fails on itself 2 Regional differences, ISPs down 2 Breachable SLA (99th, 95th, 50th percentiles) 3 QPS, Latency, Errors (QLE)</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#host-metrics","title":"Host Metrics","text":"<p>1 Basic self healtcheck 2 Diskspace, memory, i/o 3 Programming language specific, such as Java GC, etc 3 SLAs, SLOs, SLIs -   An SLA is a contract. -   An SLO is a specific goal that is defined in a contract. -   An SLI measures the extent to which teams comply with the SLO promises they make in SLA contracts.     - The SLI (Service Level Indicator) equation is the number of good events divided by the total number of valid events, multiplied by 100 to keep it a uniform percentage.     - measures how well a company actually meets the SLO promises that it sets within SLAs. 3 Alerting/reporting on SLA breaches</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#log-aggregation","title":"Log Aggregation","text":"<p>1 Why this is important in a high-availability / distributed system 2 Queryable</p>"},{"location":"Specifics/SRE%20Interview%20Questions/#alerting-strategies","title":"Alerting Strategies","text":"<p>1 Identifies need for basic alerting 2 Thresholds, statistics, etc, exceptions 3 Burn Rate, Error budget, ML/AI * A burn-rate alerting policy notifies you when your error budget is consumed faster than a threshold you define, measured over the alert's compliance period.</p> <p>Other Strategies * Look for code changes that correlate to the problem beginning * Try to reproduce with synthetic traffic (time consuming) * Try a git-bisect (time consuming) * Performance Profiling * Heap dumps * </p>"},{"location":"Specifics/SRE%20Interview%20Questions/#extra","title":"Extra","text":"<p>Checking aliases</p> <p>Checking $PATH</p> <p>Hostname to IP resolution</p> <p>Lookup in nsswitch.conf</p> <p>For DNS, looking in client cache</p> <p>DNS query out to first-level DNS server in resolv.conf</p> <p>connect()/otherwise create a new network connection</p> <p>Network stack determining local gateway</p> <p>Using ARP to find gateway's MAC address</p> <p>Shell parsing the command line (figuring out it isn't running a builtin, describing argv, etc.)</p> <p>Shell finding the program physically on disk via VFS</p> <p>Linking in dynamic libraries when launching binary</p> <p>Detailed description of Internet routing (outside gw to our site)</p> <p>Detailed discussion of environment variables/argument parsing for the binary</p> <p>Correctly describes the 3-way TCP handshake</p> <p>Verification of remote host via SSL certificate checking or SSH host key checking</p> <p>Knowing sessions are encrypted with a symmetric session cipher, and why</p> <p>Correctly describing DNS beyond the first resolver (root NS, TLD NS, authoritative)</p> <p>Explaining the nuances of fork()/exec() - their differences, how they interact with the parent program's environment</p> <p>Detailed discussion of encryption (cipher negotiation, MAC negotiation, more detail on cert verification, etc.)</p>"},{"location":"Specifics/SSH/","title":"SSH","text":"<p>How does SSH work?  Related: Networking#TLS Handshake TLDR</p>"},{"location":"Specifics/SSH/#symmetric-encryption","title":"Symmetric Encryption","text":"<ul> <li>Where can be be used to encrypt and decrypted</li> <li>Called a shared secret</li> <li>Used to pass the messages back and forth</li> <li>The key generated during a connection is session based</li> <li>The key is established prior to authenticating the client</li> <li>Ciphers for symmetric key generation:<ul> <li>AES, Blowfish, 3DES, CAST128, and Arcfour etc</li> </ul> </li> </ul>"},{"location":"Specifics/SSH/#asymmetrical-keys","title":"Asymmetrical keys","text":"<ul> <li>only used for authentication</li> <li>The mathematical relationship between the public key and the private key allows the public key to encrypt messages that can only be decrypted by the private key.</li> <li>This is a one-way ability, meaning that the public key has no ability to decrypt the messages it writes, nor can it decrypt anything the private key may send it.</li> <li>SSH key pairs can be used to authenticate a client to a server.<ul> <li>The client creates a key pair and then uploads the public key to any remote server it wishes to access. This is placed in a file called authorized_keys within the ~/.ssh directory in the user account's home directory on the remote server.</li> <li>Key based authentication<ul> <li>Server will create a challenge message with the public key</li> <li>Client will prove it has the private key by decrypting it</li> <li>Server allows authentication</li> </ul> </li> </ul> </li> </ul>"},{"location":"Specifics/SSH/#hashing","title":"Hashing","text":"<ul> <li>hashes are mainly used for data integrity purposes and to verify the authenticity of communication. The main use in SSH is with HMAC, or hash-based message authentication codes. These are used to ensure that the received message text is intact and unmodified.</li> <li>As part of the symmetrical encryption negotiation outlined above, a message authentication code (MAC) algorithm is selected. The algorithm is chosen by working through the client's list of acceptable MAC choices.</li> <li>The MAC is calculated from the symmetrical shared secret, the packet sequence number of the message, and the actual message content.</li> <li>The MAC itself is sent outside of the symmetrically encrypted area as the final part of the packet. Researchers generally recommend this method of encrypting the data first, and then calculating the MAC.</li> </ul>"},{"location":"Specifics/SSH/#stages","title":"Stages","text":""},{"location":"Specifics/SSH/#stage-1","title":"Stage 1","text":"<p>Agree upon and establish encryption to protect future communication.  Accomplishes the task of generating an identical shared secret without ever having to send that information over insecure channels.</p> <p>The basis of this procedure for classic Diffie-Hellman is: -   Both parties agree on a large prime number, which will serve as a seed value. -   Both parties agree on an cipher (typically AES), which will be used to manipulate the values in a predefined way. -   Independently, each party comes up with another prime number which is kept secret from the other party. This number is used as the private key for this interaction (different than the private SSH key used for authentication). -   The generated private key, the cipher , and the shared prime number are used to generate a public key that is derived from the private key, but which can be shared with the other party. -   Both participants then exchange their generated public keys. -   The receiving entity uses their own private key, the other party's public key, and the original shared prime number to compute a shared secret key. Although this is independently computed by each party, using opposite private and public keys, it will result in the same shared secret key. -   The shared secret is then used to encrypt all communication that follows.</p>"},{"location":"Specifics/SSH/#stage-2","title":"Stage 2","text":"<p>Authenticate the user and discover whether access to the server should be granted. -   simplest is probably password authentication, in which the server simply prompts the client for the password of the account they are attempting to login with. The password is sent through the negotiated encryption, so it is secure from outside parties. -   Not recommended because scripts can break passwords, especially easy ones -   Client send ID for the key pair it wants to authenticate with -   Server checks the authorized_keys fo a match -   If found, encrypted a random # with the public key -   Send to client -   Client decrypts and sends back hash of key -   Server also hashes, compares, and hopefully says, \"Cool\"</p>"},{"location":"Specifics/Systems%20Design%20Template/","title":"Systems Design Template","text":""},{"location":"Specifics/Systems%20Design%20Template/#feature-expectations-5-min","title":"FEATURE EXPECTATIONS - 5 min","text":"<ul> <li>Waht does it need to do?</li> <li>Use cases</li> <li>Scenarios that will not be covered</li> <li>Who will use</li> <li>How many will use</li> <li>Usage patterns</li> </ul>"},{"location":"Specifics/Systems%20Design%20Template/#estimations-5-min","title":"ESTIMATIONS - 5 min","text":"<ul> <li>Throughput (QPS for read and write queries)</li> <li>Latency expected from the system (for read and write queries)</li> <li>Read/Write ratio</li> <li>Traffic estimates<ul> <li>Write (QPS, Volume of data)</li> <li>Read  (QPS, Volume of data)</li> </ul> </li> <li>Storage estimates</li> <li>Memory estimates<ul> <li>If we are using a cache, what is the kind of data we want to store in cache</li> <li>How much RAM and how many machines do we need for us to achieve this ?</li> <li>Amount of data you want to store in disk/ssd</li> </ul> </li> </ul>"},{"location":"Specifics/Systems%20Design%20Template/#design-goals-5-min","title":"DESIGN GOALS - 5 min","text":"<ul> <li>Latency and Throughput requirements</li> <li>Consistency vs Availability  [Weak/strong/eventual =&gt; consistency | Failover/replication =&gt; availability]</li> </ul>"},{"location":"Specifics/Systems%20Design%20Template/#high-level-design-5-10-min","title":"HIGH LEVEL DESIGN - 5-10 min","text":"<ul> <li>APIs for Read/Write scenarios for crucial components</li> <li>Database schema</li> <li>Basic algorithm</li> <li>High level design for Read/Write scenario</li> </ul>"},{"location":"Specifics/Systems%20Design%20Template/#deep-dive-15-20-min","title":"DEEP DIVE - 15-20 min","text":"<ul> <li>Deployment<ul> <li>Google Systems Design - Code Deployment</li> </ul> </li> <li>Scaling the algorithm</li> <li>Scaling individual components: <ul> <li>Availability, Consistency and Scale story for each component</li> <li>Consistency and availability patterns</li> </ul> </li> <li>Call graph depth<ul> <li>Longer sequence call depths in the graph means compounding availability problems<ul> <li>If both\u00a0<code>Foo</code>\u00a0and\u00a0<code>Bar</code>\u00a0each had 99.9% availability, their total availability in sequence would be 99.8%.</li> </ul> </li> </ul> </li> <li>What is a graceful (aka. zero downtime) restart?  <ul> <li>A graceful restart takes place in two steps:<ul> <li>A new generation of parent and child processes are spawned to take over service.</li> <li>Processes from the older generation only exit once they have finished their tasks.</li> </ul> </li> </ul> </li> <li>Think about the following components, how they would fit in and how it would help<ul> <li>DNS Networking#How does DNS work</li> <li>CDN [Push vs Pull]</li> <li>Load Balancers [Active-Passive, Active-Active, Layer 4, Layer 7]<ul> <li>Reverse Proxy</li> </ul> </li> <li>Single points of failure</li> <li>Application layer scaling [Microservices, Service Discovery]</li> <li>DB Cliff Notes - Someguy#Relational model vs document model<ul> <li>RDBMS <ul> <li>Master-slave, Master-master, Federation, Sharding, Denormalization, SQL Tuning</li> <li>Benefits - great at many-to-many and many-to-one relationships, one copy of data</li> <li>Downfalls - high impedance (results format don't match the model format)</li> </ul> </li> <li>NoSQL<ul> <li>Benefits: Greater scalability, high write throughput, large datasets, dynamic schemas, low impendance</li> <li>Downfalls: not good at many-to-one or many-to-many relationships, data can be replicated in many objects, eventual consistency</li> <li>Key-Value, Wide-Column, Graph, Document<ul> <li>Fast-lookups:<ul> <li>RAM  [Bounded size] =&gt; Redis, Memcached</li> <li>AP [Unbounded size] =&gt; Cassandra, RIAK, Voldemort</li> <li>CP [Unbounded size] =&gt; HBase, MongoDB, Couchbase, DynamoDB</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Caches<ul> <li>Caching</li> </ul> </li> <li>Asynchronism<ul> <li>Message queues</li> <li>Task queues</li> <li>Back pressure</li> </ul> </li> <li>Communication<ul> <li>TCP</li> <li>UDP</li> <li>REST</li> <li>RPC</li> </ul> </li> <li>Metrics<ul> <li>External (off-box, not off prem) Monitoring<ul> <li>in case local monitoring collapses</li> <li>regional problems e.g. check for down isps</li> <li>breaches in SLA</li> <li>QPS, Latency, Errors, Calltimes</li> </ul> </li> <li>Host <ul> <li>health check</li> <li>disk space, memory, swap, i/o, load</li> <li>anything specific to the programming language<ul> <li>e.g. Java GC</li> </ul> </li> </ul> </li> <li>SLA  Alerting/Reporting/Breaches<ul> <li>An SLA is a contract.</li> <li>An SLO is a specific goal that is defined in a contract.</li> <li>An SLI measures the extent to which teams comply with the SLO promises they make in SLA contracts.<ul> <li>The SLI (Service Level Indicator) equation is the number of good events divided by the total number of valid events, multiplied by 100 to keep it a uniform percentage.</li> <li>measures how well a company actually meets the SLO promises that it sets within SLAs.</li> <li>Measured over some period</li> </ul> </li> <li>Burn rate - notify when you're consuming your error budget faster than threshold, measured over compliance period</li> </ul> </li> </ul> </li> <li>Log Aggregation<ul> <li>Why this is important in a high-availability / distributed system</li> <li>Query-able</li> </ul> </li> </ul> </li> </ul>"},{"location":"Specifics/Systems%20Design%20Template/#justify-5-min","title":"JUSTIFY - 5 min","text":"<pre><code>-  Throughput of each layer\n-  Latency caused between each layer\n-  Overall latency justification\n</code></pre>"},{"location":"Specifics/Troubleshooting%20via%20Metrics/","title":"Troubleshooting via Metrics","text":""},{"location":"Specifics/Troubleshooting%20via%20Metrics/#looking-at-graphs","title":"Looking at Graphs","text":"<p>Wants to check for deployment activity Wants to check for cyclical pattern (week over week etc) Wants to check for recent changes to threshold Wants to check for magnitude of difference below threshold Asks NOC for nothing (while investigating) OR Asks NOC to look for other reg-related alerts/flows/metrics Makes a sensible guess for the cause of the problem Makes a second sensible guess (some examples: ABI, seasonality, one colo failure) Questions the time it took NOC to escalate or mentions it as a followup item</p>"},{"location":"Specifics/Troubleshooting%20via%20Metrics/#initial-troubleshooting","title":"Initial Troubleshooting","text":"<p>Reviews all presented data and asks appropriate questions about it Suggests a correlation between deployment and drop in regs from graph Suggests problem may be due to deployment from informed Wants to check error logs on the reg service Wants to check some other log (that makes some degree of sense to check)</p>"},{"location":"Specifics/Troubleshooting%20via%20Metrics/#initial-resolution","title":"Initial Resolution","text":"<p>Takes a logical action to resolve the issue Second action is also logical based on 1st failing Stays calm and doesn\u2019t argue when told their solution didn\u2019t work</p>"},{"location":"Specifics/Troubleshooting%20via%20Metrics/#secondary-resolution","title":"Secondary Resolution","text":"<p>Last action also meaningful Suggests escalation to engineering Suggests escalation to management Has articulable thing for escalated person to do Articulable thing makes sense given the problem Second articulable thing for escalated person, that also makes sense</p>"},{"location":"Specifics/Unix%20Processes/","title":"Linux/Unix Processes","text":""},{"location":"Specifics/Unix%20Processes/#define-the-boot-process-of-a-linux-system","title":"Define the boot process of a Linux system","text":"<ul> <li>Once you power a system on, the first thing that happens is the BIOS loads and performs POST or a power on self test, to ensure that the components needed for a boot are ok. <ul> <li>For instance if the CPU is defective, the system will give an error that POST has failed. (BIOS stands for Basic Input/Output system)  </li> </ul> </li> <li>After POST the BIOS looks at the MBR or master book record and executes the boot loader. <ul> <li>In case of a Linux system that might be GRUB or Grand Unified BootLoader. GRUB\u2019s job is to give you the choice of loading a Linux kernel or other OS that you may be running  </li> </ul> </li> <li>Once you ask GRUB to load a kernel, usually an initial ramdisk kernel is loaded, which is a small kernel that understands filesystem. <ul> <li>This will in turn mount the filesystem and will start the Linux kernel from the filesystem  </li> </ul> </li> <li>The kernel will then start init, which is the very first process, usually having PID 1. </li> <li>Init will look at /etc/inittab and will switch to the default run-level which on Linux servers tends to be 3.  </li> <li>There are different run level scripts in /etc/rc.d/rc[0-6].d/ which are then executed based on the runlevel the system needs to be in.  </li> </ul>"},{"location":"Specifics/Unix%20Processes/#how-do-you-make-changes-to-kernel-parameters-that-you-want-to-persist-across-boot","title":"How do you make changes to kernel parameters that you want to persist across boot?","text":"<p>/etc/sysctl.conf contains kernel parameters that can be modified. You can also use the sysctl command to make changes at runtime.</p>"},{"location":"Specifics/Unix%20Processes/#what-is-the-difference-between-a-process-and-a-thread","title":"What is the difference between a process and a thread?","text":"<p>A thread is a lightweight process. Each process has a separate stack, text, data and heap. Threads have their own stack, but share text, data and heap with the process. Text is the actual program itself, data is the input to the program and heap is the memory which stores files, locks, sockets.</p>"},{"location":"Specifics/Unix%20Processes/#what-is-a-zombie-process","title":"What is a zombie process?","text":"<p>A zombie process is a one which has completed execution, however it\u2019s entry is still in the process table to allow the parent to read the child\u2019s exit status. The reason the process is a zombie is because it is \u201cdead\u201d but not yet \u201creaped\u201d by it\u2019s parent. Parent processes normally issue the wait system call to read the child\u2019s exit status whereupon the zombie is removed. The kill command does not work on zombie process. When a child dies the parent receives a SIGCHLD signal. Zombie processes do not take up system resources, except for the tiny amount of space they use up when appearing in the process id table.</p>"},{"location":"Specifics/Unix%20Processes/#how-do-you-end-up-with-zombie-processes","title":"How do you end up with zombie processes?","text":"<p>Zombie processes are created when the parent does not reap the child. This can happen due to parent not executing the wait() system call after forking.</p>"},{"location":"Specifics/Unix%20Processes/#how-to-daemonize-a-process","title":"How to daemonize a process","text":"<ol> <li>The fork() call is used to create a separate process.</li> <li>The setsid() call is used to detach the process from the parent (normally a shell).</li> <li>The file mask should be reset. The reason for this is because we want to create new files with the mask that is needed for the child process.</li> <li>The current directory should be changed to something benign. We may not want the child to be in the same pwd as the parent.</li> <li>The standard files (stdin,stdout and stderr) need to be reopened.</li> </ol>"},{"location":"Specifics/Unix%20Processes/#describe-ways-of-process-inter-communication","title":"Describe ways of process inter-communication","text":"<ol> <li>POSIX mmap<ol> <li>memory-mapped file containing the shared-memory object</li> </ol> </li> <li>Message queues</li> <li>Semaphores<ol> <li>Semaphore is used to protect any resources such as Global shared memory that needs to be accessed and updated by many processes simultaneously.</li> <li>Whenever a process needs to access the resource, it first needs to take permission from the semaphore. Semaphore give permission to access a resource if resource is free otherwise process has to wait.</li> </ol> </li> <li>Shared memory<ol> <li>The fastest way for processes to communicate is directly, through a shared segment of memory. A common memory area is added to the address space of sharing processes.</li> </ol> </li> <li>Anonymous pipes<ol> <li>Anonymous pipes is Unidirectional.</li> <li>One process to another</li> <li>Local only</li> <li>No ownership</li> </ol> </li> <li>Named pipes<ol> <li>Named Pipe can be used one-way or two-way(Duplex) Communication.</li> <li>Can be used between processes or over the network</li> <li>Multiple processes can use the same pipe</li> <li>Security and ownership included</li> </ol> </li> <li>Unix domain sockets<ol> <li>Data is exchanged between programs directly in the operating system\u2019s kernel via files on the host filesystem. To send or receive data using domain sockets, programs read and write to their shared socket file, bypassing network based sockets and protocols entirely.<ol> <li><code>SOCK_STREAM</code> (compare to TCP) \u2013 for a stream-oriented socket</li> <li><code>SOCK_DGRAM</code> (compare to UDP) \u2013 for a datagram-oriented socket that preserves message boundaries (as on most UNIX implementations, UNIX domain datagram sockets are always reliable and don't reorder datagrams</li> <li><code>SOCK_SEQPACKET</code> (compare to SCTP) \u2013 for a sequenced-packet socket that is connection-oriented, preserves message boundaries, and delivers messages in the order that they were sent</li> </ol> </li> </ol> </li> <li>RPC<ol> <li>Remote procedure call (RPC) is an Inter-process communication technology that allows a computer program to cause a subroutine or procedure to execute in another address space (commonly on another computer on a shared network) without the programmer explicitly coding the details for this remote interaction.</li> <li>Programmer doesn't have to account for the fact the function call is remote.</li> </ol> </li> </ol>"},{"location":"Specifics/Unix%20Processes/#fork-vs-exec","title":"Fork vs Exec","text":"<p>fork() creates a new process by duplicating the calling process, The new process, referred to as child, is an exact duplicate of the calling process, referred to as parent, except for the following :</p> <ol> <li>The child has its own unique process ID (PID)</li> <li>The child\u2019s parent process ID is the same as the parent\u2019s process ID.</li> <li>The child does not inherit its parent\u2019s memory locks and semaphore adjustments.</li> <li>The child does not inherit outstanding asynchronous I/O operations from its parent nor does it inherit any asynchronous I/O contexts from its parent.</li> </ol> <p>Because the two processes are now running exactly the same code, they can tell which is which by the return code of <code>fork</code> - the child gets 0, the parent gets the PID of the child.</p> <p>The <code>exec</code> call is a way to basically replace the entire current process with a new program. It loads the program into the current process space and runs it from the entry point.</p> <p>Shells typically do this whenever you try to run a program like <code>find</code> - the shell forks, then the child loads the <code>find</code> program into memory, setting up all command line arguments, standard I/O and so forth.</p>"},{"location":"Specifics/Unix%20Processes/#describe-how-processes-executes-in-a-unix-shell","title":"Describe how processes executes in a Unix shell","text":"<p>Let\u2019s take the example of /bin/ls. When you run \u2018ls\u2019 the shell searches in its path for an executable named \u2018ls, when it finds it, the shell will forks off a copy of itself using the fork system call. If the fork succeeds, then in the child process the shell will run \u2018exec /bin/ls\u2019 which will replace the copy of the child shell with itself. Any parameters that that are passed to \u2018ls\u2019 are done so by exec.</p>"},{"location":"Specifics/Unix%20Processes/#what-are-unix-signals","title":"What are Unix Signals?","text":"<p>Signals are an inter process communication method. The default signal in Linux is SIG-TERM. SIG-KILL cannot be ignored and causes an application to be forcefully killed. Use the \u2018kill\u2019 command to send signals to a process. Another popular signal is the \u2018HUP\u2019 signal which is used to \u2018reset\u2019 or \u2018hang up\u2019 applications</p> <p>The signals SIGKILL and SIGSTOP cannot be caught, blocked, or  ignored.</p>"},{"location":"Specifics/Unix%20Processes/#when-you-send-a-hup-signal-to-a-process-you-notice-that-it-has-no-impact-what-could-have-happened","title":"When you send a HUP signal to a process, you notice that it has no impact, what could have happened?","text":"<p>During critical section execution, some processes can setup signal blocking. The system call to mask signals is \u2018sigprocmask\u2019. When the kernel raises a blocked signal, it is not delivered. Such signals are called pending. When a pending signal is unblocked, the kernel passes it off to the process to handle. It is possible that the process was masking SIGHUP.</p>"},{"location":"Specifics/Unix%20Processes/#process-models","title":"Process Models","text":"<p>What are the advantage of different server process models: threaded vs process vs pure async (eg. nodejs) vs hybrid (async + threadpool)  - Async are non-blocking operations within a single thread.  eg. OS reads a file, and while waiting executes some math equation, then goes back when the file is ready. - Threaded are concurrent processes (not in Python).     - Share same memory space - Processes do not share the same memory space and are separate</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/","title":"What happens when you type \"google.com\" into a browser?","text":""},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#typing-googlecom","title":"Typing google.com","text":"<p>List of things that happen when you type \"google.com\" into your browser: - Hit \"g\" (then oogle.com)     - Interrupt occurs, and the brower's autocomplete function kicks in.     - Suggestions are made     - Browser passes back control to the CPU - Hit \"enter\"     - Browser begins parsing the URL         - if protocol is absent (e.g. http) or text is not valid domain             - send as query to default search engine     - covert not ascii characters with punycode (if needed)     - check hsts list for domains that have already declared they only accepted https         - if so, add https protocol else http</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#dns-lookup","title":"DNS Lookup","text":"<ul> <li>DNS lookup<ul> <li>checks for domain name in local cache</li> <li>check for domain in hosts file</li> <li>check with dns server specified in config or from server specified via dhcp<ul> <li>If the DNS server is on the same subnet the network library follows the\u00a0<code>ARP process</code>\u00a0below for the DNS server.</li> <li>If the DNS server is on a different subnet, the network library follows the\u00a0<code>ARP process</code>\u00a0below for the default gateway IP.</li> </ul> </li> </ul> </li> <li>ARP (Address Resolution Protocol)<ul> <li>Needs IP and MAC to send broadcast</li> <li>Checks ARP cache</li> <li>Using the route table, checks to see if the IP address falls within the subnet</li> <li>If not in one of the connected subnets, <ul> <li>use the default gateway</li> <li>else lookup MAC on subnet</li> </ul> </li> <li>ARP Request<ul> <li>`Sender MAC: interface:mac:address:here</li> <li><code>Sender IP: interface.ip.goes.here</code></li> <li><code>Target MAC: FF:FF:FF:FF:FF:FF (Broadcast)</code></li> <li><code>Target IP: target.ip.goes.here</code></li> </ul> </li> <li>Response<ul> <li>directly connected to router<ul> <li>get direct reply</li> </ul> </li> <li>Hub<ul> <li>broadcast out on all ports</li> <li>if router is present, it'll reply if needed</li> </ul> </li> <li>Switch<ul> <li>will check its MAC table to see if it recognizes the MAC it'll send out on one port<ul> <li>If not, it will broadcast to all ports</li> </ul> </li> <li>if router is present, it'll reply if needed</li> </ul> </li> </ul> </li> </ul> </li> <li>DNS Server Found<ul> <li>open UDP connection to port 53<ul> <li>if response is too large TCP will be used instead</li> </ul> </li> <li>if local DNS does not have the answer, a recursive lookup begins up the chain of DNS servers until the SOA is reached, and an answer is returned.</li> <li>Networking#The 8 steps in a DNS lookup</li> </ul> </li> </ul>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#opening-a-socket","title":"Opening a Socket","text":"<p>Once we have IP address and the port number (80, 443 or unique via :some_num), requests TCP socket stream from OS.  Then it passes through these layers: - Layer 4: Transport Layer where a TCP segment is crafted. The destination port is added to the header - Layer 3: Network Layer, which wraps an additional IP header. The IP address of the destination server as well as that of the current machine is inserted to form a packet. - Layer 2:  Link Layer. A frame header is added that includes the MAC address of the machine's NIC as well as the MAC address of the gateway (local router). - Layer 1: Physical Layer Out to the internet.</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#out-in-the-wild","title":"Out in the Wild","text":""},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#snat","title":"SNAT","text":"<p>As the packet leaves the network, if the client is behind local address space SNAT will be performed at the router.</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#traveling-the-internet","title":"Traveling the internet","text":"<p>Each router along the way extracts the destination address from the IP header and routes it to the appropriate next hop. The time to live (TTL) field in the IP header is decremented by one for each router that passes. The packet will be dropped if the TTL field reaches zero or if the current router has no space in its queue (perhaps due to network congestion).</p> <ul> <li>Server receives SYN and if it's in an agreeable mood:<ul> <li>Server chooses its own initial sequence number</li> <li>Server sets SYN to indicate it is choosing its ISN</li> <li>Server copies the (client ISN +1) to its ACK field and adds the ACK flag to indicate it is acknowledging receipt of the first packet</li> </ul> </li> <li>Client acknowledges the connection by sending a packet:<ul> <li>Increases its own sequence number</li> <li>Increases the receiver acknowledgment number</li> <li>Sets ACK field</li> </ul> </li> <li>Data is transferred as follows:<ul> <li>As one side sends N data bytes, it increases its SEQ by that number</li> <li>When the other side acknowledges receipt of that packet (or a string of packets), it sends an ACK packet with the ACK value equal to the last received sequence from the other</li> </ul> </li> <li>To close the connection:<ul> <li>The closer sends a FIN packet</li> <li>The other sides ACKs the FIN packet and sends its own FIN</li> <li>The closer acknowledges the other side's FIN with an ACK</li> </ul> </li> </ul> <p>As the packet comes back, it may do DNAT at the router to get back to the local IP'ed client.</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#tls-handshake","title":"TLS Handshake","text":"<p>Networking#TLS handshake</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#if-a-packet-is-dropped","title":"If a packet is dropped","text":"<p>Sometimes, due to network congestion or flaky hardware connections, TLS packets will be dropped before they get to their final destination. The sender then has to decide how to react. The algorithm for this is called\u00a0TCP congestion control. This varies depending on the sender; the most common algorithms are\u00a0cubic\u00a0on newer operating systems and\u00a0New Reno\u00a0on almost all others. -   Client chooses a\u00a0congestion window\u00a0based on the\u00a0maximum segment size\u00a0(MSS) of the connection. -   For each packet acknowledged, the window doubles in size until it reaches the 'slow-start threshold'. In some implementations, this threshold is adaptive. -   After reaching the slow-start threshold, the window increases additively for each packet acknowledged. If a packet is dropped, the window reduces exponentially until another packet is acknowledged.</p>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#http-protocol","title":"HTTP protocol","text":"<p>If the web browser used was written by Google, instead of sending an HTTP request to retrieve the page, it will send a request to try and negotiate with the server an \"upgrade\" from HTTP to the SPDY protocol.</p> <ul> <li>Client connects and sends requests and headers, then a single newline to indicate it is done.</li> <li>Server response with response code, headers then a single new line.  </li> <li>Server then adds the HTML payload<ul> <li>if the client sends the last modification date and the data is not newer, it will respond with <code>304 not modified</code></li> </ul> </li> </ul>"},{"location":"Specifics/What%20happens%20when%20you%20type%20google.com%20into%20a%20browser/#http-server-request-handle","title":"HTTP Server Request Handle","text":"<p>What happens: - Host breaks down the requests - checks if it is configured to answer the domain - checks if it can handle the request type - Pulls the content via the handler (if handler is needed) - Responds to client</p>"}]}